-DOCSTART-

Social
media
has
become
almost
ubiquitous
in
present
times
.

Such
proliferation
leads
to
automatic
information
processing
need
and
has
various
challenges
.

The
nature
of
social
media
content
is
mostly
informal
.

Additionally
while
talking
about
Indian
social
media
,
users
often
prefer
to
use
Roman
transliterations
of
their
native
languages
and
English
embedding
.

Therefore
Information B-FIELD
retrieval I-FIELD
(
IR B-FIELD
)
on
such
Indian
social
media
data
is
a
challenging
and
difficult
task
when
the
documents
and
the
queries
are
a
mixture
of
two
or
more
languages
written
in
either
the
native
scripts
and/or
in
the
Roman
transliterated
form
.

Here
in
this
paper
we
have
emphasized
issues
related
with
Information
Retrieval
(
IR
)
for
Code
-
Mixed
Indian
social
media
texts
,
particularly
texts
from
twitter
.

We
describe
a
corpus
collection
process
,
reported
limitations
of
available
state
-
of
-
the
-
art
IR
systems
on
such
data
and
formalize
the
problem
of
Code
-
Mixed
Information
Retrieval
on
informal
texts
.


-DOCSTART-

This
paper
proposes
new
work
in
intelligent
information
retrieval
.

It
views
the
task
of
bibliographic
information
retrieval
as
a
cooperative
problemsolving
process
in
which
the
human
user
and
the
computer
system
enter
into
an
iterative
dialog
about
the
human
's
information
need
and
the
system
's
successes
or
failures
at
successive
approximations
to
that
need
.

The
work
hypothesizes
that
,
if
goals
are
described
abstractly
,
a
small
number
of
goals
account
for
a
large
number
of
searches
.

Similarly
,
if
retrieval
failures
are
described
abstractly
,
a
small
number
of
stereotypical
patterns
account
for
a
large
percentage
of
failed
searches
.

The
main
aim
of
the
work
is
to
use
these
stereotypical
patterns
as
the
basis
of
a
system
that
can
select
appropriate
strategies
for
interacting
with
the
user
,
eliciting
further
information
,
and
modifying
the
direction
of
a
search
session
by
assigning
the
user
's
information
need
and
the
intermediate
search
successes
and
failures
to
the
appropriate
abstract
categories
.

Speci
c
Aims

The
aim
of
this
research
is
to
develop
theoretical
and
practical
enhancements
in
the
area
of
on
-
line
bibliographic
search
and
retrieval
.

In
particular
,
the
project
is
intended
to
lead
to
a
deeper
theoretical
understanding
of
the
interactions
between
biomedical
researchers
and
the
literature
,
which
will
drive
the
development
of
a
high
-
performance
information
retrieval
system
.

This
work
views
information
retrieval
as
a
cooperative
problem
-
solving
process
in
which
the
participants
are
a
human
and
a
computer
system
.

As
such
,
it
focuses
on
the
mechanisms
whereby
the
system
can
infer
,
represent
,
and
reason
about
the
user
's
information
needs
and
goals
,
and
on
the
techniques
that
the
system
and
the
user
can
use
to
communicate
about
the
success
or
failure
of
a
search
in
progress
.

A
central
hypothesis
of
the
proposed
research
is
that
a
small
number
of
stereotypical
information
-
seeking
patterns
tend
to
recur
frequently
:
that
within
a
single
domain
,
a
small
number
of
goals
accounts
for
a
large
number
of
searches
.

These
goal
characterizations
,
or
abstract
query
types
can
be
used
to
select
the
system
's
behavior
when
responding
to
a
query
of
that
type
.

Such
actions
include
eliciting
further
information
from
the
user
,
selecting
di
erent
search
strategies
,
and
modifying
the
information
presented
to
the
user
.

A
second
,
related
,
hypothesis
is
that
there
are
a
small
number
of
distinct
,
abstractly
characterized
ways
in
which
a
given
document
can
be
relevant
or
irrelevant
to
a
particular
query
.

These
types
of
relevance
form
the
basis
of
qualitative
relevance
feedback
,
a
powerful
mechanism
whereby
the
user
can
critique
the
progress
of
the
current
search
,
and
whereby
the
system
can
intelligently
reformulate
the
current
query
and
also
modify
the
indexing
of
its
documents
for
the
sake
of
future
queries
.

The
project
will
gain
analytic
breadth
by
taking
advantage
of
existing
studies
of
information
-
seeking
behavior
,
and
gain
depth
by
making
structured
observations
of
biomedical
researchers
as
they
use
a
computerized
retrieval
system
.

The
goal
is
to
develop
and
validate
a
taxonomy
of
query
types
and
relevance
judgments
.

The
project
will
then
extend
a
conventional
CD
-
ROM
based
biomedical
bibliographic
system
to
include
abstract
query
types
and
corresponding
search
strategies
,
and
qualitative
relevance
feedback
with
search
modi
cation
and
reindexing
strategies
,
and
will
perform
experiments
with
the
enhanced
system
.

An
important
scienti
c
result
of
this
work
will
be
a
better
understanding
of
the
interaction
between
researchers
and
the
literature
.

This
understand1
ing
will
take
the
form
of
a
taxonomy
of
the
kinds
of
queries
researchers
make
against
on
-
line
retrieval
systems
,
and
another
concrete
taxonomy
of
the
ways
in
which
retrieved
documents
can
either
satisfy
or
fail
to
satisfy
those
queries
.

The
impact
of
this
theoretical
understanding
extends
beyond
the
domain
of
biomedical
information
retrieval
systems
;
an
enhanced
model
of
relevance
will
be
signi
ca
nt
to
a
broad
range
of
research
in
both
Arti
cial
Intelligence
and
Information
Retrieval
.

Practically
,
the
work
will
result
in
a
functioning
and
evaluated
prototype
information
retrieval
system
that
embodies
both
abstract
query
types
and
qualitative
relevance
feedback
.

2
Background
and
Signi
cance

The
proposed
work
draws
upon
work
in
both
Information
Retrieval
(
henceforth
IR
and
Arti
cial
Intelligence

(
AI
From
IR
,
the
proposed
work
draws
speci
cally
upon
previous
work
in
intelligent
user
interfaces
,
and
broadly
upon
work
in
query
reformulation
,
user
modeling
,
the
theoretical
nature
of
relevance
,
the
analysis
of
user
needs
and
goals
,
and
relevance
feedback
.

From
AI
,
the
work
draws
speci
cally
upon
previous
work
in
case
-
based
reasoning
and
plan
repair
,
and
generally
upon
work
in
concept
acquisition
and
learning
.

The
proposed
work
is
signi
ca
nt
to
IR
because
it
will
provide
a
mechanism
for
representing
and
reasoning
about
user
goals
,
and
because
it
will
extend
the
eld
's
de
nitions
of
similarity
and
relevance
.

It
is
signi
ca
nt
to
AI
because
the
task
of
relevance
-
based
document
search
and
retrieval
provides
a
solid
testbed
for
theories
of
functional
relevance
,
storage
and
re
-
use
of
abstract
strategies
,
and
failure
-
based
modi
cation
.

2.1
Retrieval
and
relevance

The
purpose
of
a
bibliographic
information
retrieval
system
is
to
identify
published
articles
and
other
documents
likely
to
be
relevant
to
the
system
user
's
information
needs
.

Accordingly
,
an
intelligent
information
retrieval
system
must
be
able
to
represent
and
reason
about
the
user
's
needs
and
goals
,
to
estimate
the
relevance
of
a
particular
document
to
a
particular
set
of
needs
and
goals
,
and
to
e
ciently
search
its
memory
for
documents
likely
to
satisfy
these
relevance
criteria
.

Unfortunately
,
a
system
has
only
scanty
information
from
which
to
deduce
the
user
's
needs
and
goals
,
and
relevance
2
is
an
elusive
concept.1
In
a
typical
information
retrieval
system
,
e.g
Salton
and
McGill
,
1983
the
user
expresses
his
or
her
needs
and
goals
by
entering
a
query
,
either
in
free
text
or
in
some
specialized
query
language
,
and
the
system
attempts
to
satisfy
the
need
underlying
the
query
by
nding
documents
that
are
similar
to
or
that
match
,
along
one
or
more
dimensions
,
the
query
.

Matching
can
be
based
on
subject
taxonomies
controlled
indexing
vocabularies
or
upon
the
raw
text
of
the
queries
and
documents
.

Controlled
indexing
vocabularies
attempt
to
represent
the
semantic
content
of
documents
by
establishing
a
standardized
vocabulary
to
be
used
by
searchers
and
document
indexers
.

The
Dewey
Decimal
system
used
by
libraries
to
organize
books
on
shelves
is
one
example
of
such
a
subject
taxonomy
;
the
MeSH
indexing
terminology
and
the
current
Uni
ed
Medical
Language
System
project
of
the
National
Library
of
Medicine
[
UMLS
,
1991
;
Humphreys
and
Lindberg
,
1989
]
represent
another
.

Obviously
,
controlled
indexing
vocabularies
require
either
that
the
user
be
pro
cient
in
the
use
of
the
vocabulary
,
or
that
the
system
provide
some
assistance
.

A
deeper
conceptual
approach
is
typi
ed
in
the
system
described
by
[
Mauldin
,
1989
which
details
a
scheme
whereby
the
system
acquires
a
deep
and
structured
,
language
-
independent
conceptual
representation
of
each
of
the
documents
in
its
memory
.

It
responds
to
queries
by
extracting
a
deep
conceptual
representation
of
each
query
and
comparing
that
with
the
representations
in
memory
.

While
this
approach
probably
holds
the
most
promise
in
the
long
term
,
an
enormous
amount
of
e
ort
is
required
to
produce
a
system
capable
of
parsing
a
wide
variety
of
documents
into
a
deep
conceptual
representation
.

The
research
proposed
herein
will
focus
on
retrieval
techniques
usable
with
the
existing
body
of
on
-
line
documents
documents
for
which
no
deep
content
representation
is
available
.

Another
approach
attempts
to
use
the
full
text
of
the
document
to
determine
relevance
to
the
user
's
query
.

In
the
simplest
kind
of
full
-
text
document
search
,
the
system
ranks
documents
according
to
how
many
of
a
set
of
user
-
supplied
words
they
contain
.

In
practice
,
this
capability
is
usually
augmented
by
boolean
combination
and
word
proximity
operators
.

See
[
Salton
and
McGill
,
1983
]
and
the
review
in
[
Tenopir
,
1984
]
for
discussions
of
the
advantages
of
full
text
searching
over
systems
based
on
authors
,
titles
,
and
controlled
indexing
terms
.

1For
an
overview
of
relevance
from
the
point
of
view
of
the
information
retrieval
community
,
see
[
Saracevic
,
1975
]
or
[
Cooper
,
1971
]

A
failure
of
full
-
text
approaches
is
that
the
author
and
the
searcher
might
use
di
erent
words
to
describe
the
same
phenomenon
.

This
can
lead
to
surprisingly
low
recall
rates
[
Blair
and
Maron
,
1985
To
a
certain
extent
,
this
can
be
addressed
by
the
use
of
thesauri
to
augment
queries
to
include
with
synonyms
(
for
example
,
work
by
Furnas
and
others
,
reported
in
[
Dumais
,
1988
Thesauri
can
be
either
conventional
externally
supplied
or
generated
statistically
from
the
text
corpus
itself
as
in
the
latent
semantic
indexing
approach
[
Deerwester
et
al
1990
]
or
the
parallel
approach
described
in
[
Stan
ll
and
Kahle
,

1986
See
[
Salton
and
Buckley
,
1991
]
for
another
recent
example
of
work
in
the
area
of
enhanced
full
-
text
retrieval
.

Another
improvement
on
text
-
based
retrieval
that
begins
to
address
the
issue
of
representing
relationships
among
concepts
is
to
use
textual
units
that
are
larger
than
words
but
smaller
than
full
sentences
[
Lewis
and
Croft
,
1990
2.2
Beyond
similarity
:

User
goals
and
needs
The
problem
with
all
these
notions
of
relevance
is
that
in
fact
they
do
not
directly
measure
relevance
,
they
measure
various
forms
of
similarity
between
the
user
's
query
and
the
articles
stored
in
the
system
's
memory
.

Although
index
term
overlap
,
vocabulary
overlap
,
and
other
measurements
of
similarity
are
relatively
easy
to
compute
,
they
are
at
best
only
an
approximation
to
a
true
measure
of
relevance
:
what
the
user
ultimately
wants
is
not
a
document
that
is
similar
to
his
or
her
query
;
the
user
wants
documents
that
directly
address
his
or
her
information
needs
.

The
two
are
not
necessarily
the
same
.

Several
researchers
in
IR
have
focused
on
the
analysis
of
user
goals
.

In
[
Belkin
et
al
1982a
]
and
[
Belkin
et
al
1982b
for
example
,
the
authors
shift
the
focus
of
the
information
retrieval
task
away
from
similarity
and
towards
the
idea
of
resolving
the
user
's
\anomalous
state
of
knowledge
Brooks
et
al
1985
]
examines
the
role
of
user
goals
in
the
formulation
of
problem
statements
Croft
and
Thompson
,
1987
]
recognizes
the
utility
of
retrieving
documents
based
on
ful
llment
of
a
user
's
information
need
rather
than
based
on
similarity
Saracevic
and
Kantor
,
1988a
]
contains
another
analysis
of
user
goals
and
needs
.

More
speci
cally
in
the
medical
domain
,
the
NLM
\Critical
Incident
Technique
"
study
of
MEDLINE
[

Wilson
et
al
1989
]
attempts
to
build
a
taxonomy
of
the
di
erent
kinds
of
information
needs
with
which
users
approach
a
biomedical
bibliographic
information
retrieval
system
.

To
a
certain
extent
,
user
needs
and
goals
are
represented
implicitly
in
4
many
controlled
vocabularies
.

People
asking
about
a
disease
,
for
example
,
are
likely
to
have
a
goal
of
either
treating
,
diagnosing
,
predicting
the
prognosis
of
,
or
understanding
the
biomedical
mechanism
underlying
that
disease
,
and
these
categories
are
re
ected
in
the
indexing
terms
in
MeSH
.

The
UMLS
goes
further
,
to
provide
a
scheme
for
representing
,
for
example
,
the
possible
relationships
between
a
drug
and
a
disease
(
role
in
treatment
,
disease
as
a
side
-
e
ect
,
disease
precludes
use
of
drug
,
etc
These
relationships
are
represented
in
the
indexing
language
because
they
are
likely
to
satisfy
the
goals
with
which
users
approach
the
literature
.

While
this
prior
work
has
established
the
role
of
goals
in
information
retrieval
,
and
as
such
is
an
essential
underpinning
of
the
proposed
work
,
the
proposed
work
will
explore
in
concrete
,
computational
terms
how
a
system
should
taxonomize
or
characterize
user
goals
,
and
what
use
a
system
can
make
of
this
characterization
.

The
purpose
of
the
abstract
query
types
described
in
Section
4.2
below
is
to
make
what
is
implicit
in
current
indexing
vocabularies
explicit
.

The
goal
of
qualitative
relevance
feedback
,
described
in
Section
4.4
,
is
to
provide
the
system
with
a
mechanism
for
inferring
knowledge
about
the
user
's
needs
and
goals
,
by
learning
from
its
successes
and
failures
.

2.3
Iterative
search
and
relevance
feedback
A
key
feature
of
online
searching
is
that
searches
performed
by
librarians
and
experienced
searchers
tend
to
be
iterative
:
The
user
makes
an
initial
query
,
and
the
system
returns
some
documents
.

Generally
,
the
user
can
examine
the
returned
documents
and
reformulate
the
query
to
broaden
,
narrow
,
or
shift
the
search
.

Such
dialog
is
examined
in
[
Oddy
,
1977
]
as
a
possible
basis
for
information
retrieval
systems
Swanson
,
1980
]
presents
another
view
of
information
retrieval
as
an
iterative
,
dialog
-
like
process
.

Surprisingly
,
the
MEDLINE
utilization
survey
[
Wilson
et
al
1989
]
found
very
little
evidence
of
iterative
search
among
inexperienced
users
,
implying
that
a
powerful
technique
is
being
underutilized
.

One
reason
may
be
that
in
typical
systems
,
this
iterative
dialog
is
generally
implicit
although
the
experienced
user
is
attuned
to
the
possibility
of
using
the
results
of
any
given
search
as
a
basis
for
selecting
the
next
,
the
system
does
n't
take
any
note
of
this
iterative
dialog
.

It
does
n't
know
that
the
second
query
is
in
any
related
to
the
rst
,
or
even
that
it
was
entered
by
the
same
user
.

But
an
explicit
dialog
,
in
which
the
user
critiques
the
system
's
performance
and
the
system
suggests
query
modi
cation
strategies
,
is
a
powerful
5
mechanism
for
allowing
the
system
to
infer
the
user
's
information
needs
and
for
the
system
and
the
user
to
jointly
develop
a
query
likely
to
ful
ll
those
needs
.

Such
an
explicit
dialog
plays
an
essential
role
in
the
experimental
system
described
below
in
Section
4.5
.

One
explicit
technique
for
allowing
the
system
to
re
ne
queries
is
relevance
feedback
,
an
approach
described
in
[
Salton
,
1971
Using
relevance
feedback
,
a
system
responds
to
a
query
by
presenting
an
initial
set
of
documents
exhibiting
high
term
overlap
with
the
query
.

The
user
examines
a
subset
of
the
documents
and
marks
each
document
as
being
either
relevant
or
irrelevant
.

Using
this
feedback
,
the
system
formulates
a
new
query
,
typically
based
on
some
kind
of
inductive
generalization
across
the
documents
marked
by
the
user
as
relevant
.

Using
the
reformulated
query
,
the
system
retrieves
a
set
of
documents
having
high
term
overlap
with
the
\relevant
"
documents
.

The
process
can
be
repeated
iteratively
,
reportedly
yielding
improved
precision
and
recall
.

As
is
described
below
in
Section
4.4
,
our
proposal
de
nes
an
alternate
notion
of
relevance
feedback
by
treating
it
as
a
qualitative
,
rather
than
a
statistical
process
.

Instead
of
merely
judging
documents
as
\relevant
"
or
\irrelevant
the
user
has
a
mechanism
to
comment
on
why
the
document
is
or
is
not
relevant
or
what
aspect
of
the
document
is
relevant
.

This
feedback
allows
for
the
selection
of
appropriate
search
modi
cation
and
re
-
indexing
strategies
.

2.4
Intermediaries
,
strategies
and
reformulation
Several
researchers
in
IR
have
recognized
the
need
for
systems
that
play
some
of
the
role
traditionally
played
by
the
expert
search
intermediary
,
that
assist
users
by
:
Suggesting
appropriate
search
strategies
,
Assisting
the
user
with
the
system
's
controlled
indexing
vocabulary
,
or
O
ering
to
broaden
or
narrow
searches
by
including
or
excluding
terms
.

Examples
of
such
work
includes
the
query
reformulation
work
discussed
in
[
Smith
et
al
1989
the
CONIT
system
[
Marcus
,
1988
the
OAK
system
[
Meadow
et
al
1989
I3R
[
Croft
and
Thompson
,
1987
the
KISIR
system
under
development
in
Finland
[
Sormunen
,
1989
and
theoretical
work
described
in
[
Brooks
,
1987
Vickery
,
1988

6
While
the
proposed
work
draws
from
this
prior
work
,
it
di
ers
primarily
in
that
its
focus
is
on
an
explicit
notion
of
bibliographic
search
as
an
iterative
,
cooperative
process
of
debugging
a
partially
-
correct
plan
.

Accordingly
,
the
primary
focus
of
our
theoretical
work
will
be
on
representing
and
reasoning
about
user
goals
,
classes
of
plan
failures
,
and
recovery
or
repair
strategies
in
the
domain
of
bibliographic
search
.

2.5
Diagnosis
and
adaptation

An
important
piece
of
AI
background
for
the
proposed
research
is
the
use
of
failure
diagnoses
to
modify
abstract
knowledge
structures
.

Case
-
based
reasoning
(
see
[
Kolodner
,
1988
Hammond
,
1989a
]
for
a
variety
of
descriptive
papers
)
involves
the
use
of
stored
representations
of
prior
experiences
to
solve
novel
problems
.

To
solve
a
problem
,
a
case
-
based
reasoner
retrieves
from
memory
a
description
of
a
relevant
previously
solved
problem
and
applies
the
recalled
solution
to
the
current
problem
.

The
important
feature
here
is
that
previous
problems
are
seldom
exact
matches
for
new
problems
.

Accordingly
,
case
-
based
reasoners
must
diagnose
the
inappropriateness
of
the
retrieved
case
,
and
use
the
diagnosis
to
adapt
the
case
to
modify
it
to
t
the
current
situation
.

Examples
of
using
this
strategy
to
repair
plans
can
be
found
in
[
Hammond
,
1989b
]
or
[
Kolodner
et
al
1985
Using
the
strategy
to
debug
causal
explanations
can
be
found
in
[
Leake
,
1990
]
and
[
Kass
,
1990
described
below
in
Section
3.1
.

Our
proposed
research
draws
on
these
ideas
in
that
an
active
search
strategy
is
like
a
plan
that
is
being
debugged
.

Qualitative
relevance
feedback
provides
the
diagnostic
information
(
it
is
the
user
that
is
diagnosing
the
failure
in
this
case
rather
than
the
system
and
search
modi
cation
strategies
perform
the
adaptation
.

Supervised
category
formation

Another
relevant
area
of
prior
research
is
exempli
ed
by
the
protos
system
described
in
[
Bareiss
,
1989
Protos
learns
classi
catory
knowledge
from
examples
,
under
the
supervision
of
a
skilled
user
.

It
embodies
a
content
theory
of
category
formation
:
it
enumerates
a
number
of
speci
c
ways
in
which
a
category
judgment
can
be
in
error
,
and
it
provides
a
speci
c
,
wellconstrained
mechanism
whereby
the
user
can
explain
to
the
system
why
a
proposed
categorization
judgment
is
or
is
not
correct
.

The
system
can
use
7
these
user
-
provided
explanations
to
recover
from
incorrect
category
assignments
.

Although
protos
's
task
of
explicit
categorization
di
ers
from
the
task
of
incremental
search
and
retrieval
in
this
proposed
research
,
the
notion
of
a
vocabulary
for
the
user
to
critique
the
system
's
relevance
judgments
is
analogous
to
protos
's
notion
of
a
vocabulary
for
the
tutor
to
critique
the
system
's
categorizations
.

The
speci
c
content
and
the
underlying
functional
motivation
of
the
two
vocabularies
will
of
course
di
er
.
3

Progress
Report
Preliminary
Studies

The
proposed
work
represents
a
natural
extension
of
our
previous
work
on
memory
organization
,
indexing
,
and
retrieval
into
the
area
of
online
information
retrieval
.

Of
particular
relevance
is
work
on
memory
-
based
explanation
in
the
swale
project
,
and
work
on
integrating
feature
extraction
with
memory
search
in
the
anon
project
.

3.1
Retrieval
,
failure
diagnoses
and
modi
cation

The
task
of
the
swale
program
was
to
build
explanations
of
anomalous
events
.

Given
a
description
containing
the
facts
of
an
episode
,
swale
attempts
to
build
a
causally
coherent
framework
that
could
tie
those
facts
together
.

This
causal
framework
,
or
explanation
,
becomes
the
abstraction
under
which
the
episode
could
then
be
labeled
in
memory
.

The
program
is
described
in
[
Kass
et

al
1986
more
details
are
in
[
Schank
,
1986
Owens
,
1990
Kass
,
1990
and
[
Leake
,
1990

The
main
idea
behind
the
program
was
to
employ
a
case
-
based
approach
to
explanation
construction
.

Rather
than
building
an
explanation
by
chaining
through
a
space
of
causal
primitives
,
swale
built
explanations
by
retrieving
and
modifying
explanations
that
it
had
previously
used
and
stored
in
its
memory
.

The
knowledge
structures
used
by
swale
to
build
its
explanations
are
Explanation
Patterns
,
or
XPs
,
which
are
variablized
,
abstract
template
explanations
that
can
be
instantiated
with
the
particulars
of
a
given
situation
in
order
to
create
a
complete
,
concrete
explanation
.

swale
gets
its
name
from
one
example
of
the
type
of
anomalous
events
that
it
was
called
upon
to
explain
:
the
mysterious
death
of
the
three
-
year
-
old
racehorse
named
Swale
.

The
program
was
designed
to
imitate
the
behavior
of
subjects
in
an
informal
study
who
,
when
asked
to
explain
why
this
horse
had
died
at
the
peak
of
its
career
,
tended
to
be
reminded
of
existing
patterns
8
of
explanation
,
which
they
adapted
to
the
particulars
of
the
Swale
case
.

One
person
was
reminded
of
the
runner
and
author
Jim
Fixx
,
who
,
while
running
,
died
of
a
heart
attack
caused
by
a
congenital
heart
defect
.

Mapping
this
explanation
onto
the
Swale
case
,
the
subject
surmised
that
the
horse
had
had
a
latent
heart
problem
.

Other
individuals
were
reminded
of
the
plan
of
murdering
a
spouse
for
the
inheritance
or
insurance
money
;
still
others
recalled
the
idea
of
one
competitor
disabling
another
in
a
sporting
event
.

While
these
explanations
may
be
novel
and
creative
when
applied
to
the
particulars
of
the
Swale
story
,
each
in
fact
represents
a
common
pattern
of
causality
a
standard
explanation
that
can
be
reused
and
applied
to
a
wide
variety
of
experiences
Killed
for
the
insurance
money
"
or
\Killed
by
stress
-
induced
heart
attack
"
are
explanations
that
everyone
knows
,
has
heard
before
,
and
can
adapt
to
t
new
situations
.

swale
worked
by
retrieving
an
approximately
relevant
XP
,
by
diagnosing
any
failure
of
the
XP
to
exactly
t
the
current
situation
,
and
by
adapting
the
XP
modifying
it
based
on
the
failure
diagnosis
.

Each
of
these
processes
is
applicable
to
the
proposed
research
:
The
search
strategies
used
to
look
for
XPs
[
Owens
,
1990
]
are
similar
in
form
to
the
search
strategies
described
in
Section
4.3
below
.

The
failure
diagnoses
for
XP
inapplicability
[
Leake
,
1990
]
share
some
fundamental
characteristics
with
the
abstract
relevance
types
described
Section
4.4.1
.

And
nally
,
swale
's
XP
adaptation
strategies
[
Kass
,
1990
]
are
similar
to
the
search
modi
cation
strategies
of
Section
4.4.2
.

The
di
erence
between
these
three
aspects
of
swale
and
the
corresponding
aspects
of
the
current
project
is
that
while
swale
's
strategies
were
organized
around
the
functionality
of
generating
causal
explanations
,
the
proposed
project
's
strategies
are
to
be
organized
around
the
functional
requirements
imposed
by
our
developing
theory
of
document
relevance
.

3.2
Criteria
for
indexing
vocabularies
In
work
on
the
anon
project
Owens
,
1990
Owens
,
1988
Owens
,
1989b
Owens
,
1989a
we
explored
the
functional
considerations
that
constrain
the
choice
of
indexing
terms
in
a
system
designed
to
diagnose
and
reason
about
plan
failures
by
retrieving
and
applying
XP
-
like
knowledge
structures
.

The
project
also
explored
the
use
of
a
parallel
retrieval
algorithm
to
explicitly
manage
the
tradeo
between
the
inferential
cost
of
acquiring
descriptive
information
about
the
world
and
the
utility
of
that
information
.

Although
the
functional
criteria
that
apply
in
the
task
of
document
retrieval
are
di
erent
than
those
that
apply
in
anon
's
task
of
reasoning
about
9
plan
failures
,
the
same
principles
of
selecting
appropriate
functional
criteria
and
applying
them
to
the
selection
of
index
terms
applies
in
both
cases
.

Although
the
proposed
research
will
initially
use
already
-
existing
indexing
vocabulary
(
e.g.
MeSH
the
principles
of
identifying
useful
indexing
terms
will
be
brought
to
bear
on
the
development
of
re
-
indexing
strategies
described
in
Section
4.4.3
.

4
Research
Design
and
Methods

The
project
has
three
fundamental
components
:
data
gathering
,
system
development
,
and
evaluation
.

Data
gathering
and
system
development
will
be
interleaved
,
as
described
below
.

4.1
Data
gathering
The
purpose
of
the
initial
data
gathering
is
to
functionally
taxonomize
:
the
broad
classes
of
interactions
that
exist
between
information
system
users
and
the
literature
,
the
search
strategies
that
appear
to
be
appropriate
for
each
class
of
interaction
,
the
qualitative
judgments
that
users
make
about
the
relevance
or
lack
thereof
of
a
retrieved
document
to
the
current
information
need
,
and
the
available
strategies
for
broadening
,
narrowing
,
or
shifting
the
focus
of
a
query
in
response
to
relevance
judgments
.

4.1.1
Method

The
subjects
in
this
initial
study
are
residents
,
attending
physicians
,
and
biomedical
researchers
at
the
University
of
Chicago
hospitals
,
who
will
be
studied
in
their
use
of
existing
retrieval
systems
in
the
course
of
their
normal
research
and
practice
.

Initially
,
data
will
be
gathered
by
interviewing
users
before
,
during
,
and
after
interactive
sessions
using
existing
information
retrieval
systems
.

As
the
experimental
system
is
developed
(
see
Section
4.5
below
it
will
provide
the
capability
to
gather
detailed
data
about
individual
search
sessions
:
how
queries
were
formulated
,
which
retrieved
citation
records
were
examined
,
printed
,
or
rejected
as
irrelevant
and
why
,
and
how
10
much
time
the
user
spent
on
each
step
of
the
process
.

But
the
interview
technique
will
remain
critical
as
a
means
of
establishing
the
user
's
initial
information
need
.

An
example
of
the
use
of
interviews
to
establish
a
taxonomy
of
information
needs
and
goals
can
be
found
in
the
NLM
's
extensive
MEDLINE
utilization
survey
[
Wilson
et
al
1989

Our
technique
will
di
er
,
in
that
users
will
be
observed
and
interviewed
during
the
search
process
rather
than
later
,
and
that
objective
observations
of
the
users
'
activities
will
be
a
part
of
the
record
of
a
session
.

The
goal
of
the
interviewers
will
be
not
only
to
establish
a
taxonomy
of
goals
,
but
to
observe
the
relationship
between
di
erent
classes
of
information
needs
and
the
appropriate
search
strategies
.

4.1.2
Representativeness
There
is
always
a
tradeo
between
the
breadth
of
a
sample
group
and
the
depth
with
which
that
group
can
be
studied
.

Some
researchers
,
e.g
Meadow
et
al
1989
suggest
that
patterns
of
searching
vary
so
widely
that
an
information
retrieval
system
should
not
be
designed
around
the
general
case
,
but
rather
should
be
designed
around
the
typical
goals
and
information
seeking
style
of
a
speci
c
target
user
population
.

Our
approach
is
a
compromise
.

Our
subjects
are
in
one
speci
c
area
of
medicine
:
radiation
physics
and
radiation
oncology
.

As
such
,
they
can
be
expected
to
have
an
idiosyncratic
set
of
information
needs
and
searching
styles
.

On
the
other
hand
,
because
the
subjects
are
at
various
stages
in
their
educational
and
professional
careers
,
we
expect
to
encounter
a
reasonably
representative
sample
of
query
types
.

Working
with
this
small
group
will
allow
us
to
go
beyond
a
taxonomic
survey
of
goals
,
to
develop
a
detailed
pairing
of
stereotypical
information
needs
with
successful
search
strategies
.

We
also
expect
to
be
able
to
track
variations
in
queries
across
tasks
within
an
individual
as
well
as
across
individuals
.

We
can
assess
the
representativeness
of
our
sample
by
comparing
the
taxonomy
of
goals
we
observe
with
the
taxonomy
reported
in
the
broader
MEDLINE
utilization
survey
.

One
improbable
but
possible
result
is
that
the
data
we
gather
will
be
completely
idiosyncratic
to
radiation
oncology
.

Even
in
this
unlikely
case
,
however
,
we
will
have
developed
a
methodology
for
pairing
stereotypical
information
needs
with
appropriate
search
strategies
,
a
methodology
that
could
be
applied
to
other
domains
.

Even
if
the
abstract
query
types
and
search
strategies
are
completely
idiosyncratic
,
the
system
resulting
from
the
11
work
described
in
Section
4.5
below
could
be
utilized
,
albeit
with
a
di
erent
knowledge
base
of
goals
and
search
strategies
,
in
another
domain
.

4.1.3
Functional
orientation

This
research
di
ers
from
the
past
user
studies
(
e.g
Brooks
et
al
1985
Saracevic
and
Kantor
,
1988b
Belkin
et
al
1990
Fenichel
,
1981
and
other
examinations
of
searching
styles
and
interactions
(
e.g
Oddy
,
1977

Borgman
et
al
1985
]
because
of
its
focus
on
functional
analysis
.

The
purpose
of
the
data
gathering
is
not
to
build
a
general
user
model
,
nor
is
it
to
build
a
purely
descriptive
taxonomy
of
user
goals
or
searching
styles
.

Instead
,
the
goal
is
to
develop
a
concrete
and
computationally
relevant
library
of
query
types
,
search
strategies
,
relevance
types
,
search
modi
cation
strategies
,
and
re
-
indexing
strategies
,
each
of
which
are
described
below
.

4.2
Abstract
query
types
An
abstract
query
type
characterizes
the
nature
of
a
particular
search
.

At
the
broadest
,
most
general
level
,
it
would
re
ect
the
the
di
erence
between
one
researcher
,
who
might
be
surveying
a
subspecialty
to
determine
the
\lay
of
the
land
"
prior
to
initiating
research
in
a
new
area
,
and
another
,
who
might
be
looking
for
an
answer
to
a
speci
c
factual
question
.

A
third
might
be
looking
for
articles
that
discuss
a
speci
c
laboratory
methodology
.

A
fourth
might
be
looking
for
articles
that
con
rm
or
refute
a
particular
hypothesis
.

A
fth
might
be
interested
in
keeping
up
with
current
research
by
searching
for
anything
new
from
a
particular
group
of
researchers
or
in
a
particular
narrowly
-
de
ned
area
.

These
abstract
query
types
are
functionally
relevant
because
they
determine
what
further
information
should
be
elicited
from
the
user
,
how
search
should
proceed
,
what
kind
of
information
should
be
presented
to
the
user
,
and
how
the
system
and/or
user
can
determine
when
the
query
has
been
satis
ed
.

Strategic
information
about
how
to
deal
with
queries
of
a
particular
type
can
be
re
-
used
across
queries
that
otherwise
have
little
or
nothing
in
common
.

In
the
case
of
human
-
assisted
searching
,
an
important
function
of
the
experienced
librarian
or
search
intermediary
is
to
determine
the
appropriate
categorization
of
the
current
query
,
and
thus
to
assist
the
user
in
choosing
and
implementing
appropriate
search
strategies

See
,
e.g.
Marcus
's
comparison
of
online
with
human
intermediaries
Marcus
,
1983

As
the
exper12
imental
system
is
developed
,
it
will
embody
abstract
query
types
as
those
types
are
identi
ed
.

Initially
,
the
user
will
enter
the
appropriate
abstract
query
type
corresponding
to
the
current
query
by
choosing
from
a
menu
;
a
longer
term
goal
is
to
determine
how
the
system
might
correctly
infer
the
appropriate
query
type
directly
from
the
query
.

When
the
system
assigns
a
query
to
an
abstract
query
type
,
it
can
select
appropriate
search
strategies
as
described
below
.

The
data
-
gathering
phase
of
the
project
will
identify
abstract
query
types
at
a
level
of
detail
more
speci
c
than
the
sweeping
generalizations
described
above
,
yet
more
general
than
individual
,
content
-
speci
c
queries
.

4.3
Search
strategies

The
purpose
behind
identifying
abstract
query
types
is
to
pair
them
with
search
strategies
.

A
query
corresponding
to
a
survey
of
the
literature
dictates
a
di
erent
search
strategy
than
a
query
corresponding
to
a
speci
c
factual
question
.

As
we
develop
an
understanding
of
each
abstract
query
type
,
we
will
associate
the
query
type
with
one
or
more
search
strategies
in
the
experimental
system
.

The
user
's
choice
of
a
particular
abstract
query
type
will
initiate
a
dialog
between
the
system
and
the
user
wherein
the
system
can
gather
enough
information
about
the
query
to
search
e
ectively
;
the
nature
of
the
dialog
will
depend
upon
the
abstract
query
type
selected
.

Examples
of
search
strategies
can
be
found
in
the
[
Bates
,
1979
]
and
[
Smith
et
al
1989
and
,
indirectly
,
in
the
work
on
question
transformations
described
in
[
Kolodner
,
1984

The
search
strategies
developed
in
conjunction
with
this
project
will
be
closely
tied
with
the
abstract
query
types
described
above
.

4.4
Qualitative
relevance
feedback
The
concept
of
qualitative
relevance
feedback
is
at
the
core
of
this
work
.

It
is
the
mechanism
whereby
the
user
communicates
with
the
system
about
the
e
ectiveness
of
the
current
search
,
and
the
direction
in
which
it
should
move
.

It
is
the
information
the
system
uses
to
re
ne
its
current
search
,
and
,
possibly
,
to
modify
the
indexing
of
documents
in
its
library
.

While
the
work
described
in
Section
2.3
has
demonstrated
the
e
ectiveness
of
statistical
relevance
feedback
in
incremental
query
re
nement
,
previous
models
of
relevance
feedback
have
allowed
the
user
to
rate
the
relevance
of
documents
o
ered
by
the
system
only
along
one
dimension
,
and
gener13
ally
only
as
either
\relevant
"
or
\not
relevant
Much
more
information
is
available
from
the
user
,
and
the
purpose
of
qualitative
relevance
feedback
is
to
make
this
information
usable
by
the
system
.

Presenting
an
appropriate
or
relevant
citation
record
to
the
user
constitutes
a
success
on
the
system
's
part
;
an
inappropriate
or
irrelevant
document
a
failure
.

Successes
and
failures
provide
opportunities
for
both
short
-
term
and
long
-
term
learning
,
and
qualitative
relevance
feedback
provides
the
mechanism
for
that
learning
.

Relevance
judgments
can
apply
to
a
speci
c
document
,
or
to
the
set
of
documents
retrieved
in
response
to
a
query
.

An
example
of
the
former
is
:
This
document
discusses
the
disease
in
which
I
am
interested
,
but
it
is
about
etiology
rather
than
diagnosis
.

Examples
of
the
latter
include
the
\Too
broad
"
and
\Too
narrow
"
judgments
implicit
in
[
Smith
et
al
1989
or
the
more
idiosyncratic
:
This
set
of
documents
appears
to
contain
two
unrelated
clusters
,
because
an
ambiguous
search
term
was
used
.

Instead
of
giving
the
user
only
one
possible
comment
to
make
about
the
relevance
of
a
particular
document
to
his
or
her
query
,
qualitative
relevance
feedback
allows
the
user
more
exibility
in
critiquing
each
pro
ered
document
.

Instead
of
using
the
results
of
feedback
to
drive
a
statistical
modi
cation
of
the
current
search
,
qualitative
relevance
judgments
can
be
paired
with
distinct
,
qualitative
search
modi
cation
and
re
-
indexing
strategies
,
as
described
below
.

4.4.1
Relevance
types
By
its
nature
,
qualitative
relevance
feedback
is
open
-
ended
.

One
of
the
goals
of
this
research
is
to
make
it
less
so
,
for
both
the
user
and
the
system
.

Preliminary
interviews
with
information
system
users
have
suggested
that
the
relevance
of
a
document
to
a
query
can
usually
be
characterized
by
one
of
a
relatively
small
number
of
recurring
critiques
.

A
document
might
be
too
broadly
or
narrowly
focused
;
it
might
deal
with
the
correct
subject
domain
but
be
uninteresting
methodologically
or
vice
-
versa
;
it
might
deal
with
a
relevant
anatomical
structure
,
process
,
or
chemical
but
be
irrelevant
in
other
regards
;
it
may
be
a
case
report
when
a
theoretical
article
or
review
is
desired
or
vice
-
versa
.

If
the
data
-
gathering
phase
of
the
project
is
successful
in
identifying
a
relatively
small
,
stable
set
of
relevance
judgments
that
can
be
succinctly
,
14
meaningfully
named
and
described
,
then
those
relevance
judgments
can
be
o
ered
to
the
user
on
a
menu
or
a
checklist
.

Using
such
a
pre
-
determined
vocabulary
to
critique
the
system
's
retrieval
performance
is
expected
have
two
e
ects
:
It
will
allow
the
system
to
respond
computationally
to
the
relevance
judgments
by
attaching
one
or
more
search
modi
cation
strategies
(
below
)
to
each
relevance
judgment
.

It
will
also
assist
the
inexperienced
user
by
presenting
a
concise
list
of
possible
ways
in
which
the
current
search
can
be
modi
ed
,
many
of
which
are
likely
to
be
options
that
the
user
did
not
consider
.

As
is
apparent
in
the
brief
sample
above
,
some
types
of
relevance
judgments
appear
to
be
domain
-
independent
(
too
broad
,
too
narrow
while
others
appear
to
be
more
speci
c

(
case
reports
versus
laboratory
results

Both
domain
-
speci
c
and
domain
-
independent
relevance
types
will
be
developed
.

Search
modi
cation
strategies

In
the
course
of
an
iterative
retrieval
session
,
an
expert
user
often
uses
speci
c
strategies
to
reformulate
a
query
:
broadening
it
by
including
synonyms
or
more
general
terms
,
narrowing
it
by
using
more
speci
c
terms
or
by
coordinating
di
erent
types
of
terms
,
recovering
from
an
ambiguous
or
otherwise
problematic
query
by
substituting
one
term
for
another
,
or
following
a
lead
by
incorporating
an
index
term
from
a
retrieved
article
into
a
subsequent
query
.

An
intelligent
search
intermediary
system
can
incorporate
search
modication
strategies
within
it
,
so
that
the
system
can
employ
the
strategies
on
behalf
of
the
user
,
even
if
the
user
is
an
inexperienced
searcher
,
unfamiliar
with
the
strategies
Smith
et
al
1989
for
example
,
describes
a
system
that
o
ers
to
broaden
and
narrow
searches
by
addition
and
deletion
of
conjuncts
,
or
by
substitution
of
more
general
or
more
speci
c
terms
selected
from
a
hierarchy
.

Our
goal
is
to
go
beyond
the
broadening
and
narrowing
strategies
;
to
capture
a
more
sharply
-
focused
set
of
strategies
that
can
be
invoked
in
response
to
a
user
's
relevance
judgments
.

A
document
might
be
indicated
by
the
user
as
being
relevant
because
of
the
methodology
it
discusses
.

This
feedback
can
cause
the
system
to
modify
the
search
by
increasing
the
focus
on
index
terms
associated
with
that
article
and
relevant
to
methodology
.

Some
other
document
might
be
judged
partially
irrelevant
because
,
although
it
deals
with
a
relevant
biochemical
process
,
it
focuses
on
an
irrelevant
anatomical
structure
.

In
response
to
this
critique
,
the
system
can
continue
searching
with
15
additional
focus
on
the
index
terms
associated
with
this
document
but
not
pertaining
to
anatomy
.

As
the
data
gathering
phase
of
this
project
identi
es
useful
search
modi
cation
strategies
,
they
will
be
encoded
procedurally
,
and
made
available
to
users
of
the
experimental
system
.

This
approach
depends
upon
representing
within
the
system
some
knowledge
about
the
content
and
semantics
of
the
terms
used
to
index
documents
.

While
the
long
-
range
goal
for
the
eld
must
be
a
deep
representation
of
document
knowledge
content
,
we
expect
to
gain
considerable
leverage
from
shallow
and
intermediate
representations
such
as
,
for
example
,
the
UMLS
Semantic
Net
[
UMLS
,
1991
;
Humphreys
and
Lindberg
,
1989

Many
of
the
search
modi
cation
strategies
are
expected
to
use
the
information
from
the
UMLS
and
from
the
semantic
net
:
the
details
of
the
strategies
are
likely
to
involve
moving
up
,
down
,
or
laterally
within
the
net
,
or
by
coordinating
search
terms
of
di
erent
semantic
types
.

An
example
of
a
modi
cation
strategy
using
the
semantic
information
represented
in
the
UMLS
was
presented
by
McCray2
,
in
which
a
system
might
say
:
You
have
mentioned
a
chemical
and
a
disease
in
your
query
.

Are
you
interested
in
check
all
that
apply
The
causal
role
of
that
chemical
in
the
disease
?

The
therapeutic
use
of
that
chemical
in
treating
the
disease
?

A
history
of
exposure
to
that
chemical
as
an
aid
in
diagnosing
that
disease
?

The
presence
of
that
chemical
in
the
organism
as
an
aid
in
diagnosing
that
disease
?

Presence
of
that
disease
as
a
counterindication
for
the
therapeutic
use
of
that
chemical
?

Although
the
system
does
not
have
a
deep
representation
of
the
semantic
content
of
the
articles
nor
of
the
query
,
it
can
use
the
user
's
answer
to
this
query
,
even
with
an
existing
shallow
representation
such
as
the
present
MeSH
terms
,
to
qualify
,
expand
,
or
modify
query
terms
and
thereby
to
yield
improved
recall
and
precision
.

2Unpublished
manuscript
presented
at
the
1991
NSF
invitational
workshop
on
\future
directions
in
information
retrieval
"
16
4.4.3
Re
-
indexing
strategies
While
search
modi
cation
strategies
alter
the
course
of
the
current
search
in
response
to
user
critiques

,
re
-
indexing
strategies
allow
the
system
to
learn
for
future
searches
.

While
some
attention
has
been
paid
to
statistical
models
of
re
-
indexing
[
Gordon
,
1985
our
approach
is
to
take
a
qualitative
,
knowledge
-
intensive
view
of
re
-
indexing
.

Like
search
modi
cation
strategies
,
re
-
indexing
strategies
are
paired
with
the
speci
c
relevance
assessment
types
.

To
a
given
explanation
of
why
a
document
is
or
is
not
relevant
to
the
current
query
,
there
corresponds
a
re
-
indexing
strategy
designed
to
modify
the
indexing
of
the
system
's
documents
so
that
,
in
response
to
similar
queries
in
the
future
,
the
documents
that
were
relevant
but
not
retrieved
on
the
rst
pass
,
or
irrelevant
and
retrieved
on
the
rst
pass
,
will
be
handled
correctly
in
the
future
.

Successful
development
of
re
-
indexing
strategies
presents
a
number
of
challenges
.

How
,
for
example
,
should
the
system
deal
with
multiple
,
idiosyncratic
users
of
the
database
,
each
tending
to
pull
the
indexing
in
a
slightly
di
erent
direction
?

Should
the
system
learn
explicit
user
pro
les
?

Should
it
maintain
private
virtual
copies
of
the
database
for
each
user
or
should
experience
gained
from
online
sessions
gradually
a
ect
the
entire
public
database
?

It
is
too
early
to
answer
these
questions
,
because
the
answers
depend
upon
the
speci
c
nature
of
the
relevance
types
and
the
re
-
indexing
strategies
associated
with
them
.

Development
of
re
-
indexing
strategies
will
be
deferred
to
a
point
in
the
project
at
which
time
we
expect
to
have
more
experience
with
relevance
types
and
search
modi
cation
strategies
.

The
experimental
system

An
important
part
of
this
research
is
to
provide
,
as
early
as
possible
,
a
stable
hardware
and
software
platform
that
will
enable
users
to
perform
real
literature
searches
using
the
system
.

This
experimental
system
will
provide
a
mechanism
to
collect
data
about
queries
,
query
types
,
and
relevance
judgments
;
it
will
also
provide
the
background
against
which
abstract
query
types
and
qualitative
relevance
feedback
can
be
implemented
and
tested
.

System
functionality
can
be
divided
into
the
\front
end
which
in
this
case
includes
not
only
the
user
interface
but
also
determination
of
query
types
,
selection
of
search
strategies
,
and
the
qualitative
relevance
feedback
mechanism
;
and
the
\back
end
which
includes
the
database
itself
and
the
17
search
engine
that
responds
to
speci
c
requests
from
the
front
end
.

As
much
as
possible
,
we
will
use
existing
,
commercially
available
technology
for
the
back
end
,
which
,
initially
,
will
be
based
on
a
commercial
vendor
's
CD
-
ROM
(
optical
disk
)
edition
of
the
MEDLINE
biomedical
literature
database
and
the
vendor
's
associated
retrieval
engine
.

The
front
end
,
which
we
will
develop
,
will
present
to
the
user
a
productionquality
graphical
interface
,
suitable
for
use
by
non
-
specialists
.

Initially
,
the
front
end
will
approximately
resemble
existing
state
-
of
-
the
-
art
end
-
user
search
products
.

But
,
it
will
provide
a
mechanism
to
add
abstract
query
types
,
search
strategies
,
relevance
judgments
,
and
search
modi
cation
strategies
as
they
are
developed
.

The
fundamental
nature
of
the
user
interface
will
not
change
as
new
functionality
is
added
;
what
will
change
is
the
available
set
of
critiques
and
strategies
that
the
user
can
select
.

Implementing
re
-
indexing
strategies
will
likely
require
a
change
to
the
system
's
back
end
,
and
therefore
is
scheduled
for
later
in
the
project
.

The
user
will
interact
with
the
system
in
the
conventional
way
:
by
entering
text
queries
containing
a
mixture
of
text
terms
and
controlled


-DOCSTART-

identity
of
the
speaker
.

The
area
of
speaker
identification
is
concerned
with
extracting
the
identity
of
the
person
speaking
the
utterance
.

As
speech
interaction
with
computers
becomes
more
pervasive
in
activities
such
as
the
telephone
,
financial
transactions
and
information
retrieval
from
speech
databases
,
the
utility
of
automatically
identifying
a
speaker
is
based
solely
on
vocal
characteristic
.

This
paper
emphasizes
on
text
dependent
speaker
identification
,
which
deals
with
detecting
a
particular
speaker
from
a
known
population
.

The
system
prompts
the
user
to
provide
speech
utterance
.

System
identifies
the
user
by
comparing
the
codebook
of
speech
utterance
with
those
of
the
stored
in
the
database
and
lists
,
which
contain
the
most
likely
speakers
,
could
have
given
that
speech
utterance
.

The
speech
signal
is
recorded
for
N
speakers
further
the
features
are
extracted
.

Feature
extraction
is
done
by
means
of
LPC
coefficients
,
calculating
AMDF
,
and
DFT
.

The
neural
network
is
trained
by
applying
these
features
as
input
parameters
.

The
features
are
stored
in
templates
for
further
comparison
.

The
features
for
the
speaker
who
has
to
be
identified
are
extracted
and
compared
with
the
stored
templates
using
Back
Propogation
Algorithm
.

Here
,
the
trained
network
corresponds
to
the
output
;
the
input
is
the
extracted
features
of
the
speaker
to
be
identified
.

The
network
does
the
weight
adjustment
and
the
best
match
is
found
to
identify
the
speaker
.

The
number
of
epochs
required
to
get
the
target
decides
the
network
performance
.


-DOCSTART-

Characterizing
the
structure
of
knowledge
,
the
evolution
of
research
topics
,
and
the
emergence
of
topics
has
always
been
an
important
part
of
information
science
(
IS
Our
previous
scientometric
review
of
IS
provided
a
snapshot
of
this
fast
-
growing
field
up
to
the
end
of
2008
.

This
new
study
aims
to
identify
emerging
trends
and
new
developments
appearing
in
the
subsequent
7574
articles
published
in
10
IS
journals
between
2009
and
2016
,
including
20,960
references
.

The
results
of
a
document
co
-
citation
analysis
show
great
changes
in
the
research
topics
in
the
IS
domain
.

The
positions
of
certain
core
topics
found
in
the
previous
study
,
namely
,
information
retrieval
,
webometrics
,
and
citation
behavior
,
have
been
replaced
by
scientometric
indicators
(
H
-
index
citation
analysis
(
citation
performance
and
bibliometrics
scientific
collaboration
,
and
information
behavior
in
the
most
recent
period
of
2009â€“2016
.

Dual
-
map
overlays
of
journals
show
that
the
knowledge
base
of
IS
research
has
shifted
considerably
since
2010
,
with
emerging
topics
including
scientific
evaluation
indicators
,
altmetrics
,
science
mapping
and
visualization
,
bibliometrics
,
citation
analysis
,
and
scientific
collaboration
.


-DOCSTART-

For
thousands
of
years
,
libraries
have
allowed
humanity
to
collect
and
organize
data
and
information
,
and
to
support
the
discovery
and
communication
of
knowledge
,
across
time
and
space
.

Coming
together
in
this
Internet
Age
,
the
world
's
societies
have
extended
this
process
to
span
from
the
personal
to
the
global
,
as
the
concepts
,
practices
,
systems
,
and
services
related
to
Library
and
Information
Science
unfold
through
digital
libraries
.

Scientists
,
scholars
,
teachers
,
learners
,
and
practitioners
of
all
kinds
benefit
from
the
distributed
and
collaborative
knowledge
environments
that
are
at
the
heart
of
the
digital
library
movement
.

Digital
libraries
thus
encompass
the
dimensions
in
the
5S
Framework
:

Societies
,
Scenarios
,
Spaces
,
Streams
,
and
Structures
.

To
clarify
this
approach
,
we
explain
the
role
of
meta
-
models
,
such
as
of
a
minimal
digital
library
(
DL
and
of
more
specialized
(
discipline
-
oriented
)
DLs
,
such
as
archeological
DLs
.

We
illustrate
how
suitable
knowledge
environments
can
be
more
easily
prepared
as
instances
of
these
meta
-
models
,
resulting
in
usable
and
useful
DLs
,
including
for
education
,
computing
,
and
archaeology
.


-DOCSTART-

During
the
last
three
years
,
we
have
developed
and
described
components
of
ELBook
,
a
semantically
based
information
-
retrieval
system
[
14
Using
these
components
,
domain
experts
can
specify
a
query
model
,
indexers
can
use
the
query
model
to
index
documents
,
and
end
-
users
can
search
these
documents
for
instances
of
indexed
queries
.

Project
Goals
and
Design

The
goal
of
our
project
is
to
make
it
easier
to
find
crucial
information
in
online
medical
resources
.

As
more
full
text
material
becomes
available
on
the
World
Wide
Web
,
there
is
an
increased
need
to
develop
better
search
mechanisms
.

In
time
-
critical
medical
situations
,
clinicians
need
to
be
able
to
pinpoint
answers
to
diagnostic
and
treatment
questions
.

We
have
developed
novel
methods
for
indexing
and
searching
medical
textbooks
.

The
methods
work
by
describing
questions
that
the
material
was
written
to
answer
.

An
example
question
is
"
What
antimicrobial
agent
can
be
used
in
the
treatment
of
organism
<
E.Coli
>
in
patients
with
infection
<
meningitis
>
with
underlying
disease
<
renal
failure
>
or
special
condition
<
allergy
to
penicillin
Our
goal
was
to
associate
these
questions
with
passages
of
textbooks
(
i.e
index
documents
with
these
questions
and
then
retrieve
appropriate
passages
depending
on
specific
questions
that
users
pose
to
ELBook
.

We
first
built
a
specialized
search
engine
that
matches
the
questions
asked
by
the
user
to
indexed
questions
in
documents
.

Next
,
we
developed
ISAID
,
an
indexing
tool
that
is
able
to
suggest
which
questions
are
being
answered
by
specific
portions
of
the
text
of
documents
.

Users
of
this
tool
can
fill
in
appropriate
values
for
concepts
such
as
organism
or
infection
in
such
questions
.

The
indexing
process
is
semi
-
automated
because
of
the
difficulties
of
computer
-
based
natural
language
understanding
:
a
human
is
required
to
review
automatically
generated
,
provisional
markup
for
correctness
and
completeness
.

We
have
reported
the
performance
of
knowledge
-
based
natural
language
processing
techniques
to
identify
domain
concepts
and
relations
in
full
text
sources
[
3,4
These
methods
used
the
UMLS
to
correctly
identify
semantic
types
(
e.g
amoxicillin
is
a
drug
)
for
about
2/3
of
the
medical
concepts
in
the
medical
textbook
chapters
we
have
tested
.

In
addition
to
the
specialized
search
engine
and
ISAID
,
we
developed
QueryEditor
,
a
knowledgeacquisition
tool
that
creates
and
maintains
the
description
of
the
queries
.

Once
the
queries
are
described
,
the
user
can
generate
the
templates
for
the
indexing
tool
and
the
HTML
queries
for
the
search
engine
.

Using
the
query
editor
,
we
have
described
a
set
of
core
medical
questions
that
can
be
used
as
the
basis
for
deriving
question
sets
for
more
specialized
areas
of
medicine
,
such
as
infectious
disease
,
patient
education
,
or
military
medicine
.


-DOCSTART-

The
term
of
big
,
data
is
an
important
concept
that
represents
the
exponential
growth
of
volume
and
variety
of
data
collection
,
one
of
the
advantages
of
this
technology
is
the
ability
of
treating
heterogeneous
data
,
such
as
textual
documents
,
also
as
its
name
indicates
â€˜
big
dataâ€™
refers
to
the
huge
volume
of
data
counted
by
petabytes
which
implies
an
information
retrieval
extension
to
help
users
to
find
their
need
this
extension
must
incorporate
other
protection
against
existent
threats
.

In
fact
,
big
data
services
such
as
cloud
computing
do
keep
traces
about
user
activities
and
queries
,
which
compromise
the
user
privacy
and
can
offer
useful
information
to
network
hackers
that
attack
users
or
even
cloud
server
to
adapt
or
personalize
their
platforms
without
the
user
â€™s
agreement
,
and
maybe
the
most
known
attack
can
be
man
-
in
-
the
-
middle
during
a
storage
or
extracting
data
session
between
a
user
and
a
cloud
server
for
this
cause
,
the
need
to
a
secure
protocol
represent
a
high
necessity
which
gives
birth
to
the
concept
of
Private
Information
Retrieval
(
PIR
as
the
authors
mention
before
,
one
of
the
checks
is
the
vast
mass
of
data
that
hinders
the
correct
handling
of
data
and
increases
the
error
rate
for
retrieving
a
relevant
information
,
for
that
the
use
of
new
techniques
and
approaches
that
allow
the
improvement
of
retrieval
models
over
this
kind
of
services
is
an
important
case
to
be
processed
.

In
this
purpose
,
the
authors
introduce
a
new
proposition
called
Meta
-
heuristic
Privet
Information
Retrieval
(
M
-
PIR
)
in
order
to
benefit
from
the
success
of
meta
-
heuristics
methods
and
improve
the
efficiency
of
PIR
protocols
in
term
of
returned
information
;
to
better
meet
the
needs
of
users
,
they
use
a
bag
of
word
for
the
text
representation
,
TFIDF
as
weighting
for
the
digitalization
,
the
benchmarking
MEDLINE
corpus
for
the
experimentation
and
panoply
of
validation
tools
(
Recall
,
Precision
,
F
-
measure
and
Entropy
)
for
the
evaluation
of
our
results
.

So
that
the
paper
is
over
the
application
of
a
meta
-
heuristics
algorithms
on
a
set
of
PIR
protocols
using
a
multitude
of
cryptographic
schemes
in
order
to
study
the
influence
of
these
schemes
on
quality
of
results
.

Application
of
Meta
-
Heuristics
Methods
on
PIR
Protocols

Over
Cloud
Storage
Services


-DOCSTART-

A
so
-
called
"
reformed
"
degree
course
in
"
Computer
Science
and
Communications
Engineering
"
has
started
at
the
Gerhard
Mercator
University
of
Duisburg
in
the
winter
semester
of
1997
.

This
degree
course
is
being
financed
by
the
"
Federal
Ministry
of
Education
,
Science
,
Research
and
Technology
"
and
is
aimed
at
students
of
all
nationalities
.

The
course
will
include
a
number
of
innovations
within
the
German
university
system
:
lectures
will
be
held
in
English
and
German
,
holders
of
related
-
subject
Bachelor
degrees
will
be
able
to
enrol
in
a
higher
semester
.

German
students
will
have
to
spend
a
mandatory
period
abroad
and
there
will
be
a
choice
of
final
qualification
:
either
a
German
Diplom
or
an
international
Master
's
degree
.

Owing
to
its
reformatory
character
,
the
degree
course
will
in
itself
become
an
object
of
study
during
its
five
-
year
pilot
phase
.

A
further
aim
is
to
develop
and
gain
experience
in
applying
new
teaching
concepts
.

This
paper
presents
the
course
curriculum
and
an
initial
multimedia
concept
to
support
administration
and
teaching
purposes
.

COURSE
CURRICULUM

The
course
curriculum
focuses
on
the
new
media
and
communication
technologies
that
have
evolved
over
recent
years
through
the
fusion
of
computer
science
and
information
technology
.

Apart
from
the
fundamentals
of
engineering
,
the
basic
study
period
also
covers
the
essentials
of
electrical
engineering
,
information
science
and
information
theory
that
are
needed
to
study
this
new
field
of
knowledge
.

The
major
study
period
then
concentrates
on
the
mandatory
subjects
of
data
processing
,
information
technology
and
communication
networks
,
while
also
offering
the
following
electives
for
in
-
depth
study
.

Elective
Concerned
with
:
Data
Processing
and
Information
Technology
information
processing
,
data
storage
and
manipulation
,
data
processing
,
software
engineering
,
computer
systems
.

Communications
Technology
Information
transfer
,
encryption
,
networks
,
equipment
,
systems
.

Technical
Electronics
technologies
and
components
used
in
information
and
communications
technology
.

TABLE
1
:
ELECTIVES
1.1
INTERNATIONAL
FLAIR

The
degree
course
has
been
designed
to
serve
two
purposes
.

The
first
is
to
make
it
easier
for
foreign
students
to
study
at
the
Gerhard
Mercator
University
of
Duisburg
.

The
second
is
concerned
with
providing
German
students
with
an
opportunity
to
qualify
themselves
for
the
global
labour
market
through
the
undermentioned

,
internationally
orientated
course
components
English
-
language
lectures
during
the
course
,
also
by
guest
lecturers
from
the
UK
and
the
USA
language
courses
(
German
and
English
)
for
newcomers
at
least
one
semester
abroad
for
German
students
support
for
foreign
students
in
finding
accommodation
,
etc
support
for
German
students
in
organising
their
stay
abroad
tutorials
to
support
students
and
to
integrate
foreign
students
in
a
common
degree
course
and
studying
in
a
multi
-
cultural
environment
.

1.2
CAREER
PROSPECTS
A
new
professional
field
concerned
primarily
with
information
and
communication
technology
,
networks
and
multimedial
services
has
emerged
in
recent
years
.

These
new
areas
all
promise
high
future
growth
rates
and
good
prospects
for
job
beginners
as
well
as
for
diverse
careers
.

This
degree
course
will
endeavour
to
equip
students
with
the
necessary
technical
expertise
to
enable
work
in
these
areas
.

The
completion
of
course
projects
,
dissertations
and
practical
training
all
provide
ample
opportunity
for
team
-
orientated
work
on
the
latest
in
research
and
development
.

In
addition
to
obtaining
a
specialist
qualification
,
students
will
also
be
able
to
practise
their
English
and
gain
an
insight
into
international
cooperation
and
foreign
cultures
.

This
will
provide
perfect
training
for
the
coming
internationalisation
of
the
labour
market
.


-DOCSTART-

Location
-
based
services
are
influencing
our
lives
and
the
way
we
experience
the
surrounding
environment
;
smartphone
and
tablet
applications
supply
a
huge
amount
of
information
:
shops
around
us
,
traffic
conditions
,
etc
.

A
recent
trend
in
this
kind
of
services
is
to
provide
personalized
information
,
such
as
friendsâ€™
position
or
events
users
could
be
interested
in
.

In
this
paper
we
present
BOTTARI
,
an
Android
application
that
exploits
social
media
and
context
to
provide
point
of
interest
(
POI
)
recommendations
to
user
in
a
specific
geographic
location
.

BOTTARI
exploits
a
number
of
semantic
techniques
(
sentiment
analysis
,
inductive
reasoning
,
stream
reasoning
)
for
social
media
analysis
and
suggests
POIs
on
the
basis
of
usersâ€™
tastes
and
influencing
people
â€™s
opinion
.

1
Description
of
BOTTARI
and
its
innovation

The
Insa
-
dong
area
of
Seoul
is
one
of
the
most
popular
visitor
attractions
in
South
Korea
:
it
is
the
focal
point
of
Korean
traditional
culture
and
crafts
,
with
a
multitude
of
shops
,
restaurant
and
points
of
interest
(
POIs
BOTTARI
is
a
location
-
based
mobile
application
developed
for
Android
tablets
targeted
to
Korean
users
moving
in
Insa
-
dong
.

It
provides
personalized
POI
recommendations
,
to
help
users
find
their
way
when
they
are
in
a
specific
location
.

BOTTARI
collects
relevant
information
from
social
media
such
as
Twitter
and
blog
posts
,
elaborates
it
and
provides
contextualized
suggestions
.

The
Korean
word
â€œ
bottari
â€
refers
to
a
bundle
or
container
made
from
patterned
cloth
that
is
used
to
transport
a
one
â€™s
belongings
when
travelling
;
the
BOTTARI
application
lets
the
user
â€œ
transport
â€
the
location
-
specific
knowledge
,
derived
from
social
media
,
when
moving
in
the
physical
space
.

BOTTARI
has
two
main
sources
of
information
.

One
is
a
curated
dataset
about
the
Insa
-
dong
area
,
and
collects
information
about
some
hundred
POIs
,
each
one
described
by
a
few
dozen
attributes
(
location
,
description
,
place
category
,
price
range
,
reviews
,
contacts
,
etc
this
dataset
content
is
quite
static
and
is
used
as
â€œ
background
â€
information
about
the
POIs
.

This
data
are
expressed
in
RDF
,
described
with
regards
to
an
OWL
ontology
and
sums
up
to
more
than
20
thousand
triples
.

2
Irene
Celino
et
al
.

The
other
dataset
is
gathered
from
social
media
.

Apart
from
blog
posts
,
which
are
manually
collected
from
Korean
web
sites
,
the
main
source
consists
in
tweets
collected
from
Korean
users
(
i.e
all
tweets
are
written
in
Korean
language
)
since
April
2010
;
those
short
messages
are
acquired
by
means
of
the
Twitter
APIs
,
are
further
elaborated
to
identify
the
tweets
talking
about
POIs
in
Insa
-
dong
and
processed
to
assess
the
â€œ
sentiment
â€

they
express
(
positive
judgement
vs.
negative
rating

The
results
are
expressed
in
RDF
and
described
with
regards
to
the
OWL
ontology
illustrated
in
[
1
those
triples
are
then
stored
in
a
SOR
repository
;
the
triple
count
is
currently
0.6
billion
,
but
it
is
continuously
and
steadily
increasing
.

Screenshots
of
BOTTARI
a
)
augmented
reality
display
of
recommended
POIs
b
)
POI
selection
and
(
c
)
visualization
of
the
selected
POI
details
d
)
trends
in
user
sentiment
about
the
POI
.

Figure
1
shows
some
screenshots
of
the
application
.

BOTTARI
provides
to
its
users
four
different
types
of
recommendations
Interesting
recommendations
suggest
POIs
indicated
for
foreign
visitors
in
Korea
;
this
feature
calls
for
analysis
and
retrieval
of
POIs
attributes
Popular
recommendations
suggest
the
POIs
which
show
the
highest
level
of
reputation
on
social
media
;
this
feature
call
for
a
complete
analysis
of
the
social
sentiment
about
POIs
Emerging
recommendations
suggest
the
most
popular
POIs
in
a
delimited
period
of
time
(
e.g.
last
6
months
this
feature
calls
for
the
identification
of
â€œ
hypes
â€
and
new
trends
in
the
social
sentiment
;
5
SOR
is
a
Saltlux
product
:
http
semanticwiki-en.saltlux.com/index.php/SOR
.

Location
based
Social
Media
Analysis
with
Semantic
Web
3

For
me
recommendations
suggest
POIs
of
interest
for
the
current
user
;
this
feature
calls
for
personalized
recommendations
.

The
innovation
brought
by
BOTTARI
consists
in
offering
a
location
-
based
service
through
a
simple
and
intuitive
interface
for
a
natural
user
experience
;
the
application
provides
advanced
semantic
features
,
hiding
the
complexity
of
their
computation
from
the
user
â€™s
sight
.

The
details
of
the
internal
functioning
of
the
BOTTARI
application
are
given
in
the
following
section
.

2
Semantic
features
in
BOTTARI

The
four
types
of
recommendations
provided
by
BOTTARI
requires
different
levels
of
semantic
technologies
.

In
this
section
,
we
explain
how
we
addressed
the
different
challenges
.

2.1
Semantic
Information
Retrieval
to
get
Interesting
POIs

The
first
kind
of
recommendations
requires
to
suggest
the
user
with
a
subset
of
the
POIs
that
matches
(
1
)
the
user
current
location
and
(
2
)
the
category
of
â€œ
attractions
of
interest
for
foreign
visitors
To
provide
those
recommendations
,
we
used
the
Semantic
Information
Retrieval
features
of
SOR
,
which
provides
a
geographic
extension
of
SPARQL
that
allows
to
query
both
the
â€œ
semantic
â€
description
of
POIs
and
their
physical
location
.

2.2
Sentiment
Analysis
of
social
media
to
get
Popular
POIs

The
second
type
of
recommendations
requires
an
analysis
of
the
social
media
.

The
tweets
and
blog
posts
are
processed
by
a
sentiment
analysis
algorithm
that
detects
if
the
message
talks
about
a
POI
and
,
in
case
,
if
it
expresses
a
positive
or
negative
rating
on
the
POI
.

We
adopted
a
twofold
approach
to
compute
the
â€œ
sentiment
we
applied
the
two
methods
both
separately
and
in
cooperation
,
to
improve
the
precision
of
results
.

On
the
one
hand
,
we
used
a
pure
machine
learning
approach
using
SVMs
with
syllable
kernel
.

On
the
other
hand
,
we
used
a
rule
-
based
approach
:
the
messages
are
analysed
with
respect
to
some
rules
about
the
structure
and
language
.

Those
rules
are
both
manually
coded
and
generated
by
machine
learning
algorithms
;
this
is
a
NLP
technology
of
Saltlux
,
specialized
on
the
Korean
language
.

Once
the
sentiment
is
elicited
,
this
information
is
attached
as
metadata
to
the
message
description
in
the
triple
store
.

The
popular
recommendations
are
then
generated
by
querying
the
knowledge
base
and
suggesting
the
POIs
with
the
highest
number
of
positive
ratings
;
the
geographic
features
of
SOR
are
also
used
to
filter
POIs
and
recommend
only
those
around
the
user
current
location
.

4
Irene
Celino
et
al
.

2.3
Stream
Reasoning
to
get
Emerging
POIs

The
opinion
of
users
on
POIs
can
change
over
time
:
the
third
kind
of
recommendations
suggests
the
users
with
POIs
that
are
â€œ
on
fashion
â€
in
the
latest
period
of
time
.

To
this
end
,
we
adopted
stream
reasoning
[
2
]
to
identify
trends
and
changes
in
the
sentiment
about
the
POIs
.

Figure
2
illustrates
how
we
elaborated
the
social
media
stream
to
derive
synthetic
and
aggregated
information
that
help
the
user
in
choosing
the
POI
to
visit
.

Because
of
the
sentiment
analysis
elaboration
,
the
stream
of
messages
annotated
with
the
user
sentiment
is
not
in
real
-
time
,
but
it
is
â€œ
re
-
streamed
â€
from
its
storage
.

The
queries
enabled
by
the
C
-
SPARQL
Engine
[
3
,
4
]
let
find
the
emerging
opinion
of
users
about
POIs
:
from
left
to
right
in
the
figure
,
our
engine
counts
the
positive
opinions
about
a
POI
per
each
day
,
so
to
create
top-10
lists
;
the
positive
messages
per
day
can
be
further
aggregated
by
week
or
by
month
and
be
visualized
as
plot
lines
or
heatmaps
.

Query
chain
to
elaborate
social
media
streams
with
C
-
SPARQL
2.4
Inductive
Reasoning
to
get
For
me
POIs

Finally
,
POI
recommendations
can
be
personalized
:
the
user
can
be
suggested
with
POIs
that
could
be
interesting
for
her
.

To
this
end
,
we
adopted
inductive
reasoning
on
social
media
to
compute
BOTTARI
â€™s
for
me
recommendations
.

We
exploited
the
SUNS
approach
(
Statistical
Unit
Node
Set
)
described
in
[
5
,
6
SUNS
is
a
machine
learning
approach
for
exploiting
the
regularities
in
large
data
sets
in
relational
and
semantic
domains
.

The
approach
can
be
used
to
detect
interesting
data
patterns
and
predict
unknown
but
potentially
true
statements
.

In
BOTTARI
we
applied
SUNS
to
estimate
the
probability
that
a
user
will
like
a
POI
,
based
on
the
sentiment
the
same
user
expressed
about
other
POIs
and
the
opinion
that
other
users
expressed
about
that
POI
.

In
this
sense
,
we
provide
a
personalized
collaborative
filtering
recommendation
engine
,
to
suggest
users
with
the
most
interesting
POIs
with
respect
to
their
preferences
.

Location
based
Social
Media
Analysis
with
Semantic
Web
5
3
Design
and
development
of
BOTTARI

The
front
-
end
of
BOTTARI
is
a
dedicated
Android
application
which
uses
localization
and
augmented
reality
to
provide
its
functionality
to
users
in
mobility
in
the
Insa
-
dong
area
of
Seoul
;
the
visual
appearance
of
the
front
-
end
is
exemplified
in
Figure
1
.

The
BOTTARI
back
-
end
is
where
the
semantic
technologies
are
adopted
for
social
media
analysis
.

To
design
and
develop
the
back
-
end
we
exploited
the
potentialities
of
the
LarKC
platform
[
7
,
8
The
LarKC
platform
,
realized
in
the
homonymous
EU
project
,
is
aimed
to
reason
on
massive
heterogeneous
information
such
as
social
media
data
.

The
platform
consists
of
a
framework
to
build
workflows
,
i.e.
sequences
of
connected
components
(
plug
-
ins
)
able
to
consume
and
process
data
.

Each
plug
-
in
exploits
techniques
and
heuristics
from
diverse
areas
such
as
databases
,
machine
learning
and
the
Semantic
Web
.

In
BOTTARI
,
we
designed
a
workflow
that
makes
use
of
a
number
of
plug
-
ins
that
implements
the
features
explained
in
Section
2
(
semantic
information
retrieval
,
stream
reasoning
,
inductive
reasoning
Every
time
a
user
of
the
mobile
application
requests
a
type
of
recommendation
,
the
BOTTARI

LarKC
workflow
is
invoked
and
instantiates
the
plug
-
ins
to
compute
the
requested
data
.

Each
plug
-
in
encapsulate
a
part
of
the
data
processing
and
interacts
with
the
other
plug
-
ins
through
well
-
defined
interfaces
;
the
plug
-
ins
used
in
BOTTARI
are
described
in
several
technical
reports
.

The
adoption
of
LarKC
made
it
easy
the
composition
and
interplay
of
the
different
semantic
technologies
used
in
the
BOTTARI
back
-
end
.

While
the
mobile
application
requires
an
Android
device
and
the
physical
presence
of
the
user
in
Insa
-
dong
,
the
back
-
end
application
runs
on
a
server
.

The
interest
reader
can
have
a
look
â€œ
behind
the
scenes
â€
of
BOTTARI
at
http
larkc.cefriel.it/lbsma/bottari
4
Evaluation
and
Outlook
on
BOTTARI

The
semantic
technologies
behind
BOTTARI
have
been
evaluated
in
terms
of
the
LarKC
plug
-
ins
used
in
the
workflow
.

We
started
various
evaluation
campaigns
on
our
implementation
and
their
early
results
are
available
in
some
of
LarKC
project
technical
reports
.

Hereafter
,
we
present
some
of
those
early
results
.

In
the
stream
reasoning
processing
,
one
of
the
main
components
is
the
CSPARQL
Engine
[
4

In
Figure
3(a
we
report
the
results
of
a
long
lasting
execution
of
a
simple
continuous
query
registered
in
the
C
-
SPARQL
Engine
:
the
query
matches
all
triples
in
the
stream
and
regenerates
them
out
.

In
our
experimental
settings
(
an
Intel
Core
2
Duo
T7500
at
2.2GHz
,
4
GB
of
RAM
DDR2
at
667
MHz
,
Hard
Disk
at
5400
rpm
the
C
-
SPARQL
Engine
throughput
was
between
20,987
and
21,015
triples
/
second
;
the
average
throughput
was
20,999
triples
/
second
.

This
result
proves
that
we
are
able
to
process
the
amount
of
data
currently
produced
in
social
media
streams
.

6
Large
Knowledge
Collider
,
cf
.

http
www.larkc.eu
7
Cf
.

D3.9
,
D6.10
and
D6.11
at
http
www.larkc.eu/deliverables
8
Cf
.
D3.9
and
D6.11
at
http
www.larkc.eu/deliverables
6
Irene
Celino
et
al
.

BOTTARI
evaluation
a
)
execution
of
a
continuous
query
registered
in
the
C
-
SPARQL
Engine
and
(
b
)
accuracy
of
recommendations
In
Figure
3(b

we
show
the
accuracy
of
recommendations
:
we
compare
two
baseline
algorithms
random
guess
(
random
)
and
item
-
based
k
-
nearest
neighbour
(
knnItem
with
C
-
SPARQL
â€™s
emerging
recommendations
(
mostLiked
)
and
SUNSâ€™
for
me
recommendations
;
here
,
we
do
not
consider
the
location
dimension
(
i.e
in
this
evaluation
we
recommend
POIs
regardless
the
current
position
of
the
user

As
expected
,
the
random
is
the
worst
;
C
-
SPARQL
is
slightly
better
than
the
similarity
-
based
method
:
this
might
indicate
the
â€œ
bandwagon
effect
â€
that
exists
in
many
social
communities
;
SUNS
significantly
outperformed
all
other
methods
(
with
a
number
of
the
latent
variables
greater
than
100
The
best
ranking
was
produced
by
the
combination
of
both
SUNS
and
C
-
SPARQL
:
these
results
confirm
again
the
effectiveness
of
combined
approach
of
deductive
and
inductive
reasoning
[
6
The
design
of
the
BOTTARI
service
from
the
user
point
of
view
,
as
well
as
its
prototypical
implementation
and
its
early
evaluation
,
demonstrate
that
Semantic
Web
technologies
can
be
successfully
applied
to
concrete
scenarios
and
can
help
in
adding
added
-
value
functionalities
.

We
foresee
a
potential
market
exploitation
of
this
kind
of
location
-
based
social
media
analysis
applications
and
Saltlux
is
going
to
continue
the
development
of
BOTTARI
to
a
commercial
product
for
Korean
customers
.


-DOCSTART-

Arabic
,
the
mother
tongue
of
over
300
million
people
around
the
world
,
is
known
as
one
of
the
most
difficult
languages
in
Automatic
Natural
Language
processing
(
NLP
)
in
general
and
information
retrieval
in
particular
.

Hence
,
Arabic
can
not
trust
any
web
information
retrieval
system
as
reliable
and
relevant
as
Google
search
engine
.

In
this
context
,
we
dared
to
focus
all
our
researches
to
implement
an
Arabic
web
-
based
information
retrieval
system
entitled
ARABIRS
(
ARABic
Information
Retrieval
System

Therefore
,
to
launch
such
a
process
so
long
and
hard
,
we
will
start
with
Indexing
as
one
of
the
crucial
steps
of
the
system
.


-DOCSTART-

Shastri
Ajjanagadde
have
proposed
a
biologically
plausible
connectionist
rule
-
based
reasoning
system
(
hereafter
referred
to
as
a
knowledge
base
,
or
KB
that
represents
a
dynamic
binding
as
the
simultaneous
,
or
in
-
phase
,
activity
of
the
appropriate
nodes
[
9
This
paper
makes
the
first
attempt
at
designing
a
biologically
plausible
connectionist
interface
mechanism
between
2
distinct
phase
-
based
KB
,
as
the
next
step
toward
providing
a
computational
account
of
common
-
sense
reasoning
.

The
Dynamic
Binding
Communication
Mechanism
(
DBCM
)
extracts
a
dynamic
binding
from
a
source
KB
and
incorporates
the
binding
into
a
destination
KB
so
that
it
is
consistent
with
the
knowledge
already
represented
in
the
latter
.

DBCM
consists
of
several
distinct
,
special
-
purpose
modules
.

The
Binding
Memory
(
BM
)
is
made
up
of
several
identical
banks
of
nodes
.

Each
time
a
temporally
-
encoded
dynamic
binding
is
extracted
from
the
source
KB
,
it
is
transferred
into
one
of
the
banks
,
where
the
binding
is
converted
to
a
spatially
-
encoded
representation
.

The
Phase
Database
(
PD
)
monitors
the
target
KB
and
produces
a
phased
output
that
is
out
of
phase
with
all
other
nodes
in
the
target
KB
.

The
Phase
Allocator
(
PA
)
synthesizes
information
from
the
Phase
Database
and
from
the
target

KB
to
determine
the
phase
in
which
to
introduce
the
new
dynamic
binding
into
the
target
KB
.

In
turn
,
the
PA
extracts
a
single
binding
from
one
of
the
banks
in
the
BM
and
introduces
it
into
the
target
KB
.

The
interface
also
utilizes
2
searchlight
mechanisms
:
the
first
governs
which
bank
in
the
BM
receives
bindings
;
the
second
mediates
between
the
active
banks
(
those
which
are
currently
representing
bindings
and
the
Phase
Allocator
.

Comments
University
of
Pennsylvania
Department
of
Computer
and
Information
Science
Technical
Report

MSCIS-91
-
16
.

This
technical
report
is
available
at
ScholarlyCommons
:
http
repository.upenn.edu/cis_reports/486
Dynamic
Binding
Communication
Mechanism
MS
-
CIS-91
-
16
LINC
LAB
196


-DOCSTART-

The
scope
for
information
retrieval
applications
continues
to
growth
to
encompass
larger
and
more
diverse
archives
and
new
computing
environments
.

The
consequent
increased
demands
on
information
retrieval
systems
continue
to
motivate
research
into
new
and
more
efficient
search
technologies
.

Users
of
information
retrieval
systems
work
in
personal
and
physical
contexts
,
while
the
documents
containing
the
information
they
seek
often
relate
to
specific
contexts
.

It
is
argued
that
taking
account
of
context
can
improve
information
retrieval
effectiveness
.

While
potential
use
of
context
has
certainly
been
under
explored
,
it
is
already
present
in
established
techniques
such
as
relevance
feedback
.

The
emergence
of
new
information
retrieval
environments
,
such
as
those
associated
with
mobile
computing
,
raises
new
challenges
for
information
retrieval
for
which
greater
use
of
context
may
form
a
important
component
.

Among
the
many
questions
raised
by
attempting
to
perform
information
retrieval
in
context
are
issues
of
how
algorithms
might
be
extended
or
developed
to
facilitate
use
of
context
,
does
the
user
need
to
be
actively
involved
to
make
use
of
context
,
where
should
the
context
information
come
from
,
and
how
might
the
effectiveness
of
such
methods
be
tested
in
extended
evaluation
frameworks
?


-DOCSTART-

Connection
management
based
on
Quality
of
Service
(
QoS
)
offers
opportunities
for
better
resource
allocation
in
networks
providing
service
classes

Negotiation
"
describes
the
process
of
cooperatively
configuring
application
and
network
resources
for
an
application
's
use
.

Complex
and
long
-
running
applications
can
reduce
the
inefficiencies
of
static
allocations
by
splitting
resource
use
into
"
eras
"
bounded
by
renegotiation
of
QoS
parameters
.

Renegotiation
can
be
driven
by
either
the
application
or
the
network
in
order
to
best
match
application
and
network
dynamics
.

A
key
element
in
this
process
is
a
translation
between
differing
perspectives
on
QoS
maintained
by
applications
and
network
service
provision
.

We
model
translation
with
an
entity
called
a
"
broker
Comments
University
of
Pennsylvania
Department
of
Computer
and
Information
Science
Technical
Report
No
.
MSCIS-93
-
34
.

This
technical
report
is
available
at
ScholarlyCommons
:
http
repository.upenn.edu/cis_reports/285
Revision
of
QoS
Guarantees
at
the
Application
/
Network
Interface
MS
-
CIS-93
-
34
DISTRIBUTED
SYSTEMS
LAB
31
Klara
Nahrstedt

Jonathan
M.
Snlith
University
of
Pennsylvania
School
of
Engineering
and
Applied
Science
Computer
and
Information
Science
Department
Philadelphia
.

PA
10104-G389


-DOCSTART-

We
present
PRISM
,
a
privacy
-
preserving
scheme
for
word
search
in
cloud
computing
.

In
the
face
of
a
curious
cloud
provider
,
the
main
challenge
is
to
design
a
scheme
that
achieves
privacy
while
preserving
the
efficiency
of
cloud
computing
.

Solutions
from
related
research
,
like
encrypted
keyword
search
or
Private
Information
Retrieval
(
PIR
fall
short
of
meeting
real
-
world
cloud
requirements
and
are
impractical
.

PRISM
â€™s
idea
is
to
transform
the
problem
of
word
search
into
a
set
of
parallel
instances
of
PIR
on
small
datasets
.

Each
PIR
instance
on
a
small
dataset
is
efficiently
solved
by
a
node
in
the
cloud
during
the
â€œ
Map
â€
phase
of
MapReduce
.

Outcomes
of
map
computations
are
then
aggregated
during
the
â€œ
Reduce
â€
phase
.

Due
to
the
linearity
of
PRISM
,
the
simple
aggregation
of
map
results
yields
the
final
output
of
the
word
search
operation
.

We
have
implemented
PRISM
on
Hadoop
MapReduce
and
evaluated
its
efficiency
using
real
-
world
DNS
logs
.

PRISM
â€™s
overhead
over
non
-
private
search
is
only
11

Thus
,
PRISM
offers
privacy
-
preserving
search
that
meets
cloud
computing
efficiency
requirements
.

Moreover
,
PRISM
is
compatible
with
standard
MapReduce
,
not
requiring
any
change
to
the
interface
or
infrastructure
.


-DOCSTART-

Electronic
Health
Records
(
EHRs
)
contain
a
large
amount
of
free
text
documentation
which
is
potentially
very
useful
for
Information
Retrieval
and
Text
Mining
applications
.

We
have
,
in
an
initial
annotation
trial
,
annotated
6
739
sentences
randomly
extracted
from
a
corpus
of
Swedish
EHRs
for
sentence
level
(
un)certainty
,
and
token
level
speculative
keywords
and
negations
.

This
set
is
split
into
different
clinical
practices
and
analyzed
by
means
of
descriptive
statistics
and
pairwise
Inter
-
Annotator
Agreement
(
IAA
)
measured
by
F1-score
.

We
identify
geriatrics
as
a
clinical
practice
with
a
low
average
amount
of
uncertain
sentences
and
a
high
average
IAA
,
and
neurology
with
a
high
average
amount
of
uncertain
sentences
.

Speculative
words
are
often
n
-
grams
,
and
uncertain
sentences
longer
than
average
.

The
results
of
this
analysis
is
to
be
used
in
the
creation
of
a
new
annotated
corpus
where
we
will
refine
and
further
develop
the
initial
annotation
guidelines
and
introduce
more
levels
of
dimensionality
.

Once
we
have
finalized
our
guidelines
and
refined
the
annotations
we
plan
to
release
the
corpus
for
further
research
,
after
ensuring
that
no
identifiable
information
is
included
.


-DOCSTART-

Qanda
uses
a
general
architecture
for
human
language
technology
called
Catalyst
Burger
Mardis
2002
,
Nyberg
et
al
to
appear
Developed
at
MITRE
for
the
DARPA
TIDES
program
,
the
Catalyst
architecture
is
specifically
designed
for
fast
processing
and
for
combining
the
strengths
of
Information
Retrieval
and
Natural
Language
Processing
into
a
single
framework
.

Catalyst
uses
a
dataflow
architecture
in
which
standoff
annotations
are
passed
from
one
component
to
another
,
with
the
components
connected
in
arbitrary
(
acyclic
)
topologies
.

The
use
of
standoff
annotation
permits
components
to
be
optimized
for
just
those
pieces
of
information
they
require
for
their
processing
.


-DOCSTART-

Speculating
about
where
the
future
of
the
field
of
corrosion
prevention
,
detection
,
and
rehabilitation
might
lead
is
interesting
â€”
but
not
without
risk
.

We
all
are
aware
of
the
revolution
in
electronics
,
particularly
digital
electronics
,
and
information
science
during
the
past
20
years
.

We
also
should
recognize
that
materials
science
and
technology
have
undergone
a
major
revolution
,
bringing
us
new
materials
such
as
plastics
,
ceramics
,
and
chemicals
.

The
construction
industry
also
has
made
major
advances
in
automation
,
equipment
,
and
process
control
.

All
of
these
disciplines
will
affect
the
highway
construction
and
maintenance
industry
during
the
next
few
decades
.

It
is
against
this
backdrop
that
we
consider
the
future
of
corrosion
prevention
.

Corrosion
is
the
reaction
between
a
material
and
its
environment
.

Perhaps
the
best
known
example
of
corrosion
is
steel
;
as
soon
as
iron
ore
has
been
smelted
and
refined
to
produce
steel
,
nature
begins
to
reverse
the
process
.

The
steel
reacts
with
the
environment
to
form
products
such
as
oxides
,
sulfates
,
sulfides
,
and
chlorides
that
no
longer
have
the
required
physical
and
chemical
properties
.

All
materials
react
to
some
degree
in
some
environments
,
and
one
of
the
responsibilities
of
the
corrosion
engineer
is
to
identify
materials
that
will
provide
the
desired
life
in
a
given
service
environment
at
a
reasonable
cost
.

Billions
of
dollars
are
spent
every
year
in
protecting
against
,
investigating
,
repairing
,
and
replacing
damage
due
to
corrosion
.

Occasionally
,
corrosion
-
induced
failures
are
catastrophic
;
more
often
,
corrosion
is
a
slow
process
that
can
be
detected
before
failure
occurs
.

Therefore
,
the
consequences
usually
are
economic
rather
than
life
-
threatening
.


-DOCSTART-

We
propose
a
new
application
of
the
Friedman
statistical
test
of
significance
to
compare
multiple
retrieval
methods
.

After
measuring
the
average
precision
at
the
eleven
standard
levels
of
recall
,
our
application
of
the
Friedman
test
provides
a
global
comparison
of
the
methods
.

In
some
experiments
this
test
provides
additional
and
useful
information
to
decide
if
methods
are
different
.


-DOCSTART-

A
crucial
task
in
many
recommender
problems
like
computational
advertising
,
content
optimization
,
and
others
is
to
retrieve
a
small
set
of
items
by
scoring
a
large
item
inventory
through
some
elaborate
statistical
/
machine
-
learned
model
.

This
is
challenging
since
the
retrieval
has
to
be
fast
(
few
milliseconds
)
to
load
the
page
quickly
.

Fast
retrieval
is
well
studied
in
the
information
retrieval
(
IR
)
literature
,
especially
in
the
context
of
document
retrieval
for
queries
.

When
queries
and
documents
have
sparse
representation
and
relevance
is
measured
through
cosine
similarity
(
or
some
variant
thereof
one
could
build
highly
efficient
retrieval
algorithms
that
scale
gracefully
to
increasing
item
inventory
.

The
key
components
exploited
by
such
algorithms
is
sparse
query
-
document
representation
and
the
special
form
of
the
relevance
function
.

Many
machine
-
learned
models
used
in
modern
recommender
problems
do
not
satisfy
these
properties
and
since
brute
force
evaluation
is
not
an
option
with
large
item
inventory
,
heuristics
that
filter
out
some
items
are
often
employed
to
reduce
model
computations
at
runtime
In
this
paper
,
we
take
a
two
-
stage
approach
where
the
first
stage
retrieves
top
-
K
items
using
our
approximate
procedures
and
the
second
stage
selects
the
desired
top
-
k
using
brute
force
model
evaluation
on
the
K
retrieved
items
.

The
main
idea
of
our
approach
is
to
reduce
the
first
stage
to
a
standard
IR
problem
,
where
each
item
is
represented
by
a
sparse
feature
vector
(
a.k.a
.
the
vector
-
space
representation
)
and
the
query
-
item
relevance
score
is
given
by
vector
dot
product
.

The
sparse
item
representation
is
learnt
to
closely
approximate
the
original
machine
-
learned
score
by
using
retrospective
data
.

Such
a
reduction
allows
leveraging
extensive
work
in
IR
that
resulted
in
highly
efficient
retrieval
systems
.

Our
approach
is
model
-
agnostic
,
relying
only
on
data
generated
from
the
machine
-
learned
model
.

We
obtain
significant
improvements
in
the
computational
cost
vs.
accuracy
tradeoff
compared
to
several
baselines
in
our
empirical
evaluation
on
both
synthetic
models
and
on
a
click
-
through
(
CTR
)
model
used
in
online
advertising
.


-DOCSTART-

In
an
adaptive
information
retrieval
(
IR
)
setting
,
the
information
seekers
'
beliefs
about
which
terms
are
relevant
or
nonrelevant
will
naturally
fluctuate
.

This
article
investigates
how
the
theory
of
belief
revision
can
be
used
to
model
adaptive
IR
.

More
specifically
,
belief
revision
logic
provides
a
rich
representation
scheme
to
formalize
retrieval
contexts
so
as
to
disambiguate
vague
user
queries
.

In
addition
,
belief
revision
theory
underpins
the
development
of
an
effective
mechanism
to
revise
user
profiles
in
accordance
with
information
seekers
'
changing
information
needs
.

It
is
argued
that
information
retrieval
contexts
can
be
extracted
by
means
of
the
information
-
flow
text
mining
method
so
as
to
realize
a
highly
autonomous
adaptive
IR
system
.

The
extra
bonus
of
a
belief
-
based
IR
model
is
that
its
retrieval
behavior
is
more
predictable
and
explanatory
.

Our
initial
experiments
show
that
the
belief
-
based
adaptive
IR
system
is
as
effective
as
a
classical
adaptive
IR
system
.

To
our
best
knowledge
,
this
is
the
first
successful
implementation
and
evaluation
of
a
logic
-
based
adaptive
IR
model
which
can
efficiently
process
large
IR
collections
.


-DOCSTART-

One
of
the
key
components
of
designing
usable
and
useful
collaborative
information
retrieval
systems
is
to
understand
the
needs
of
the
users
of
these
systems
.

Our
research
team
has
been
exploring
collaborative
information
behavior
in
a
variety
of
organizational
settings
.

Our
research
goals
have
been
two
-
fold
:

First
,
to
develop
a
conceptual
understanding
of
collaborative
information
behavior
and
second
,
gather
requirements
for
the
design
of
collaborative
information
retrieval
systems
.

In
this
paper
,
we
present
a
brief
overview
of
our
fieldwork
in
a
three
different
organizational
settings
,
discuss
our
methodology
for
collecting
data
on
collaborative
information
behavior
,
and
highlight
some
lessons
that
we
are
learning
about
potential
users
of
collaborative
information
retrieval
systems
in
these
domains
.


-DOCSTART-

Systematic
collection
,
storage
,
and
organization
of
ground
-
water
data
are
necessary
as
the
volume
and
variety
of
records
increase
,
so
that
the
data
will
be
accessible
and
useful
.

When
an
automated
system
is
being
planned
,
it
is
important
to
consider
what
will
go
into
that
system
,
the
uses
of
the
system
,
and
the
types
of
data
outputs
that
will
be
provided
.

This
publication
discusses
the
types
of
ground
-
waterrecords
maintained
at
the
Illinois
State
Water
Survey
,
and
the
system
developed
for
filing
the
records
.

It
then
reports
the
results
of
a
survey
of
state
agencies
in
Michigan
,
Indiana
,
Kentucky
,
and
Wisconsin
regarding
the
types
of
groundwater
data
in
their
state
database
systems
,
financial
support
available
for
automated
information
systems
,
system
features
and
characteristics
,
data
sources
,
methods
for
making
updates
and
corrections
,
and
record
identification
systems
.

Recommendations
are
offered
to
database
and
computer
groups
regarding
documentation
,
assignment
of
record
identification
numbers
,
archival
of
automated
information
systems
.

Geographic
Information
System
(

GIS
)
capabilities
of
information
systems
,
and
responsibilities
of
information
managers
.

Reference
:
Schock
,
Susan
C
,
Kay
Mumm
,
Trudy
K.
Dahl
,
and
Susie
Dodd
.

Automated
Data
Systems
for
Ground
-
Water
Information
.

Illinois
State
Water
Survey
,
Champaign
,
Circular
174
,
1990
.

Indexing
Terms
:
Automation
,
data
processing
,
data
storage
and
retrieval
,
ground
-
water
data
,
Illinois
,
Indiana
,
information
retrieval
,
Kentucky
,
Michigan
,
water
wells
,
well
data
,
Wisconsin
.

STATE
OF
ILLINOIS
HON
.

JAMES
R.
THOMPSON
.

Governor
DEPARTMENT
OF
ENERGY
AND
NATURAL
RESOURCES
Karen
A.
Witter
,
M.S
Director
BOARD
OF
NATURAL
RESOURCES
AND
CONSERVATION
Karen
A.
Witter
,
M.S
Chair
Robert
H.
Benton
,
B.S.C.E
Engineering
H.S.
Gutowsky
,
Ph
.
D
Chemistry
Roy
L.
Taylor
,

Ph
.
D
Plant
Biology
Robert
L.
Metcalf
,

Ph
.
D
Biology
Judith
Liebman
,
Ph.D.
University
of
Illinois
John
H.
Yopp
,
Ph.D.
Southern
Illinois
University
STATE
WATER
SURVEY
DIVISION
RICHARD
G.
SEMONIN
,
Chief
2204
GRIFFITH
DRIVE
CHAMPAIGN
,
ILLINOIS
61820
-
7495


-DOCSTART-

Boolean
Conjunctive
Normal
Form
(
CNF
)
expansion
can
effectively
address
the
vocabulary
mismatch
problem
,
a
problem
that
current
retrieval
techniques
have
very
limited
ability
to
solve
.

Meanwhile
,
expert
searchers
are
found
to
spend
large
amounts
of
time
carefully
creating
manual
CNF
queries
.

These
CNF
queries
are
highly
effective
,
and
can
outperform
bag
of
word
queries
by
a
large
margin
.

However
,
not
many
effective
tools
exist
that
can
facilitate
the
efficient
manual
creation
of
effective
CNF
queries
.

We
describe
such
a
publicly
available
search
tool
,
WikiQuery
,
which
can
efficiently
assist
the
users
to
create
CNF
queries
through
easy
query
editing
and
immediate
access
to
search
results
.

Experiments
show
that
ordinary
search
users
,
with
limited
prior
knowledge
of
Boolean
queries
,
can
use
this
intuitive
tool
to
create
effective
CNF
queries
.

We
argue
that
tools
like
WikiQuery
can
attract
and
retain
certain
users
from
the
commercial
Web
search
engines
,
and
may
be
a
good
starting
point
to
build
a
research
Web
search
engine
.


-DOCSTART-

Recently
there
have
been
appearing
new
applications
of
genetic
algorithms
to
information
retrieval
,
most
of
them
specifically
to
relevance
feedback
.

The
evolution
of
the
possible
solutions
are
guided
by
fitness
functions
that
are
designed
as
measures
of
the
goodness
of
the
solutions
.

These
functions
are
naturally
the
key
to
achieving
a
reasonable
improvement
,
and
which
function
is
chosen
most
distinguishes
one
experiment
from
another
.

In
previous
work
,
we
found
that
,
among
the
functions
implemented
in
the
literature
,
the
ones
that
yield
the
best
results
are
those
that
take
into
account
not
only
when
documents
are
retrieved
,
but
also
the
order
in
which
they
are
retrieved
.

Here
,
we
therefore
evaluate
the
efficacy
of
a
genetic
algorithm
with
various
order
-
based
fitness
functions
for
relevance
feedback
(
some
of
them
of
our
own
design
and
compare
the
results
with
the
Ide
dec
-
hi
method
,
one
of
the
best
traditional
methods
.


-DOCSTART-

This
work
describes
an
algorithm
which
aims
at
increasing
the
quantity
of
relevant
documents
retrieved
from
a
Peer
-
To
-
Peer
(
P2P
)
network
.

The
algorithm
is
based
on
a
statistical
model
used
for
ranking
documents
,
peers
and
ultra
-
peers
,
and
on
a
â€œ
piggybacking
â€
technique
performed
when
the
query
is
routed
across
the
network
.

The
algorithm
â€œ
amplifies

â€
the
statistical
information
about
the
neighborhood
stored
in
each
ultra
-
peer
.

The
preliminary
experiments
provided
encouraging
results
as
the
quantity
of
relevant
documents
retrieved
through
the
network
almost
doubles
once
query
piggybacking
is
exploited
.


-DOCSTART-

Text
classification
(
TC
)
is
the
task
to
automatically
classify
documents
based
on
learned
document
features
.

Many
popular
TC
models
use
simple
occurrence
of
words
in
a
document
as
features
.

They
also
commonly
assume
word
occurrences
to
be
statistically
independent
in
their
design
.

Although
it
is
obvious
that
such
assumption
does
not
hold
in
general
,
these
TC
models
have
been
robust
and
efficient
in
their
task
.

Some
recent
studies
have
shown
context
-
sensitive
TC
approaches
,
which
take
into
consideration
contexts
in
the
form
of
word
co
-
occurrences
,
have
been
able
to
perform
better
in
general
.

On
the
other
hand
,
there
have
been
many
studies
in
the
use
of
complex
linguistic
or
semantic
features
instead
of
simple
word
occurrences
as
features
for
information
retrieval
and
classification
tasks
.

While
these
complex
features
may
intuitively
have
more
relevance
to
the
tasks
concerned
,
results
of
these
studies
on
their
effectiveness
have
been
mixed
and
not
been
conclusive
.

In
this
paper
we
present
our
investigation
on
the
use
of
some
complex
linguistic
features
with
context
-
sensitive
TC
method
.

Our
experiment
results
show
some
potential
advantages
of
such
approach
.


-DOCSTART-

Crowdsourcing
relevance
judgments
for
test
collection
construction
is
attractive
because
the
practice
has
the
possibility
of
being
more
affordable
than
hiring
high
quality
assessors
.

A
problem
faced
by
all
crowdsourced
judgments
even
judgments
formed
from
the
consensus
of
multiple
workers
is
that
there
will
be
differences
in
the
judgments
compared
to
the
judgments
produced
by
high
quality
assessors
.

For
two
TREC
test
collections
,
we
simulated
errors
in
sets
of
judgments
and
then
measured
the
effect
of
these
errors
on
effectiveness
measures
.

We
found
that
some
measures
appear
to
be
more
tolerant
of
errors
than
others
.

We
also
found
that
to
achieve
high
rank
correlation
in
the
ranking
of
retrieval
systems
requires
conservative
judgments
for
average
precision
(
AP
)
and
nDCG
,
while
precision
at
rank
10
requires
neutral
judging
behavior
.

Conservative
judging
avoids
mistakenly
judging
non
-
relevant
documents
as
relevant
at
the
cost
of
judging
some
relevant
documents
as
non
-
relevant
.

In
addition
,
we
found
that
while
conservative
judging
behavior
maximizes
rank
correlation
for
AP
and
nDCG
,
to
minimize
the
error
in
the
measuresâ€™
values
requires
more
liberal
behavior
.

Depending
on
the
nature
of
a
set
of
crowdsourced
judgments
,
the
judgments
may
be
more
suitable
with
some
effectiveness
measures
than
others
,
and
the
use
of
some
effectiveness
measures
will
require
higher
levels
of
judgment
quality
than
others
.


-DOCSTART-

Advanced
Traveler
Information
Systems
(
ATIS
)
require
e
cient
information
retrieval
and
updating
in
a
dynamic
environment
at
di
erent
geographical
scales
.

ATIS
applications
are
useful
in
yielding
a
better
utilization
of
the
limited
costly
transportation
arteries
and
providing
value
-
added
traveler
information
.

Many
ATIS
applications
are
built
on
the
functionalities
provided
by
Geographical
Information
Systems
(
GIS
which
often
can
not
meet
extra
requirements
like
real
-
time
response
.

We
investigate
GIS
-
based
systems
in
ATIS
and
propose
a
system
architecture
based
on
GIS
and
distributed
database
technology
.

Issues
on
data
modeling
,
data
representation
,
storage
and
retrieval
,
data
aggregation
,
and
parallel
processing
of
queries
are
discussed
.

This
paper
introduces
a
distributed
system
architecture
for
ATIS
based
on
recent
technology
;
presents
new
data
models
for
information
representation
;
proposes
data
shipping
for
e
cient
query
processing
and
function
shipping
in
reducing
communication
overhead
;
exploits
network
of
computers
for
solving
complex
problems
more
timely
;
and
incorporates
privacy
protection
for
sensitive
data
.


-DOCSTART-

We
address
wildcard
search
in
structured
peer
-
to
-
peer
(
P2P
)
networks
,
which
,
to
our
knowledge
,
has
not
yet
been
explored
in
the
literature
.

We
begin
by
presenting
an
approach
based
on
some
well
-
known
techniques
in
information
retrieval
(
IR
)
and
discuss
why
it
is
not
appropriate
in
a
distributed
environment
.

We
then
present
a
simple
and
novel
technique
to
index
objects
for
wildcard
search
in
a
fully
decentralized
manner
,
along
with
some
search
strategies
to
retrieve
objects
.

Our
index
scheme
,
as
opposed
to
a
traditional
IR
approach
,
can
achieve
quite
balanced
loads
,
avoid
hop
spots
and
single
point
of
failure
,
reduce
storage
and
maintenance
costs
,
and
offer
some
ranking
mechanisms
for
matching
objects
.

We
use
the
compact
disc
(
CD
)
records
collected
in
FreeDB
(
http
freedb.org
)
as
the
experimental
data
set
to
evaluate
our
scheme
.

The
results
confirm
that
our
index
scheme
is
very
effective
in
balancing
the
load
.

Moreover
,
search
efficiency
depends
on
the
information
given
in
a
query
:
the
more
the
information
,
the
higher
the
performance
.


-DOCSTART-

However
we
can
distinguish
three
basic
recommendation
approaches
:
demographic
,
contentbased
and
collaborative
[
7
they
are
using
many
different
general
reasoning
methods
know
from
many
other
disciplines
such
as
Artificial
Intelligence
,
Expert
Systems
or
Information
Retrieval
.

Recently
,
the
recommender
systems
also
adopt
some
nature
inspired
methods
such
as
artificial
immune
system
[
4,7
In
the
Biology
,
the
immune
system
is
defined
as
â€œ
the
system
of
specialized
cells
and
organs
that
protect
an
organism
from
outside
biological
influences
12

In
the
literature
we
can
find
some
application
of
AIS
for
collaborative
filtering

[
1
We
are
going
to
develop
,
implement
and
verify
hybrid
recommendation
method
for
layout
,
structure
and
content
recommendation
in
the
existing
wiki
-
based
system
(
for
example
Wikitravel
or
Wikinews
12

Besides
AIS
we
are
going
to
implement
some
consensus
-
based
methods
for
content
-
based
recommendation
[
8,9,10
]
and
fuzzy
inference
rules
for
demographic
filtering
[
7
]
.


-DOCSTART-

The
papers
in
this
special
section
are
devoted
to
the
growing
field
of
acoustic
scene
classification
and
acoustic
event
recognition
.

Machine
listening
systems
still
have
difficulties
to
reach
the
ability
of
human
listeners
in
the
analysis
of
realistic
acoustic
scenes
.

If
sustained
research
efforts
have
been
made
for
decades
in
speech
recognition
,
speaker
identification
and
to
a
lesser
extent
in
music
information
retrieval
,
the
analysis
of
other
types
of
sounds
,
such
as
environmental
sounds
,
is
the
subject
of
growing
interest
from
the
community
and
is
targeting
an
ever
increasing
set
of
audio
categories
.

This
problem
appears
to
be
particularly
challenging
due
to
the
large
variety
of
potential
sound
sources
in
the
scene
,
which
may
in
addition
have
highly
different
acoustic
characteristics
,
especially
in
bioacoustics
.

Furthermore
,
in
realistic
environments
,
multiple
sources
are
often
present
simultaneously
,
and
in
reverberant
conditions
.


-DOCSTART-

Sentiment
Analysis
is
a
two
level
task
.

The
first
one
is
Identifying
Topic
and
the
second
is
,
classifying
sentiment
related
to
that
topic
.

Sentiment
Analysis
starts
with
â€œ
What
other
people
thinks
Sentiment
Extraction
deals
with
the
retrieval
of
the
opinion
or
mood
conveyed
in
a
block
of
Unstructured
text
in
relation
to
the
domain
of
the
document
being
analyzed
.

Although
a
lot
of
research
has
gone
in
the
NLP
,
machine
learning
and
web
mining
community
on
extracting
structured
data
from
unstructured
sources
,
most
of
the4
proposed
methods
depend
on
tediously
labeled
unstructured
data
.

The
World
Wide
Web
has
been
dominated
by
unstructured
content
and
searching
the
web
has
been
based
on
techniques
from
Information
Retrieval
.

Supervised
learning
algorithm
analyzes
the
training
data
and
produces
an
inferred
function
which
is
called
classification
.


-DOCSTART-

We
study
a
new
notion
of
graph
centrality
based
on
absorbing
random
walks
.

Given
a
graph
G
V
,
E
)
and
a
set
of
query
nodes
Q
x2286
;
V
,
we
aim
to
identify
the
k
most
central
nodes
in
G
with
respect
to
Q.
Specifically
,
we
consider
central
nodes
to
be
absorbing
for
random
walks
that
start
at
the
query
nodes
Q.

The
goal
is
to
find
the
set
of
k
central
nodes
that
minimizes
the
expected
length
of
a
random
walk
until
absorption
.

The
proposed
measure
,
which
we
call
k
absorbing
random
-
walk
centrality
,
favors
diverse
sets
,
as
it
is
beneficial
to
place
the
k
absorbing
nodes
in
different
parts
of
the
graph
so
as
to
x201C;intercept&#x201D

;
random
walks
that
start
from
different
query
nodes
.

Although
similar
problem
definitions
have
been
considered
in
the
literature
,
e.g
in
information
-
retrieval
settings
where
the
goal
is
to
diversify
web
-
search
results
,
in
this
paper
we
study
the
problem
formally
and
prove
some
of
its
properties
.

We
find
that
the
problem
is
NP
-
hard
,
while
the
objective
function
is
monotone
and
supermodular
,
implying
that
a
greedy
algorithm
provides
solutions
with
an
approximation
guarantee
.

On
the
other
hand
,
the
greedy
algorithm
involves
expensive
matrix
operations
that
make
it
prohibitive
to
employ
on
large
datasets
.

To
confront
this
challenge
,
we
explore
the
performance
of
efficient
heuristics
.


-DOCSTART-

This
thesis
presents
a
system
for
web
-
based
information
retrieval
that
supports
precise
and
informative
post
-
query
organization
(
automated
document
clustering
by
topic
)
to
decrease
real
search
time
on
the
part
of
the
user
.

Most
existing
Information
Retrieval
systems
depend
on
the
user
to
perform
intelligent
,
specific
queries
with
Boolean
operators
in
order
to
minimize
the
set
of
returned
documents
.

The
user
essentially
must
guess
the
appropriate
keywords
before
performing
the
query
.

Other
systems
use
a
vector
space
model
which
is
more
suitable
to
performing
the
document
similarity
operations
which
permit
hierarchical
clustering
of
returned
documents
by
topic
.

This
allows
"
post
query
"
refinement
by
the
user
.

The
system
we
propose
is
a
hybrid
beween
these
two
systems
,
compatibile
with
the
former
,
while
providing
the
enhanced
document
organization
permissable
by
the
latter
.


-DOCSTART-

Identifying
the
most
important
or
prominent
actors
in
a
network
has
been
an
area
of
much
interest
in
Social
Network
Analysis
dating
back
to
Moreno
â€™s
work
in
the
1930
â€™s

[
1
This
interest
has
spurred
the
formulation
of
many
graph
-
based
sociometrics
for
ranking
actors
in
complex
physical
,
biological
and
social
networks
.

These
sociometrics
are
usually
based
on
intuitive
notions
such
as
access
and
control
over
resources
,
or
brokerage
of
information
[
2
and
has
yielded
measures
such
as
Degree
Centrality
,
Closeness
Centrality
and
Betweeness
Centrality
[
3

In
the
exploratory
analysis
of
networks
,
the
question
of
whether
these
measures
of
centrality
really
capture
what
we
mean
by
â€œ
importance
â€
is
often
not
directly
addressed
.

However
,
when
such
sociometrics
start
being
used
to
drive
decisions
in
more
quantitative
fields
,
there
emerges
a
need
to
empirically
answer
this
question
.

Probably
the
most
popular
of
these
measures
in
the
Computer
Science
community
is
PageRank
,
which
is
a
variant
of
Eigenvector
Centrality
[
4
Once
its
use
in
Information
Retrieval
(
IR
)
and
Web
search
in
particular
became
popular
,
it
led
to
more
rigorous
evaluation
of
PageRank
and
variants
on
measurable
IR
tasks
[
5
With
the
rise
of
Web
2.0
,
with
its
focus
on
user
-
generated
content
and
social
networks
,
various
socio
-
metrics
are
being
increasingly
used
to
produce
ranked
lists
of
â€œ
top
â€
bloggers
,
twitterers
,
etc
.

For
example
,
Twitterholic.com
and
WeFollow.com
use
degree
centrality
(
number
of
followers
while
TunkRank.com
uses
a
variant
of
PageRank
to
rank
users
of
the
micro
-
blogging
service
Twitter
.

In
the
domain
of
blogs
,
Technorati
assigns
an
authority
score
to
a
blogger
based
on
the
number
of
blogs
linking
to
her
website
in
the
last
six
months
.

Similarly
,
Blogpulse
ranks
blogs
based
on
the
number
of
times
it
â€™s
cited
by
other
bloggers
over
the
last
30
days
.

Do
these
rankings
really
identify
â€œ
influential
â€
authors
,
and
if
so
,
which
ranking
is
better
?

With
the
increased
demand
for
Social
Media
Analytics
,
with
its
focus
on
deriving
marketing
insight
from
the
analysis
of
blogs
and
other
social
media
,
there
â€™s
a
growing
need
to
address
this
question
.

This
paper
is
a
step
in
that
direction
.

It
is
our
position
,
that
the
question
of
whether
a
particular
influence
measure
is
good
is
ill
-
posed
,
unless
it
is
put
in
the
context
of
a
measurable
task
or
desired
outcome
.

Constructing
such
predictive
tasks
of
interest
,
not
only
guides
the
choice
of
relationships
we
build
a
network
on
,
but
also
allows
for
the
quantitative
comparison
of
different
socio
-
metrics
.

In
this
paper
,
we
present
a
case
study
on
data
collected
for
40
million
Twitter
accounts
.

We
look
at
marketing
-
driven
tasks
,
such
as
detecting
the
potential
for
viral
outbreaks
of
messages
(
tweets
We
build
three
different
graphs
based
on
the
network
of
followers
,
rebroadcast
(
retweet
)
networks
,
and
the
network
of
replies
and
mentions
.

We
conduct
a
similar
study
on
detecting
the
influence
of
publications
,
through
the
analysis
of
citation
networks
.

Extensive
empirical
results
demonstrate
that
different
measures
provide
the
best
ranking
for
these
tasks
underscoring
the
importance
of
addressing
the
question
of
influence
based
on
a
desired
objective
.

Taking
a
predictive
perspective
of
measures
of
influence
can
also
suggest
alternative
socio
-
metrics
,
and
we
show
that
combining
aspects
of
different
measures
produces
a
composite
ranking
mechanism
that
is
most
beneficial
for
each
desired
predictive
task
.

We
compare
several
approaches
to
combining
influence
measures
through
rank
aggregation
methods
,
such
as
approximations
of
Kemeny
optimal
aggregation
[
6

In
addition
,
we
introduce
novel
supervised
rank
aggregation
techniques
that
leverage
the
ground
truth
on
a
subset
of
users
to
further
improve
ranking
.

We
demonstrate
the
efficacy
of
these
methods
compared
to
several
baseline
approaches
.


-DOCSTART-

Classification
of
reusable
software
components
is
essential
to
successful
software
reuse
initiatives
and
a
critical
feature
of
library
development
.

This
paper
provides
a
survey
of
storage
and
retrieval
methods
and
highlights
the
main
characteristics
of
each
class
of
methods
.

The
work
focuses
on
information
retrieval
methods
with
emphasis
on
Component
Rank
and
Latent
Semantic
Analysis
models
that
are
applied
to
component
classification
.

These
models
have
shown
to
provide
efficient
storage
and
retrieval
algorithms
as
they
narrow
the
results
of
a
user
query
and
provide
accurate
component
selection
.

Thus
,
leading
to
efficient
reusable
repository
.

Reuse
Libraries
,
Asset
Classification
,
Asset
Retrieval
,
and
Retrieval
Methods
.


-DOCSTART-

The
growth
in
the
volume
of
text
data
such
as
books
and
articles
in
libraries
for
centuries
has
imposed
to
establish
effective
mechanisms
to
locate
them
.

Early
techniques
such
as
abstraction
,
indexing
and
the
use
of
classification
categories
have
marked
the
birth
of
a
new
field
of
research
called
"
Information
Retrieval
Information
Retrieval
(
IR
)
can
be
defined
as
the
task
of
defining
models
and
systems
whose
purpose
is
to
facilitate
access
to
a
set
of
documents
in
electronic
form
(
corpus
)
to
allow
a
user
to
find
the
relevant
ones
for
him
,
that
is
to
say
,
the
contents
which
matches
with
the
information
needs
of
the
user
.

Most
of
the
models
of
information
retrieval
use
a
specific
data
structure
to
index
a
corpus
which
is
called
"
inverted
file
"
or
"
reverse
index
This
inverted
file
collects
information
on
all
terms
over
the
corpus
documents
specifying
the
identifiers
of
documents
that
contain
the
term
in
question
,
the
frequency
of
each
term
in
the
documents
of
the
corpus
,
the
positions
of
the
occurrences
of
the
word
In
this
paper
we
use
an
oriented
object
database
(
db4o
)
instead
of
the
inverted
file
,
that
is
to
say
,
instead
to
search
a
term
in
the
inverted
file
,
we
will
search
it
in
the
db4o
database
.

The
purpose
of
this
work
is
to
make
a
comparative
study
to
see
if
the
oriented
object
databases
may
be
competing
for
the
inverse
index
in
terms
of
access
speed
and
resource
consumption
using
a
large
volume
of
data
.

Keywords
â€”
Information
Retrieval
,
indexation
,
oriented
object
database
(
db4o
inverted
file
.


-DOCSTART-

This
paper
reports
on
a
study
that
explored
the
needs
and
challenges
with
respect
to
the
creation
of
a
collaboratory
for
liÂ­
brary
and
information
science
practitioners
.

To
identify
needs
and
challenges
interviews
were
conducted
with
practitioners
at
a
variety
of
institutions
.

The
results
suggest
that
there
is
a
need
for
a
collaboratory
to
facilitate
on
-
demand
,
personalized
knowledge
sharing
.

The
collaboratory
should
also
be
well
integrated
into
the
everyday
practice
of
library
and
information
science
practitioners
.


-DOCSTART-

D
imensionality
reduction
is
one
of
the
basic
operations
in
the
toolbox
of
data
analysts
and
designers
of
ma
chine
learning
and
pattern
recognition
systems
.

Given
a
large
set
of
measured
variables
but
few
observations
,
an
obvious
idea
is
to
reduce
the
degrees
of
freedom
in
the
measurements
by
representing
them
with
a
smaller
set
of
more
â€œ
condensed
â€
variables
.

Another
reason
for
reducing
the
dimensionality
is
to
reduce
computational
load
in
further
processing
.

A
third
reason
is
visualization
Looking
at
the
data
â€
is
a
central
ingredient
of
exploratory
data
analysis
,
the
first
stage
of
data
analysis
where
the
goal
is
to
make
sense
of
the
data
before
proceeding
with
more
goal
-
directed
modeling
and
analyses
.

It
has
turned
out
that
although
these
different
tasks
seem
alike
,
their
solution
requires
different
tools
.

In
this
article
,
we
show
that
dimensionality
reduction
for
data
visualization
can
be
represented
as
an
information
retrieval
task
,
where
the
quality
of
visualization
can
be
measured
by
precision
and
recall
measures
and
their
smoothed
extensions
.

Furthermore
,
we
show
that
visualization
can
be
optimized
to
directly
maximize
the
quality
for
any
desired
tradeoff
between
precision
and
recall
,
yielding
very
well
-
performing
visualization
methods
.


-DOCSTART-

One
hundred
users
,
one
hundred
needs
.

As
more
and
more
topics
are
being
discussed
on
the
web
and
our
vocabulary
remains
relatively
stable
,
it
is
increasingly
difficult
to
let
the
search
engine
know
what
we
want
.

Coping
with
ambiguous
queries
has
long
been
an
important
part
of
the
research
on
Information
Retrieval
,
but
still
remains
a
challenging
task

Personalized
search</i
>
has
recently
got
significant
attention
in
addressing
this
challenge
in
the
web
search
community
,
based
on
the
premise
that
a
user
's
general
preference
may
help
the
search
engine
disambiguate
the
true
intention
of
a
query
.

However
,
studies
have
shown
that
users
are
reluctant
to
provide
any
explicit
input
on
their
personal
preference
.

In
this
paper
,
we
study
how
a
search
engine
can
learn
a
user
's
preference
<
i
>
automatically</i
>
based
on
her
past
click
history
and
how
it
can
use
the
user
preference
to
personalize
search
results
.

Our
experiments
show
that
users
'
preferences
can
be
learned
accurately
even
from
little
click
-
history
data
and
personalized
search
based
on
user
preference
yields
significant
improvements
over
the
best
existing
ranking
mechanism
in
the
literature
.


-DOCSTART-

With
the
widespread
acceptance
of
three
-
dimensional
modeling
techniques
,
high
-
speed
hardware
,
and
relatively
low
-
cost
computation
,
modeling
and
animating
one
or
more
human
figures
for
the
purposes
of
design
assessment
,
human
factors
,
task
simulation
,
and
human
movement
understanding
has
become
feasible
outside
the
animation
production
house
environment
.

This
tutorial
will
address
the
state
-
of
-
the
-
art
in
human
figure
geometric
modeling
,
figure
positioning
,
figure
animation
,
and
task
simulation
.

Disciplines
Computer
Engineering
Computer
Sciences
Comments
University
of
Pennsylvania
Department
of
Computer
and
Information
Science
Technical
Report

No
.
MSCIS-86
-
88
.

This
technical
report
is
available
at
ScholarlyCommons
:
http
repository.upenn.edu/cis_reports/995
MODELING
Â·
AND
ANIMATING
HUMAN
FIGURES
IN
A
CAD
ENVIRONMENT

Dr.
Norman
Badler
MS
-
CIS-86
-
88
GRAPHICS
LAB

14
Department
Of
Computer
and
Information
Science
School
of
Engineering
and
Applied
Science
University
of
Pennsylvania
Philadelphia
,
PA
19104
-
6389


-DOCSTART-

Product
or
company
names
used
in
this
set
are
for
identification
purposes
only
.

Inclusion
of
the
names
of
the
products
or
companies
does
not
indicate
a
claim
of
ownership
by
IGI
Global
of
the
trademark
or
registered
trademark
.

Encyclopedia
of
information
science
and
technology
Mehdi
Khosrow
-
Pour
,
editor.-2nd
ed
.

Includes
bibliographical
references
and
index
.

This
set
of
books
represents
a
detailed
compendium
of
authoritative
,
research
-
based
entries
that
define
the
contemporary
state
of
knowledge
on
technology
"-
Provided
by
publisher
.

All
work
contributed
to
this
encyclopedia
set
is
original
material
.

The
views
expressed
in
this
encyclopedia
set
are
those
of
the
authors
,
but
not
necessarily
of
the
publisher
.

Note
to
Librarians
:
If
your
institution
has
purchased
a
print
edition
of
this
publication
,
please
go
to
http
www.igi-global.com/agree-ment
for
information
on
activating
the
library
's
complimentary
online
access
.


-DOCSTART-

The
usage
of
single
geographical
footprint
model
in
existing
Geographical
Information
Retrieval
(
GIR
)
system
will
cause
many
problems
like
overestimation
or
underestimation
of
the
geographical
scopes
for
documents
to
search
.

To
be
honest
,
the
single
geographical
footprint
model
is
not
applicable
in
modern
GIR
system
although
it
is
simple
,
fast
and
widely
applied
today
.

In
order
to
improve
the
quality
of
answers
given
to
a
spatial
query
,
a
new
model
as
well
as
a
dedicated
algorithm
will
be
proposed
to
study
the
geographical
information
attached
to
documents
.

Besides
,
a
dedicated
algorithm
based
on
network
graph
,
which
is
inspired
by
the
Google
PageRank
and
Bayesian
network
theory
,
is
also
invented
to
estimate
the
geographical
similarity
between
document


-DOCSTART-

when
implementing
unfamiliar
programming
tasks
,
developers
commonly
search
code
examples
and
learn
usage
patterns
of
APIs
from
the
code
examples
or
reuse
them
by
copy
-
pasting
and
modifying
.

For
providing
high
-
quality
code
examples
,
previous
studies
present
several
methods
to
recommend
code
snippets
mainly
based
on
information
retrieval
.

In
this
paper
,
to
provide
better
recommendation
results
,
we
propose
ROSF
,
Recommending
cOde
Snippets
with
multi
-
aspect
Features
,
a
novel
method
combining
both
information
retrieval
and
supervised
learning
.

In
our
method
,
we
recommend
Top
-
K
code
snippets
for
a
given
free
-
form
query
based
on
two
stages
,
i.e
coarse
-
grained
searching
and
fine
-
grained
re
-
ranking
.

First
,
we
generate
a
code
snippet
candidate
set
by
searching
a
code
snippet
corpus
using
an
information
retrieval
method
.

Second
,
we
predict
probability
values
of
the
code
snippets
for
different
relevance
scores
in
the
candidate
set
by
the
learned
prediction
model
from
a
training
set
,
re
-
rank
these
candidate
code
snippets
according
to
the
probability
values
,
and
recommend
the
final
results
to
developers
.

We
conduct
several
experiments
to
evaluate
our
method
in
a
large
-
scale
corpus
containing
921,713
real
-
world
code
snippets
.

The
results
show
that
ROSF
is
an
effective
method
for
code
snippets
recommendation
and
outperforms
thestate
-
of
-
the
-
art
methods
by
20
%
41
%
in
Precision
and
13
%
33
%
in
NDCG
.


-DOCSTART-

Peer
-
to
-
peer
(
P2P
)
Data
-
sharing
systems
now
generate
a
significant
portion
of
Internet
traffic
.

P2P
systems
have
emerged
as
an
accepted
way
to
share
enormous
volumes
of
data
.

Needs
for
widely
distributed
information
systems
supporting
virtual
organizations
have
given
rise
to
a
new
category
of
P2P
systems
called
schema
-
based
.

In
such
systems
each
peer
is
a
database
management
system
in
itself
,
ex
-
posing
its
own
schema
.

In
such
a
setting
,
the
main
objective
is
the
efficient
search
across
peer
databases
by
processing
each
incoming
query
without
overly
consuming
bandwidth
.

The
usability
of
these
systems
depends
on
successful
techniques
to
find
and
retrieve
data
;
however
,
efficient
and
effective
routing
of
content
-
based
queries
is
an
emerging
problem
in
P2P
networks
.

This
work
was
attended
as
an
attempt
to
motivate
the
use
of
mining
algorithms
in
the
P2P
context
may
improve
the
significantly
the
efficiency
of
such
methods
.

Our
proposed
method
based
respectively
on
combination
of
clustering
with
hypergraphs
.

We
use
ECCLAT
to
build
approximate
clustering
and
discovering
meaningful
clusters
with
slight
overlapping
.

We
use
an
algorithm
MTMINER
to
extract
all
minimal
transversals
of
a
hypergraph
(
clusters
)
for
query
routing
.

The
set
of
clusters
improves
the
robustness
in
queries
routing
mechanism
and
scalability
in
P2P
Network
.

We
compare
the
performance
of
our
method
with
the
baseline
one
considering
the
queries
routing
problem
.

Our
experimental
results
prove
that
our
proposed
methods
generate
impressive
levels
of
performance
and
scalability
with
with
respect
to
important
criteria
such
as
response
time
,
precision
and
recall
.


-DOCSTART-

Many
today
â€™s
online
communities
use
TagClouds
,
an
aesthetic
and
easy
to
understand
visualization
,
to
represent
popular
tags
collaboratively
generated
by
their
users
.

However
,
due
to
the
free
nature
of
tagging
,
such
collaborative
tags
have
some
linguistic
problems
and
certain
other
intrinsic
limitations
,
such
as
high
semantic
density
.

Moreover
,
the
alphabetical
order
of
TagClouds
poorly
supports
a
hierarchical
exploration
among
tags
.

This
paper
presents
an
exploration
to
support
semantic
understanding
of
collaborative
tags
beyond
TagClouds
.

Based
on
the
results
of
our
survey
on
people
â€™s
practical
usages
of
collaborative
tags
,
we
developed
a
visualization
named
TagClusters
,
in
which
tags
are
clustered
into
different
groups
,
with
font
size
representing
tag
popularity
and
the
spatial
distance
indicating
the
semantic
similarity
between
tags
.

The
subgroups
in
each
group
and
the
overlap
between
groups
are
highlighted
,
thus
illustrating
the
underlying
hierarchical
structure
and
semantic
relations
between
groups
.

We
conducted
a
comparative
evaluation
with
TagClouds
and
TagClusters
based
on
the
same
tag
set
.

We
received
overall
positive
feedback
on
TagClusters
and
the
results
confirmed
the
advantage
of
TagClusters
in
facilitating
browsing
,
comparing
and
comprehending
semantic
relations
between
tags
.

In
future
work
,
besides
supporting
semantic
browsing
,
we
will
explore
other
usages
of
TagClusters
,
such
as
tag
suggestions
or
tag
-
based
Information
Retrieval
.

Keywords
Improvement
of
TagClouds
,
collaborative
tagging
,
user
-
contributed
tags
,
visualization
of
tags
,
semantic
analysis
.


-DOCSTART-

This
paper
describes
a
system
which
enables
users
to
create
on
-
the
-
fly
queries
which
involve
not
just
keywords
,
but
also
sortal
constraints
and
linguistic
constraints
.

The
user
can
specify
how
the
results
should
be
presented
e.g.
in
terms
of
links
to
documents
,
or
as
table
entries
.

The
aim
is
to
bridge
the
gap
between
keyword
based
Information
Retrieval
and
pattern
based
Information
Extraction
.


-DOCSTART-

The
study
used
naturalistic
methods
to
investigate
the
metacognitive
knowledge
of
10
adolescents
as
they
searched
for
,
selected
,
evaluated
,
and
used
information
for
a
school
-
based
,
inquiry
project
.

The
study
identified
thirteen
attributes
of
metacognitive
knowledge
related
to
the
information
search
process
:
Knowing
your
strengths
and
weaknesses
,
knowing
that
you
do
nâ€™t
know
,
building
a
base
,
scaffolding
,
communicating
,
changing
course
,
balancing
,
understanding
curiosity
,
understanding
time
and
effort
,
understanding
memory
,
pulling
back
and
reflecting
,
connecting
,
and
parallel
thinking
.

The
results
contribute
to
the
understanding
of
adolescent
information
-
seeking
behavior
and
have
implications
for
information
literacy
instruction
.

Annotation
:

Respondents
are
equivalent
to
â€œ
grade
12
â€
in
age
and
study
but
,
because
of
the
Montreal
school
system
,
are
first
-
year
college
students
.

Therefore
,
is
useful
in
highlighting
the
gap
(
or
lack
thereof
)
between
high
school
seniors
and
college
freshmen
.

Article
focuses
more
on
theory
of
metacognition
thinking
about
thinking
than
study
results
.

Discusses
and
uses
Kuhlthau
's
Information
Search
Process
model
.

Study
used
multiple
methods
of
collecting
data
with
a
final
interview
months
after
the
school
project
was
complete
.

Search
Strategy
:

I
felt
I
was
relying
on
Dialog
too
much
,
so
I
decided
to
do
a
bunch
of
searching
in
Google
Scholar
to
see
how
that
would
work
for
me
.

Obviously
,
not
well
,
as
this
is
the
only
thing
I
ended
up
using
.

Everything
I
found
was
either
irrelevant
or
had
already
been
covered
in
my
two
Dialog
citation
searches
.

Database
:
Google
Scholar
Method
of
Searching
:

Keyword
search
Search
String
:
information
seeking
behavior
teen
Chung
,
J.S
Neuman
,
D
2007
High
school
studentsâ€™
information
seeking
and
use
for
class
projects
.

Journal
of
the
American
Society
for
Information
Science
and
Technology
,
58(10
1503
-
1517
.

This
study
details
the
activities
and
strategies
that
11th
grade
students
with
high
academic
abilities
used
during
their
information
seeking
and
use
to
complete
class
projects
in
a
Persuasive
Speech
class
.

The
study
took
place
in
a
suburban
high
school
in
Maryland
,
and
participants
included
21
junior
honors
students
,
their
teacher
,
and
their
library
media
specialist
.

The
study
used
data
collected
from
observations
,
individual
interviews
,
and
documents
students
produced
for
their
projectsâ€”
concept
maps
,
paragraphs
,
outlines
,
and
research
journals
.

The
findings
show
that
studentsâ€™
understanding
,
strategies
,
and
activities
during
information
seeking
and
use
were
interactive
and
serendipitous
and
that
students
learned
about
their
topics
as
they
searched
.

The
research
suggests
that
high
school
honors
students
in
an
information
-
rich
environment
are
especially
confident
with
learning
tasks
requiring
an
exploratory
mode
of
learning
.

Annotation
:

This
study
focuses
on
classroom
behavior
in
a
middle
-
class
environment
.

Authors
use
many
examples
from
the
research
to
illustrate
their
points
.

Cites
â€œ
Principle
of
Least
Effort

â€
in
describing
the
teens
'
searches
for
information
.

Students
show
high
level
of
understanding
credibility
and
evaluation
of
sources
but
tend
to
use
keyword
and
natural
language
,
rather
than
Boolean
,
searches
.

Authors
call
for
a
closer
look
at
interactive
search
-
based
education
for
students
over
analytical
-
search
based
education
by
teachers
and
library
media
specialists
.

Search
Strategy
:

Having
discovered
the
wealth
of
Shenton
's
contributions
to
the
youth
-
based
information
behavior
body
of
work
,
it
seemed
like
a
good
idea
to
see
who
was
citing
him
.

InfoSci
(
Social
SciSearch
Dialog
]

Method
of
Searching
:
Citation
search
Search
String
:
s
ca
=
shenton

ak
Howard
,
V
Jin
,
S
2004

What
are
they
reading
?

A
survey
of
the
reading
habits
and
library
usage
patterns
of
teens
in
Nova
Scotia
.

Canadian
Journal
of
Information
and
Library
Science
,
28(4
25
-
44
.

This
paper
reports
on
a
2002
-
03
survey
of
the
reading
habits
and
library
usage
patterns
of
Nova
Scotia
teens
.

Overall
,
this
study
determined
that
84
%
of
Nova
Scotia
teens
read
at
least
one
book
a
year
for
pleasure
.

However
,
the
reading
gap
between
the
genders
appears
to
be
widening
,
and
both
reading
and
library
use
appear
to
decline
with
age
.

While
libraries
in
Nova
Scotia
are
still
an
important
resource
for
research
and
homework
help
,
chain
bookstores
are
rapidly
gaining
popularity
and
are
the
preferred
source
of
pleasure
reading
material
.

Teens
do
not
perceive
that
librarians
influence
their
leisure
reading
choices
.

Annotation
:

This
study
puts
Nova
Scotia
teens
within
the
framework
of
previous
and
current
Canadian
studies
and
finds
that
Nova
Scotia
teens
read
more
for
pleasure
.

Library
usage
,
however
,
is
seen
as
â€œ
uncool
â€
despite
teens
using
libraries
for
leisure
reading
,
school
-
related
work
,
and
internet
access
,
especially
among
more
rural
teens
.

Study
focuses
on
8th
grades
and
11th
graders
,
which
shows
distinct
differences
between
the
age
groups
,
and
breaks
down
results
by
age
and
gender
.

Parents
'
educational
backgrounds
are
also
taken
into
account
,
which
makes
for
interesting
results
.

This
paper
reports
on
a
2002
-
03
survey
of
the
reading
habits
and
library
usage
patterns
of
Nova
Scotia
teens
.

Overall
,
this
study
determined
that
84
%
of
Nova
Scotia
teens
read
at
least
one
book
a
year
for
pleasure
.

However
,
the
reading
gap
between
the
genders
appears
to
be
widening
,
and
both
reading
and
library
use
appear
to
decline
with
age
.

While
libraries
in
Nova
Scotia
are
still
an
important
resource
for
research
and
homework
help
,
chain
bookstores
are
rapidly
gaining
popularity
and
are
the
preferred
source
of
pleasure
reading
material
.

Teens
do
not
perceive
that
librarians
influence
their
leisure
reading
choices
.

Annotation
:

This
study
puts
Nova
Scotia
teens
within
the
framework
of
previous
and
current
Canadian
studies
and
finds
that
Nova
Scotia
teens
read
more
for
pleasure
.

Library
usage
,
however
,
is
seen
as
â€œ
uncool
â€
despite
teens
using
libraries
for
leisure
reading
,
school
-
related
work
,
and
internet
access
,
especially
among
more
rural
teens
.

Study
focuses
on
8th
grades
and
11th
graders
,
which
shows
distinct
differences
between
the
age
groups
,
and
breaks
down
results
by
age
and
gender
.

Parents
'
educational
backgrounds
are
also
taken
into
account
,
which
makes
for
interesting
results
.

Search
Strategy
:

Was
searching
for
anther
document
and
came
across
this
in
the
same
journal
.

N
/
A
Method
of
Searching
:
Serendipity
?


-DOCSTART-

The
integration
of
hypermedia
with
information
retrieval
is
usually
used
to
overcome
user
disorientation
problems
.

However
previous
approaches
have
generally
considered
only
text
retrieval
[
7
]
and
if
real
content
based
multimedia
information
retrieval
capabilities
are
used
[
12
effectiveness
and
efficiency
are
very
low
,
when
compared
with
text
retrieval
.

This
paper
describes
the
development
of
a
hypermedia
information
retrieval
system
that
uses
automatically
created
text
descriptors
to
obtain
multimedia
information
retrieval
capability
,
using
text
retrieval
techniques
.

There
are
two
main
aspects
under
consideration
:
Automatic
creation
of
text
descriptors
for
multimedia
documents
,
using
information
taken
from
the
hypermedia
network
,
at
the
document
and
abstract
classification
level
.

Integration
of
open
hypermedia
systems
,
in
this
case
the
Microcosm
system
[
4
with
text
information
retrieval
tools
to
allow
multimedia
retrieval
using
text
descriptors
.

At
the
end
of
this
paper
we
present
some
conclusions
about
the
effectiveness
of
such
a
system
and
improvements
are
suggested
for
further
research
.


-DOCSTART-

Aalto
University
,
P.O.
Box
11000
,

FI-00076
Aalto
www.aalto.fi
Author
Jarkko
Ylipaavalniemi
Name
of
the
doctoral
dissertation
Data
-
driven
Analysis
for
Natural
Studies
in
Functional
Brain
Imaging
Publisher
School
of
Science
Unit
Department
of
Information
and
Computer
Science
Series
Aalto
University
publication
series

DOCTORAL
DISSERTATIONS
70/2013
Field
of
research
Computer
and
Information
Science
Manuscript
submitted
5
November
2012
Date
of
the
defence
3
May
2013
Permission
to
publish
granted
(
date
)
20
December
2012
Language
English
Monograph
Article
dissertation
(
summary
original
articles
)

In
neuroscience
,
functional
magnetic
resonance
imaging
(
fMRI
)
has
become
a
powerful
tool
in
human
brain
mapping
.

Typically
,
fMRI
is
used
with
a
rather
simple
stimulus
sequence
,
aiming
at
improving
signal
-
to
-
noise
ratio
for
statistical
hypothesis
testing
.

When
natural
stimuli
are
used
,
the
simple
designs
are
no
longer
appropriate
.

The
aim
of
this
thesis
is
in
developing
data
-
driven
approaches
for
reliable
inference
of
brain
correlates
to
natural
stimuli
.

Since
the
beginning
of
the
nineteenth
century
,
neuroscience
has
focused
on
the
idea
that
distinct
regions
of
the
brain
support
particular
mental
processes
.

However
,
modern
research
recognizes
that
many
functions
rely
on
distributed
networks
,
and
that
a
single
brain
region
may
participate
in
more
than
one
function
.

These
rapid
paradigm
changes
in
neuroscience
raise
important
methodological
challenges
.

Purely
hypothesis
-
driven
methods
have
been
used
extensively
in
functional
imaging
studies
.

As
the
focus
in
brain
research
is
shifting
away
from
functional
specialization
towards
interaction
-
based
functional
networks
,
those
approaches
are
no
longer
appropriate
.

In
contrast
to
the
classic
statistical
hypothesis
testing
approaches
,
modern
machine
learning
methods
allow
for
a
purely
data
-
driven
way
to
describe
the
data
.

They
do
not
use
the
stimuli
,
and
make
no
assumptions
about
whether
the
brain
processes
are
stimulus
related
or
not
.

The
recordings
for
each
brain
region
may
contain
a
complicated
mixture
of
activity
,
which
is
produced
by
many
spatially
distributed
processes
,
and
artifacts
.

Each
process
can
be
described
as
a
component
having
a
separate
time
series
and
spatial
extent
,
and
producing
simultaneous
changes
in
the
fMRI
signals
of
many
regions
.

The
main
contribution
of
the
thesis
is
a
reliable
independent
component
analysis
(
ICA
)
approach
,
which
is
available
in
the
Arabica
toolbox
.

The
usefulness
of
the
approach
was
tested
extensively
with
fMRI
data
,
showing
that
the
method
is
capable
of
providing
insights
into
the
data
that
would
not
be
attainable
otherwise
.

The
new
method
was
also
theoretically
analyzed
and
its
asymptotic
convergence
was
proven
.

The
theory
offers
a
thorough
explanation
of
how
the
method
works
and
justifies
its
use
in
practice
.

Then
,
the
new
method
is
further
developed
for
analyzing
networks
of
distributed
brain
activity
,
by
combining
it
with
canonical
correlation
analysis
(
CCA
The
extension
was
shown
to
be
particularly
useful
with
fMRI
studies
that
use
natural
stimuli
.

The
approach
is
further
extended
to
be
applicable
in
cases
where
independent
subspaces
emerge
,
which
often
happens
when
using
real
measurement
data
that
is
not
guaranteed
to
fit
all
the
assumptions
made
in
the
development
of
the
methods
.

In
neuroscience
,
functional
magnetic
resonance
imaging
(
fMRI
)
has
become
a
powerful
tool
in
human
brain
mapping
.

Typically
,
fMRI
is
used
with
a
rather
simple
stimulus
sequence
,
aiming
at
improving
signal
-
to
-
noise
ratio
for
statistical
hypothesis
testing
.

When
natural
stimuli
are
used
,
the
simple
designs
are
no
longer
appropriate
.

The
aim
of
this
thesis
is
in
developing
data
-
driven
approaches
for
reliable
inference
of
brain
correlates
to
natural
stimuli
.

Since
the
beginning
of
the
nineteenth
century
,
neuroscience
has
focused
on
the
idea
that
distinct
regions
of
the
brain
support
particular
mental
processes
.

However
,
modern
research
recognizes
that
many
functions
rely
on
distributed
networks
,
and
that
a
single
brain
region
may
participate
in
more
than
one
function
.

These
rapid
paradigm
changes
in
neuroscience
raise
important
methodological
challenges
.

Purely
hypothesis
-
driven
methods
have
been
used
extensively
in
functional
imaging
studies
.

As
the
focus
in
brain
research
is
shifting
away
from
functional
specialization
towards
interaction
-
based
functional
networks
,
those
approaches
are
no
longer
appropriate
.

In
contrast
to
the
classic
statistical
hypothesis
testing
approaches
,
modern
machine
learning
methods
allow
for
a
purely
data
-
driven
way
to
describe
the
data
.

They
do
not
use
the
stimuli
,
and
make
no
assumptions
about
whether
the
brain
processes
are
stimulus
related
or
not
.

The
recordings
for
each
brain
region
may
contain
a
complicated
mixture
of
activity
,
which
is
produced
by
many
spatially
distributed
processes
,
and
artifacts
.

Each
process
can
be
described
as
a
component
having
a
separate
time
series
and
spatial
extent
,
and
producing
simultaneous
changes
in
the
fMRI
signals
of
many
regions
.

The
main
contribution
of
the
thesis
is
a
reliable
independent
component
analysis
(
ICA
)
approach
,
which
is
available
in
the
Arabica
toolbox
.

The
usefulness
of
the
approach
was
tested
extensively
with
fMRI
data
,
showing
that
the
method
is
capable
of
providing
insights
into
the
data
that
would
not
be
attainable
otherwise
.

The
new
method
was
also
theoretically
analyzed
and
its
asymptotic
convergence
was
proven
.

The
theory
offers
a
thorough
explanation
of
how
the
method
works
and
justifies
its
use
in
practice
.

Then
,
the
new
method
is
further
developed
for
analyzing
networks
of
distributed
brain
activity
,
by
combining
it
with
canonical
correlation
analysis
(
CCA
The
extension
was
shown
to
be
particularly
useful
with
fMRI
studies
that
use
natural
stimuli
.

The
approach
is
further
extended
to
be
applicable
in
cases
where
independent
subspaces
emerge
,
which
often
happens
when
using
real
measurement
data
that
is
not
guaranteed
to
fit
all
the
assumptions
made
in
the
development
of
the
methods
.


-DOCSTART-

The
first
step
in
most
empirical
work
in
multilingual
NLP
is
to
construct
maps
of
the
correspondence
between
texts
and
their
translations
(
b
i
t
ex
t
maps
The
Smooth
Injective
Map
Recognizer
(
SIMR
)

algorithm
presented
here
is
a
generic
pattern
recognition
algorithm
that
is
particularly
well
-
suited
to
mapping
bitext
correspondence
.

SIMR
is
faster
and
significantly
more
accurate
than
other
algorithms
in
the
literature
.

The
algorithm
is
robust
enough
to
use
on
noisy
texts
,
such
as
those
resulting
from
OCR
input
,
and
on
translations
that
are
not
very
literal
.

SIMR
encapsulates
its
language
-
specific
heuristics
,
so
that
it
can
be
ported
to
any
language
pair
with
a
minimal
effort
.

Texts
that
are
available
in
two
languages
(
bitexts
)
are
immensely
valuable
for
many
natural
language
processing
applications

z.
Bitexts
are
the
raw
material
from
which
translation
models
are
built
.

In
addition
to
their
use
in
machine
translation
(
Sato
Nagao
,
1990
;
Brown
et
al
1993
;
Melamed
,
1997
translation
models
can
be
applied
to
machineassisted
translation
(
Sato
,
1992
;
Foster
et
al
1996
cross
-
lingual
information
retrieval
(
SIGIR
,
1996
and
gisting
of
World
Wide
Web
pages
(
Resnik
,
1997

Bitexts
also
play
a
role
in
less
automated
applications
such
as
concordancing
for
bilingual
lexicography
(
Catizone
et
al
1993
;
Gale
Church
,
1991b
computer
-
assisted
language
learning
,
and
tools
for
translators
(
e.g
Macklovitch
,
1
"
Multitexts
"
in
more
than
two
languages
are
even
more
valuable
,
but
they
are
much
more
rare
.

1995
;
Melamed
,
1996b

However
,
bitexts
are
of
little
use
without
an
automatic
method
for
constructing
bitext
maps
.

Bitext
maps
identify
corresponding
text
units
between
the
two
halves
of
a
bitext
.

The
ideal
bitext
mapping
algorithm
should
be
fast
and
accurate
,
use
little
memory
and
degrade
gracefully
when
faced
with
translation
irregularities
like
omissions
and
in
.

It
should
be
applicable
to
any
text
genre
in
any
pair
of
languages
.

The
Smooth
Injective
Map
Recognizer
(
SIMR
)
algorithm
presented
in
this
paper
is
a
bitext
mapping
algorithm
that
advances
the
state
of
the
art
on
these
criteria
.

The
evaluation
in
Section
5
shows
that
SIMR
's
error
rates
are
lower
than
those
of
other
bitext
mapping
algorithms
by
an
order
of
magnitude
.

At
the
same
time
,
its
expected
running
time
and
memory
requirements
are
linear
in
the
size
of
the
input
,
better
than
any
other
published
algorithm
.

The
paper
begins
by
laying
down
SIMR
's
geometric
foundations
and
describing
the
algorithm
.

Then
,
Section
4
explains
how
to
port
SIMR
to
arbitrary
language
pairs
with
minimal
effort
,
without
relying
on
genre
-
specific
information
such
as
sentence
boundaries
.

The
last
section
offers
some
insights
about
the
optimal
level
of
text
analysis
for
mapping
bitext
correspondence
.

i
t
e
x
t
G
e
o

e
x
t
(
Harris
,
1988
)
comprises
two
versions
of
a
text
,
such
as
a
text
in
two
different
languages
.

Translators
create
a
bitext
each
time
they
translate
a
text
.

Each
bitext
defines
a
rectangular

b
i
t
e
x
t
space
,
as
illustrated
in
Figure
1
.

The
width
and
height
of
the
rectangle
are
the
lengths
of
the
two
component
texts
,
in
characters
.

The
lower
left
corner
of
the
rectangle
is
the
or
ig
in
of
the
bitext
space
and
represents
the
two
texts
'
beginnings
.

The
upper
right
corner
is
the
t
e
r
m

i
n
u
s
and
represents
the
texts
'
ends
.

The
line
between
the
origin
and
the


-DOCSTART-

We
proposed
in
[
7
]
a
nested
relational
calculus
and
a
nested
relational
algebra
based
on
structural
recursion
[
6,5
]
and
on
monads
[
27,16

In
this
report
,
we
describe
relative
set
abstraction
as
our
third
nested
relational
query
language
.

This
query
language
is
similar
to
the
well
known
list
comprehension
mechanism
in
functional
programming
languages
such
as
Haskell
[
ll
Miranda
[
24
KRC
[
23
etc
.

This
language
is
equivalent
to
our
earlier
query
languages
both
in
terms
of
semantics
and
in
terms
of
equational
theories
.

This
strong
sense
of
equivalence
allows
our
three
query
languages
to
be
freely
combined
into
a
nested
relational
query
language
that
is
robust
and
user
-
friendly
.

Comments
University
of
Pennsylvania
Department
of
Computer
and
Information
Science

Technical
Report

No
.
MSCIS-92
-
59
.

This
technical
report
is
available
at
ScholarlyCommons
:

http
repository.upenn.edu/cis_reports/472

A
Conservative
Property
Of

A
Nested
Relational
Query
Language
MS
-
CIS-92
-
59
LOGIC
COMPUTATION
48


-DOCSTART-

On
5
August
2003
,
our
beloved
colleague
Anthony
E.
Cawkell
passed
away
from
cancer
.

Tony
was
a
longtime
member
of
the
Editorial
Board
of
the
Journal
of
Information
Science
as
well
as
Honorary
Fellow
of
the
Institute
of
Information
Scientists
,
now
the
Chartered
Institute
for
Librarians
and
Information
Professionals

(
CILIP
Before
â€˜
retiringâ€™
to
a
private
consultancy
,
CITECH
Ltd
,
he
had
been
ISI
â€™s
man
in
London
.

For
thirty
years
he
not
only
directed
their
educational
and
marketing
efforts
in
Europe
but
also
served
as
a
Vice
President
of
Research
and
Development
which
was
his
forte
.

Space
and
time
do
not
permit
me
to
provide
an
adequate
treatment
of
Tony
â€™s
contributions
to
information
science
and
technology
.

In
a
brief
essay
in
Current


-DOCSTART-

In
a
joint
effort
The
Perseus
Project
,
a
digital
library
project
hosted
at
Tufts
University
and
Arachne
,
the
central
database
for
archaeological
objects
of
the
German
Archaeological
Institute
(
DAI
)
and
the
Research
Archive
for
Ancient
Sculpture
at
the
University
of
Cologne
(
FA
want
to
make
their
data
accessible
to
a
greater
audience
using
the
CIDOC
CRM
data
model
.

Given
the
fact
that
the
information
on
each
of
the
databases
is
of
interest
for
a
large
community
of
people
,
efforts
to
overcome
the
current
lack
of
data
integration
have
to
be
made
.

Another
area
of
interest
is
the
usability
of
the
CIDOC
CRM
for
a
multilingual
interface
.

Besides
the
philosophical
implications
and
the
mathematical
background
,
the
practicability
of
a
software
implementation
of
all
relevant
concepts
using
basic
Semantic
Web
technologies
as
described
by
the
W3C
will
be
the
main
concern
.

The
main
purpose
of
the
implementation
process
is
to
get
a
deeper
understanding
of
the
concepts
and
technologies
involved
when
dealing
with
the
Semantic
Web
,
Ontologies
in
general
and
the
CIDOC
CRM
in
particular
.

For
the
process
of
implementation
it
is
essential
that
methodical
proceedings
and
software
tools
have
to
team
up
,
appropriate
tools
have
to
be
discovered
or
developed
and
documented
.

Functional
requirements
have
a
tendency
to
evolve
relatively
rapid
while
information
systems
are
used
by
historians
.

Especially
information
systems
in
the
humanities
are
confronted
with
the
problem
of
constant
change
through
acquisition
of
new
project
partners
carrying
new
source
material
with
different
properties
.

As
a
consequence
potential
integration
efforts
have
to
cope
with
changes
of
the
database
schemas
and
therefore
should
be
flexible
.

Many
different
information
systems
with
different
methodical
approaches
can
be
found
in
the
field
of
historical
cultural
research
;
each
one
is
designed
according
to
a
specific
scientific
question
and
perspective
.

This
is
a
productive
situation
and
therefore
should
be
welcomed
,
but
the
experience
of
using
information
systems
for
historical
research
could
be
greatly
enhanced
by
creating
a
common
platform
for
information
retrieval
.

Scientific
databases
holding
historical
cultural
material
use
the
specialized
terminology
of
their
respective
areas
of
research
and
a
certain
national
language
.

The
general
problems
involved
with
machine
translation
are
well
known
.

Moreover
,
terminology
and
conventions
used
can
vary
within
one
sub
-
domain
and
therefore
should
be
restricted
.

Given
the
fact
that
the
information
in
each
of
the
databases
is
of
interest
to
a
large
community
of
people
,
efforts
to
overcome
the
current
problems
with
data
integration
have
to
be
made
10
]

To
build
a
software
system
able
to
do
that
,
each
database
and
its
interface
have
to
comply
with
several
requirements
and
have
to
supply
functionality
for
importing
and
exporting
data
.

Against
this
background
it
seems
reasonable
that
the
CIDOC
CRM
1
]
delivering
a
set
of
standardized
terms
and
properties
,
could
serve
as
a
foundation
for
heterogeneity
.

In
a
joint
effort
,
two
parties
from
classics
and
archaeology
intend
to
formulate
a
research
program
for
achieving
the
goals
mentioned
above
.

These
parties
will
be
The
Perseus
Project
[
8
]
and
Arachne
[
4
The
Perseus
Project
is
a
digital
library
hosted
at
Tufts
University
.

It
provides
humanities
resources
in
digital
form
with
a
focus
on
Classics
but
also
provides
early
modern
and
even
more
recent
material
.

Arachne
is
the
central
database
for
archaeological
objects
of
the
German
Archaeological
Institute
(
DAI
)
and
the
Research
Archive
for
Ancient
Sculpture
at
the
University
of
Cologne
(
FA
DAI
and
FA
joined
their
efforts
in
developing
Arachne
as
a
free
tool
for
archaeological
internet
research
.

Currently
only
a
few
implementations
exist
that
try
to
bridge
the
gap
between
more
than
one
language
and
several
data
models
at
the
same
time
.

To
overcome
the
lack
of
research
with
implementing
the
CIDOC
CRM
as
an
intellectual
concept
and
as
a
software
system
,
we
will
undertake
a
robust
implementation
.

Although
it
is
often
emphasized
that
the
CIDOC
CRM
is
an
intellectual
artefact
which
does
not
directly
deal
with
implementation
,
exactly
that
is
intended
:
To
apply
the
CIDOC
CRM
to
two
information
systems
holding
historical
cultural
material
(
in
this
case
Perseus
and
Arachne
)
and
to
elaborate
a
robust
implementation
of
a
mapping
agent
2
]
But
what
exactly
are
we
doing
while
implementing
intellectual
concepts
as
a
software
system
?

To
learn
how
to
implement
an
intellectual
concept
like
the
CIDOC
CRM
in
the
field
of
archaeology
,
we
need
to
understand
what
archaeologists
are
doing
when
conducting
research
.

In
contrast
to
enabling
both
databases
to
deal
with
the
CIDOC
CRM
data
,
it
seems
to
be
easier
to
build
a
mapping
agent
for
each
of
them
than
to
change
the
structure
of
all
participating
information
systems
.

This
mapping
agent
has
to
be
aware
of
both
database
schemas
to
be
able
to
translate
data
to
a
shared
common
format
.

Because
it
has
been
argued
that
the
belief
in
easily
building
a
mapping
agent
is
naÃ¯ve

[
9
]
we
will
focus
on
that
point
in
order
to
find
ways
to
overcome
the
current
problems
.

After
exporting
the
data
and
mapping
it
to
the
CIDOC
CRM
data
model
the
exported
and
integrated
data
will
be
stored
in
a
central
repository
providing
basic
query
functionality
for
the
time
being
.

At
a
later
stage
this
repository
should
offer
facilities
for
complex
database
queries
in
more
than
one
language
with
acceptable
performance
.

Since
a
large
corpus
of
possible
queries
will
be
supported
,
massive
problems
with
complexity
and
performance
are
expected
.

More
knowledge
about
how
historians
formulate
their
queries
could
help
to
reduce
this
complexity
.

In
order
to
put
these
ideas
into
action
,
standards
for
representing
and
querying
archaeological
domain
knowledge
have
to
be
found
.

For
experimentation
both
project
partners
will
provide
an
XML
dump
as
raw
material
of
their
data
model
and
content
which
at
a
later
stage
can
be
done
by
periodic
data
harvesting
3
]

It
is
the
first
goal
to
map
the
structure
and
the
data
to
a
markup
format
that
both
parties
will
agree
to
and
which
will
be
stored
in
a
central
ontology
driven
repository
server
for
further
development
(
e.g.
RDF
5

The
data
storage
within
the
central
repository
has
the
advantage
that
the
databases
do
not
have
to
change
their
implementation
.

The
repository
will
provide
basic
query
functionality
through
a
semantic
layer
using
an
abstract
language
independent
data
model
like
the
CIDOC
CRM
(
e.g.
SPARQL

Moreover
,
both
parties
have
to
agree
on
the
level
of
semantic
and
structural
granularity
of
the
provided
data
.

To
answer
the
questions
essential
for
the
process
of
implementation
,
methodical
proceedings
and
software
tools
have
to
team
up
.

A
survey
of
existing
software
that
is
able
to
deal
with
the
standards
mentioned
above
has
already
been
carried
out
,
with
the
following
results
.

ProtÃ©gÃ©
,
a
free
ontology
editor
and
knowledge
base
framework
,
helped
with
exploring
and
analysing
the
data
models
against
the
CIDOC
CRM
ontology
and
experimenting
with
retrieval
techniques
PROTÃ‰GÃ‰
]

Jena
is
a
framework
for
building
Semantic
Web
applications
.

The
Jena
API
developed
by
HP
Labs
in
connection
with
Schemagen
is
one
component
for
a
very
simple
mapping
that
dumps
the
exported
data
to
RDF
/
XML
using
the
RDF
API
.

The
Jena
framework
furthermore
supports
reading
and
writing
RDF
in
several
formats
:
an
OWL
API
,
in
-
memory
and
persistent
storage
,
and
a
SPARQL
query
engine
JENA
]

Both
projects
are
free
of
charge
and
seem
to
have
an
active
development
team
and
a
strong
user
community
,
promising
further
development
and
enhancements
.

After
having
developed
a
prototype
of
a
mapping
component
using
the
tools
and
standards
mentioned
above
with
an
extremely
simple
data
model
there
is
much
to
be
said
for
a
CRMish
implementation
relying
on
standard
technologies
as
proposed
by
the
W3C
rather
than
for
a
full
CRMized
implementation
.

Although
the
high
complexity
offered
by
new
data
models
enables
us
to
realize
new
ideas
,
neither
the
intellectual
considerations
nor
the
available
software
tools
have
long
been
discussed
in
the
humanities
,
which
leads
to
a
lack
of
experience
in
applying
the
relevant
methods
and
discovering
adequate
standards
.

Due
to
this
it
is
difficult
to
estimate
efforts
and
costs
to
formulate
realistic
project
goals
for
organizations
wanting
to
implement
the
CIDOC
CRM
.

Ways
to
reduce
,
hide
or
manage
complexity
have
to
be
found
.

The
internal
data
model
is
presented
to
the
user
by
a
graphical
user
interface
.

Here
new
GUI
concepts
have
to
be
developed
to
present
internal
complexity
to
users
in
a
manner
that
they
can
handle
.

In
order
to
archive
this
,
visualization
is
indispensable
.

Using
a
data
model
which
offers
a
potentially
huge
complexity
therefore
poses
a
great
demand
on
the
usability
of
an
information
system
to
be
put
across
to
the
user
in
a
way
he
can
understand
and
handle
.

As
an
example
the
ProtÃ©gÃ©
Project
made
some
efforts
to
automatically
build
user
interfaces
from
the
underlying
data
model
.

Ontobroker
,
a
deductive
,
object
oriented
database
system
,
combines
classic
search
interfaces
with
visualization
techniques
ONTOBROKER
]

What
is
meant
by
saying
that
the
mapping
agent
has
to
be
flexible
?

In
software
technology
one
often
means
that
a
piece
of
software
has
to
be
modular
,
adaptable
and
maintainable
.

It
is
interesting
that
all
three
points
are
dealing
with
complexity
.

These
points
become
problematic
when
dealing
with
information
systems
working
with
historical
cultural
data
.

In
this
context
an
information
system
lays
the
foundation
for
being
able
to
handle
complex
and
often
semistructured
data
from
the
field
of
history
.

In
addition
,
functional
requirements
have
a
tendency
to
evolve
relatively
rapid
while
information
systems
are
used
by
historians
.

As
the
understanding
of
the
subject
increases
,
new
questions
and
requirements
come
up
.

A
flexible
information
system
therefore
must
be
able
to
advance
at
the
same
pace
as
an
information
system
evolves
an
aspect
to
be
considered
in
the
design
phase
already
0
]

Due
to
a
strong
commercial
influence
,
relational
databases
are
widely
used
for
historical
cultural
knowledge
.

Relational
databases
do
not
support
rich
semantic
modelling
of
data
which
urges
the
software
developer
to
model
semantics
on
higher
levels
of
the
information
system
.

Therefore
it
has
to
be
taken
into
consideration
that
each
information
system
changes
on
several
layers
.

A
change
in
one
of
these
layers
potentially
causes
the
need
to
adapt
the
mapping
mechanism
and
regarding
only
the
data
model
is
not
enough
.

Beneath
the
internal
data
model
(
storage
and
internal
representation
of
factual
knowledge
)
the
application
logic
(
first
layer
of
interpretation
)
retrieves
and
recombines
the
data
for
the
graphical
user
interface
(
second
layer
of
interpretation
,
interpretation
of
data
model
)
including
layouts
that
display
the
information
to
the
user
in
multiple
views
and
the
user
â€™s
implicit
knowledge
(
third
layer
of
interpretation
)
about
the
information
system
,
including
implicit
conventions
,
have
to
be
taken
into
account
.

The
implementation
of
the
CIDOC
CRM
impacts
all
the
mentioned
layers
and
a
mapping
agent
has
to
be
aware
of
all
these
layers
because
it
needs
to
preserve
all
implicit
layers
of
meaning
.

This
is
reminiscent
of
the
principles
of
composition
that
have
been
formulated
by
Frege
.

That
principle
has
to
be
formalized
to
make
a
mapping
agent
reusable
The
meaning
of
a
complex
expression
is
determined
by
its
structure
and
the
meanings
of
its
constituents
12
]

The
resulting
question
is
:
How
can
a
mapping
component
be
built
in
such
a
way
that
it
can
adapt
to
changes
easily
?

As
mentioned
above
,
the
semantics
of
a
database
application
is
spread
over
several
layers
.

It
is
at
least
questionable
that
complex
and
highly
structured
data
contained
in
one
information
system
(
context
I
)
can
be
transferred
to
another
information
system
(
context
II
)
without
loss
of
(
structure
information
without
preserving
the
process
of
composition
.

Most
current
databases
in
the
humanities
do
nâ€™t
meet
halfway
because
of
their
proprietary
semantic
modelling
without
relying
on
standards
.

In
the
future
it
could
be
reasonable
to
build
awareness
of
a
shared
data
model
like
the
CIDOC
CRM
into
the
original
database
application
.

Since
it
appears
too
complex
to
map
the
whole
data
structure
to
a
shared
data
model
it
is
important
to
determine
those
parts
of
the
data
model
that
are
most
important
and
valuable
for
integration
.

Furthermore
it
is
not
practicable
to
map
each
detail
and
therefore
a
reasonable
level
of
detail
has
to
be
determined
.

The
resulting
data
will
be
held
in
a
central
repository
committed
to
the
CIDOC
CRM
data
model
.

This
repository
will
contain
many
lean
but
highly
structured
records
.

Each
record
links
to
the
original
data
source
for
further
information
.

The
analytical
strength
of
the
CIDOC
CRM
data
model
is
most
effective
when
the
form
of
query
operations
is
not
limited
.

In
general
a
query
is
fast
if
data
is
requested
in
a
way
that
is
supported
by
the
data
structure
.

Therefore
questions
should
be
restricted
or
precompiled
for
those
which
are
used
often
by
a
crawler
application
.

How
can
the
conflict
between
open
scientific
questions
resulting
in
complex
and
non
predictable
query
operations
and
performance
be
solved
?


-DOCSTART-

In
a
User
-
Private
Information
Retrieval
(
UPIR
)
scheme
,
a
set
of
users
collaborate
to
retrieve
files
from
a
database
without
revealing
to
observers
which
participant
in
the
scheme
requested
the
file
.

Protocols
have
been
proposed
based
on
pairwise
balanced
designs
and
symmetric
designs
.

We
propose
a
new
class
of
UPIR
schemes
based
on
generalised
quadrangles
(
GQ

We
prove
that
while
the
privacy
of
users
in
the
previously
proposed
schemes
could
be
compromised
by
a
single
user
,
the
new
GQ
-
UPIR
schemes
proposed
in
this
paper
maintain
privacy
with
high
probability
even
when
up
to
O(n1/4
users
collude
,
where
n
is
the
total
number
of
users
in
the
scheme
.


-DOCSTART-

This
paper
reports
on
the
Evaluation
Methodologies
in
Information
Retrieval
Seminar

[
1
]
held
from
27
October
to
1
November
2013
at
the
Schloss
Dagstuhl
Leibniz
Center
for
Informatics
that
is
a
world
-
wide
renowned
venue
for
informatics
where
scientists
come
together
to
exchange
their
knowledge
and
to
discuss
their
research
findings
.

Dagstuhl
offers
modern
facilities
and
is
located
in
the
beautiful
countryside
of
Saarland
,
Germany
.

Schloss
Dagstuhl
has
previously
hosted
other
seminars
on
information
retrieval
topics
,
e.g.
the
Seminar
09101
on
Interactive
Information
Retrieval
which
took
place
in
2009
,
but
this
one
was
the
first
one
devoted
to
the
specific
topic
of
information
retrieval
evaluation
.

The
seminar
was
attended
by
42
participants
from
thirteen
different
countries
,
including
a
large
number
of
established
researchers
as
well
as
some
some
promising
young
researchers
,
and
also
practitioners
from
industry
.

As
it
is
well
-
known
evaluation
of
information
retrieval
(
IR
)
systems
has
a
long
tradition
(
see
,
for
example

2
however
,
the
test
-
collection
based
evaluation
paradigm
is
of
limited
value
for
assessing
today
â€™s
IR
applications
,
since
it
fails
to
address
major
aspects
of
the
IR
process
.

Thus
there
is
a
need
for
new
evaluation
methodologies
,
which
are
able
to
deal
with
the
following
issues
:


-DOCSTART-

Automatic
classification
of
text
documents
has
become
an
important
research
issue
now
days
.

Proper
classification
of
text
documents
requires
information
retrieval
,
machine
learning
and
Natural
language
processing
(
NLP
)
techniques
.

Our
aim
is
to
focus
on
important
approaches
to
automatic
text
classification
based
on
machine
learning
techniques
viz
.

supervised
,
unsupervised
and
semi
supervised
.

In
this
paper
we
present
a
review
of
various
text
classification
approaches
under
machine
learning
paradigm
.

We
expect
our
research
efforts
provide
useful
insights
on
the
relationships
among
various
text
classification
techniques
as
well
as
sheds
light
on
the
future
research
trend
in
this
domain
.


-DOCSTART-

We
present
a
framework
for
information
retrieval
that
combines
document
models
and
query
models
using
a
probabilistic
ranking
function
based
on
Bayesian
decision
theory
.

The
framework
suggests
an
operational
retrieval
model
that
extends
recent
developments
in
the
language
modeling
approach
to
information
retrieval
.

A
language
model
for
each
document
is
estimated
,
as
well
as
a
language
model
for
each
query
,
and
the
retrieval
problem
is
cast
in
terms
of
risk
minimization
.

The
query
language
model
can
be
exploited
to
model
user
preferences
,
the
context
of
a
query
,
synonomy
and
word
senses
.

While
recent
work
has
incorporated
word
translation
models
for
this
purpose
,
we
introduce
a
new
method
using
Markov
chains
defined
on
a
set
of
documents
to
estimate
the
query
models
.

The
Markov
chain
method
has
connections
to
algorithms
from
link
analysis
and
social
networks
.

The
new
approach
is
evaluated
on
TREC
collections
and
compared
to
the
basic
language
modeling
approach
and
vector
space
models
together
with
query
expansion
using
Rocchio
.

Significant
improvements
are
obtained
over
standard
query
expansion
methods
for
strong
baseline
TF
-
IDF
systems
,
with
the
greatest
improvements
attained
for
short
queries
on
Web
data
.


-DOCSTART-

In
this
paper
,
we
extend
the
work
of
Kraft
et
al
.

to
present
a
new
method
for
fuzzy
information
retrieval
based
on
fuzzy
hierarchical
clustering
and
fuzzy
inference
techniques
.

First
,
we
present
a
fuzzy
agglomerative
hierarchical
clustering
algorithm
for
clustering
documents
and
to
get
the
document
cluster
centers
of
document
clusters
.

Then
,
we
present
a
method
to
construct
fuzzy
logic
rules
based
on
the
document
clusters
and
their
document
cluster
centers
.

Finally
,
we
apply
the
constructed
fuzzy
logic
rules
to
modify
the
user
's
query
for
query
expansion
and
to
guide
the
information
retrieval
system
to
retrieve
documents
relevant
to
the
user
's
request
.

The
fuzzy
logic
rules
can
represent
three
kinds
of
fuzzy
relationships
(
i.e
fuzzy
positive
association
relationship
,
fuzzy
specialization
relationship
and
fuzzy
generalization
relationship
)
between
index
terms
.

The
proposed
fuzzy
information
retrieval
method
is
more
flexible
and
more
intelligent
than
the
existing
methods
due
to
the
fact
that
it
can
expand
users
'
queries
for
fuzzy
information
retrieval
in
a
more
effective
manner
.


-DOCSTART-

Stemming
and
Lemmatization
are
two
significant
natural
language
processing
techniques
extensively
used
in
Information
Retrieval
for
query
processing
and
Machine
Translation
for
reducing
the
data
sparseness
.

Most
of
the
existing
Stemmers
and
Lemmatizers
are
based
on
some
language
dependent
rules
which
require
the
supervision
of
a
language
expert
.

Some
probabilistic
approach
needs
vast
amounts
of
monolingual
corpus
.

Both
Stemming
and
Lemmatization
minimize
inflectional
structure
,
and
sometimes
derivationally
related
structure
of
a
word
to
a
common
base
form
.

This
paper
proposes
an
unsupervised
stemming
which
is
hybridized
with
partial
lemmatization
for
four
morphologically
different
languages
such
as
English
,
French
,
Tamil
and
Hindi
.

An
innovative
attempt
is
being
made
to
develop
a
stemming
algorithm
for
a
novel
conflation
method
that
exploits
the
quality
of
words
and
uses
some
standard
Natural
Language
Processing
tools
like
Levenshtein
Distance
and
Longest
Common
Subsequence
for
Stemming
process
.

This
approach
can
even
support
other
Indian
and
Non
-
Indian
languages
.


-DOCSTART-

With
the
explosive
growth
of
the
amount
of
information
stored
on
computer
networks
such
as
the
Internet
,
it
is
increasingly
more
difficult
for
information
seekers
to
retrieve
relevant
information
.

Traditional
document
ranking
functions
employed
by
Internet
search
engines
can
be
enhanced
to
improve
the
effectiveness
of
information
retrieval
(
IR
This
paper
illustrates
the
design
and
development
of
a
granular
IR
system
to
facilitate
domain
specific
search
.

In
particular
,
a
novel
computational
model
is
designed
to
rank
documents
according
the
searchersâ€™
specific
granularity
requirements
.

The
initial
experiments
confirm
that
our
granular
IR
system
outperforms
a
classical
vector
-
based
IR
system
.

In
addition
,
user
-
based
evaluations
also
demonstrate
that
our
granular
IR
system
is
effective
when
compared
with
a
well
-
known
Internet
search
engine
.

Our
research
work
opens
the
door
to
the
design
and
development
of
the
next
generation
of
Internet
search
engines
to
alleviate
the
problem
of
information
overload
.


-DOCSTART-

The
Web
Track
of
2014
Text
REtrieval
Conference
(
TREC
)
addresses
the
most
fundamental
problem
of
Information
Retrieval
.

We
did
not
intend
to
craft
a
system
that
beats
the
state
-
of
-
the
-
art
search
engines
,
but
to
design
a
light
weight
and
cost
-
effective
system
with
comparable
performances
.

We
introduce
a
twopass
retrieval
framework
,
with
the
first
pass
consisting
of
a
simple
and
efficient
retrieval
model
that
focuses
on
recall
,
and
the
second
pass
a
wave
of
feature
extraction
algorithms
run
on
the
set
of
top
ranked
documents
,
followed
by
Learning
to
Rank
(
LETOR
)
algorithms
that
provide
different
precision
oriented
rankings
,
and
their
outputs
are
combined
using
data
fusion
.

We
have
focused
on
using
statistical
Language
Models
with
novel
and
well
-
known
smoothing
techniques
,
different
LETOR
methods
,
and
various
data
fusion
techniques
.

In
addition
,
we
have
also
tried
using
topic
modelling
with
Hierarchical
Dirichlet
Allocation
for
query
expansion
in
the
hope
of
improving
diversity
of
our
results
.

However
,
the
topic
modelling
approach
has
turned
out
to
be
unsuccessful
,
and
we
have
not
been
able
to
spot
the
problem
and
benefit
from
it
in
this
work
.

In
addition
,
we
also
present
some
further
analyses
demonstrating
that
our
approach
is
robust
against
overfitting
,
and
some
general
studies
on
overfitting
in
the
context
of
LETOR
.


-DOCSTART-

Document
ranking
is
well
known
to
be
a
crucial
process
in
information
retrieval
(
IR
It
presents
retrieved
documents
in
an
order
of
their
estimated
degrees
of
relevance
to
query
.

Traditional
document
ranking
methods
are
based
on
different
measurements
of
similarity
between
documents
and
query
.

Due
to
information
explosion
and
the
popularity
of
WWW
information
retrieval
,
the
increased
variety
of
information
and
users
makes
it
insufficient
to
consider
similarity
alone
in
the
ranking
process
.

In
some
cases
,
there
is
a
need
for
user
to
retrieve
documents
which
are
generally
or
broadly
describing
a
certain
topic
.

This
is
particularly
the
case
in
some
specific
domains
such
as
bio
-
medical
IR
.

To
satisfy
the
stringent
requirement
of
generality
based
retrieval
,
we
propose
a
novel
approach
to
re
-
rank
the
retrieved
documents
by
considering
their
generality
as
a
compliment
.

By
analyzing
the
semantic
cohesion
of
text
,
document
generality
can
be
quantified
.

The
retrieved
documents
are
then
re
-
ranked
by
their
combined
scores
of
similarity
and
the
closeness
of
documentsâ€™
generality
to
the
query

Results
show
an
encouraging
performance
on
a
large
scale
bio
-
medical
text
corpus
,
OHSUMED
,
which
is
a
subset
of
MEDLINE
collection
containing
348,566
medical
journal
references
and
101
test
queries
.


-DOCSTART-

Xerox
participated
in
the
Cross
Language
Information
Retrieval
(
CLIR
)
track
of
TREC-6
.

This
track
examines
the
problem
of
retrieving
documents
written
in
one
language
using
queries
written
in
another
language
.

Our
approach
is
to
use
a
bilingual
dictionary
at
query
time
to
construct
a
target
language
version
of
the
original
query
.

We
concentrate
our
experiments
this
year
on
manual
query
construction
based
on
a
weighted
boolean
model
and
on
an
automatic
method
for
the
translation
of
multi
-
word
units
.

We
also
introduce
a
new
derivational
stemming
algorithm
whose
word
classes
are
generated
automatically
from
a
monolingual
lexicon
.

We
present
our
results
on
the
22
TREC-6
CLIR
topics
which
have
been
assessed
and
brieey
discuss
the
problems
inherent
in
the
cross
-
language
IR
task
.


-DOCSTART-

Dr.
Abdur
Chowdhury
serves
as
Twitter
â€™s
Chief
Scientist
.

Prior
to
that
,
Dr.
Chowdhury
co
-
founded
Summize
,
a
real
-
time
search
engine
sold
to
Twitter
in
2008
.

Dr.
Chowdhury
has
held
positions
at
AOL
as
their
Chief
Architect
for
Search
,
Georgetown
â€™s
Computer
Science
Department
and
University
of
Maryland
â€™s
Institute
for
Systems
Research
.

His
research
interest
lays
in
Information
Retrieval
focusing
on
making
information
accessible
.


-DOCSTART-

The
primary
objective
of
document
annotation
in
whatever
form
,
manual
or
electronic
is
to
allow
those
who
may
not
have
control
to
original
document
to
provide
personal
view
on
information
source
.

Beyond
providing
personal
assessment
to
original
information
sources
,
we
are
looking
at
a
situation
where
annotation
made
can
be
used
as
additional
source
of
information
for
document
tracking
and
recommendation
service
.

Most
of
the
annotation
tools
existing
today
were
conceived
for
their
independent
use
with
no
reference
to
the
creator
of
the
annotation
.

We
propose
AMIEDoT
(
Annotation
Model
for
Information
Exchange
and
Document
Tracking
)
an
annotation
model
that
can
assist
in
document
tracking
and
recommendation
service
.

The
model
is
based
on
three
parameters
in
the
acts
of
annotation
.

We
believe
that
introducing
document
parameters
,
time
and
the
parameters
of
the
creator
of
annotation
into
an
annotation
process
can
be
a
dependable
source
to
know
,
who
used
a
document
,
when
a
document
was
used
and
for
what
a
document
was
used
for
.

Beyond
document
tracking
,
our
model
can
be
used
in
not
only
for
selective
dissemination
of
information
but
for
recommendation
services
.

AMIEDoT
can
also
be
used
for
information
sharing
and
information
reuse
.

Categories
and
subject
descriptors
Information
systems
design
,
Document
tracking
,
routing
,
recommenders
,
Information
retrieval
,
filtering
,
and
extraction


-DOCSTART-

With
the
recent
enormous
increase
of
information
dissemination
via
the
web
as
incentive
there
is
a
growing
interest
in
supporting
tools
for
cross
-
language
retrieval
.

In
this
paper
we
describe
a
disclosure
and
retrieval
approach
that
fulllls
the
needs
of
both
information
providers
and
users
by
ooering
fast
and
cheap
access
to
a
large
amounts
of
documents
from
various
language
domains
.

Relevant
information
can
be
retrieved
irrespective
of
the
language
used
for
the
speciication
of
a
query
.

In
order
to
realize
this
type
of
multilingual
functionality
the
availability
of
several
translation
tools
is
needed
,
both
of
a
generic
and
a
domain
speciic
nature
.

Domain
speciic
tools
are
often
not
available
or
only
against
large
costs
.

In
this
paper
we
will
therefore
focus
on
a
way
to
reduce
these
costs
,
namely
the
automatic
derivation
of
multilingual
resources
from
so
-
called
parallel
text
corpora
.

The
beneets
of
this
approach
will
be
illustrated
for
an
example
system
,
i.e.
the
demonstrator
developed
within
the
project
Twenty
-
One
,
which
is
tuned
to
information
from
the
area
of
sustainable
development
.


-DOCSTART-

This
paper
describes
a
designed
and
implemented
system
for
efficient
storage
,
indexing
and
search
in
collections
of
spoken
documents
that
takes
advantage
of
automatic
speech
recognition
.

As
the
quality
of
current
speech
recognizers
is
not
sufficient
for
a
great
deal
of
applications
,
it
is
necessary
to
index
the
ambiguous
output
of
the
recognition
,

i.
e.
the
acyclic
graphs
of
word
hypotheses
recognition
lattices
.

Then
,
it
is
not
possible
to
directly
apply
the
standard
methods
known
from
text
-
based
systems
.

The
paper
discusses
an
optimized
indexing
system
for
efficient
search
in
the
complex
and
large
data
structure
that
has
been
developed
by
our
group
.

The
search
engine
works
as
a
server
.

The
meeting
browser
JFerret
,
developed
withing
the
European
AMI
project
,
is
used
as
a
client
to
browse
search
results
.


-DOCSTART-

In
an
Information
Retrieval
system
,
text
representation
is
typically
achieved
by
associating
a
group
of
keywords
to
the
text
.

The
lack
of
structure
in
keyword
representation
limits
the
versatility
and
effectiveness
of
Information
Retrieval
systems
.

Characterising
the
text
as
well
as
the
request
for
information
with
appropriate
features
such
as
events
,
objectives
,
conclusions
,
or
results
facilitates
information
retrieval
.

This
work
has
as
a
main
objective
to
design
a
process
that
allows
to
extract
automatically
some
of
these
characteristics
from
documents
and
generates
a
structured
representation
of
them
,
providing
in
this
way
specific
details
of
the
text
content
.

The
structured
representation
will
allow
that
this
kind
of
characteristics
can
be
requested
and
used
by
a
retrieval
mechanism
in
a
WWW
information
server
,
attempting
to
overcome
some
of
the
limitations
of
Information
Retrieval
systems
.

Introduction

When
a
user
requires
information
from
an
information
server
,
the
first
search
produces
information
generally
extensive
[
Berghel
,
97

This
is
a
problem
for
the
users
because
they
have
to
analyse
all
the
documents
dedicating
time
and
effort
;
in
the
best
situation
,
the
user
finds
some
documents
in
the
initial
efforts
,
and
in
the
worst
case
,
he
/
she
abandons
the
evaluation
because
of
reading
saturation
.

For
this
reason
,
we
are
working
on
a
search
system
attempting
to
reduce
the
kind
of
problems
that
the
user
faces
when
searching
in
a
server
.

The
general
objective
is
to
create
a
search
mechanism
that
overcomes
the
limitations
of
traditional
mechanisms
using
terms
or
keywords
[
Salton
McGill
,
83
This
new
mechanism
is
thought
to
operate
as
part
of
an
information
server
accessible
by
WWW
browsers
.

We
assume
that
the
server
will
have
available
thousands
of
documents
.

One
of
the
goals
is
to
provide
two
modes
of
search
,
a
traditional
mode
based
on
terms
[
SÃ¡nchez
-
LÃ³pez
LÃ³pez
-
LÃ³pez
,
97
]
and
another
,
more
advanced
,
that
performs
detailed
search
.

This
is
illustrated
in
Figure
1
.

We
expect
that
the
latter
will
overcome
some
limitations
of
the
former
.

General
Diagram
of
Information
Server

The
present
work
reports
the
method
of
information
extraction
that
precedes
the
detailed
search
.

This
method
presupposes
a
previous
information
retrieval
using
the
traditional
mechanism
with
the
aims
of
taking
advantage
of
their
efficiency
and
,
in
addition
,
reducing
the
set
of
documents
to
analyse
.

The
method
is
based
on
previous
investigations
on
experimental
collections
[
LÃ³pez
-
LÃ³pez
Myaeng
,
96
A
parallel
project
[
LÃ³pez
-
LÃ³pez
TenorioPÃ©rez
,
97
]
works
on
constructing
the
mechanism
that
compares
the
representations
produced
by
the
method
of
extraction
for
the
documents
and
the
user
request
.

Information
Extraction
Process

A
method
of
Information
Retrieval
has
been
proposed
[
LÃ³pez
-
LÃ³pez
Myaeng
,
96
]
that
consists
of
a
two
-
level
representation
of
documents
,
where
the
first
level
(
topic
)
serves
as
the
primary
means
to
access
document
Publicado
en
VIII
Congreso
Internacional
de
ElectrÃ³nica
,
Comunicaciones
y
Computadoras
,
CONILECOMP
98
,
Memorias
,
pp
.

287
-
291
,
Febrero
de
1998
,
UDLA
-
P
,
Cholula
,
Puebla
,
MÃ©xico
.

descriptions
and
the
second
level
(
extra
-
topical
details
)
helps
to
refine
the
search
by
giving
more
details
.

This
second
level
consists
of
a
structured
representation
[
Chisnell
,
Rama
Srinivasan
,
93
]
of
the
content
of
the
text
and
the
user
request
that
allows
to
carry
out
a
comparison
between
them
and
then
obtain
documents
of
higher
relevance
to
user
interest
than
using
terms
alone
.

The
design
of
the
whole
extraction
process
is
depicted
in
Figure
2
.

This
consists
of
four
components
:
section
separator
,
identifier
of
syntactic
elements
(
tagger
information
selector
,
and
structured
representation
generation
.

Design
of
Extraction
Process

The
processing
for
this
second
level
starts
from
a
set
of
documents
that
are
the
result
of
a
first
search
based
on
keywords
.

An
analysis
is
carried
out
on
these
documents
identifying
each
one
of
the
sections
they
are
composed
,
and
extracting
the
section
of
interest
(
see
Fig
.

3
Figure
3
.

Section
Separator

Then
,
it
proceeds
to
identify
the
syntactic
role
of
each
one
of
the
words
or
element
of
each
sentence
of
the
section
,
assigning
their
respective
tag
.

These
roles
can
be
verb
,
adjective
,
noun
,
determiner
,
etc
.

This
step
is
illustrated
in
Figure
4
.

Identifier
of
Syntactic
Elements
(
Tagger
)

Given
the
tagged
section
,
the
sentences
likely
to
provide
information
about
certain
document
content
are
extracted
.

The
criterion
used
to
select
the
sentences
is
based
on
metadiscourse

[
Crismore
,
84
i.e.
those
sentences
explicitly
describing
aspects
of
document
content
[
LÃ³pezLÃ³pez
,
95][Tapia
-
Melchor
,
97
Only
main
verbs
of
the
sentences
belonging
to
a
predefined
list
are
considered
.

Figure
5
shows
schematically
this
step
.

Information
Selector
Figure
6
.

Structured
Representation
Generation
Next
,
by
means
of
a
syntactic
parser
[
Strzalkowski
,
92
a
representation
of
the
logical
structure
of
each
sentence
is
obtained
;
this
is
a
sequence
of
symbols
,
where
each
symbol
is
either
a
particular
word
or
an
identifier
of
a
major
class
such
as
noun
,
adjective
,
verb
,
noun
phrase
,
etc
.

Finally
,
a
structured
representation
of
the
sentences
is
generated
expressing
the
concepts
and
how
they
are
related
(
see
Fig
.

This
representation
is
given
as
Conceptual
Graphs
[
Sowa
,
84
i.e.
an
interconnection
of
concept
nodes
and
Publicado
en
VIII
Congreso
Internacional
de
ElectrÃ³nica
,
Comunicaciones
y
Computadoras
,
CONILECOMP
98
,
Memorias
,
pp
.

287
-
291
,
Febrero
de
1998
,
UDLA
-
P
,
Cholula
,
Puebla
,
MÃ©xico
.

relation
nodes
,
where
the
concept
nodes
represents
entities
,
attributes
or
events
,
and
the
relation
nodes
identify
the
kind
of
relationship
occurring
between
concept
nodes
.

The
design
of
the
previous
process
has
been
made
following
an
object
-
oriented
philosophy
,
being
programmed
mainly
in
C
language
(
Section
Separator
and
Information
Selector
with
the
tagger
in
C
language
and
the
rest
in
Prolog
(
Analyser
Syntactic
Parser
and
generation
of
Conceptual
Graphs

One
Example

Next
,
we
show
the
outputs
that
are
already
obtained
corresponding
to
each
one
of
the
steps
described
above
:
Content
of
the
section
of
interest
:
Abstract.txt
On
the
Proof
of
Correctness
of
a
Calendar
Program
.

A
semi
-
automatic
procedure
for
resolving
semantic
problems
is
suggested
.

An
illustrative
example
is
given
.

_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
txt
On
the
Proof
of
Correctness
of
a
Calendar
Program
.

A
semi
-
automatic
procedure
for
resolving
semantic
problems
is
suggested
.

An
illustrative
example
is
given
.

_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
Output
from
the
tagger
(
input
to
the
information
selector
On|IN
the|DT
Proof|NP
of|IN
Correctness|NP
of|IN
a|DT
Calendar|NP
Program|NP
A|DT
semi
-
automatic|JJ
procedure|NN
for|IN
resolving|VBG

semantic|JJ
problems|NNS
is|VBZ

suggested|VBN
An|DT
illustrative|JJ
example|NN
is|VBZ

given|VBN
_

_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
Output
from
the
information
selector
(
input
to
the
syntactic
parser
A|DT
semi
-
automatic|JJ
procedure|NN
for|IN
resolving|VBG

semantic|JJ
problems|NNS
is|VBZ

suggested|VBN
An|DT
illustrative|JJ
example|NN
is|VBZ

given|VBN
These
selected
sentences
are
formatted
as
follows
a
,
dt
semi
-
automaticâ€™,jj
procedure
,
nn
for
,
in
resolving
,
vbg
semantic
,
jj],problems
,
nns
is
,
vbz
suggested
,
vbn
an
,
dt
illustrative
,
jj
example
,
nn
is
,
vbz
given
,
vbn

_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_

The
final
output
looks
like
:
Output
from
the
Syntactic
Parser
assert
be,[tense
,
prs3
verb
suggest,[tense
,
pstp
subject
,
anyone
object,[np,[n,[procedure
,
sg
t_pos
,
a
adj
semi
-
automatic
for
verb
resolve,[tense
,
prsp
subject
,
anyone
object,[np,[n,[problem
,
pl
adj,[semantic
assert
be,[tense
,
prs3
verb
give,[tense
,
pstp
subject
,
anyone
object,[np,[n,[example
,
sg
t_pos
,
an
adj,[illustrative
This
representation
is
transformed
into
Conceptual
Graphs
[
Sowa
,
84
CG
suggest
obj
procedure
procedure
attr
semi
-
automatic
procedure
for
resolve
,
t
,
prsp
resolve
,
t
,
prsp
obj
problem
problem
attr
semantic
]
CG
give
obj
example
example
attr
illustrative
]

_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_
_

The
different
components
of
the
extraction
process
have
been
integrated
and
the
extraction
subsystem
is
already
part
of
the
information
server
,
making
it
available
to
users
from
a
WWW
page
in
their
browser
.

Figure
7
shows
the
main
page
of
the
search
system
,
with
the
option
to
choose
one
mode
of
search
.

Main
Page
of
Search
System

As
mentioned
above
,
fast
search
is
accomplished
using
keywords
alone
.

When
choosing
fast
search
,
the
system
Publicado
en
VIII
Congreso
Internacional
de
ElectrÃ³nica
,
Comunicaciones
y
Computadoras
,
CONILECOMP
98
,
Memorias
,
pp
.

287
-
291
,
Febrero
de
1998
,
UDLA
-
P
,
Cholula
,
Puebla
,
MÃ©xico
.

displays
the
form
showed
in
Figure
8
,
where
the
user
can
supply
the
query
terms
and
select
different
search
options
.

Fast
Search
Form
Page

The
keyword
-
based
search
is
already
working
and
providing
documents
for
information
extraction
.

One
example
of
results
produced
by
a
fast
search
is
included
in
Figure
9
.

Example
of
Results
Produced
by
Fast
Search

If
detailed
search
is
selected
,
the
information
extraction
process
uses
the
abstracts
produced
by
fast
search
to
generate
their
structured
representation
.

Then
,
the
search
system
can
perform
a
comprehensive
comparison
against
the
query
that
includes
terms
and
additional
aspects
.

Figure
10
displays
the
form
requesting
additional
details
for
the
query
,
besides
the
keywords
supplied
in
previous
form
.

One
can
observed
that
the
additional
details
are
specified
as
sentences
in
natural
language
,
stating
what
the
user
would
like
to
find
in
the
document
.

These
sentences
are
processed
by
a
similar
method
as
extraction
to
produce
a
second
level
representation
of
the
query
that
would
be
matched
against
those
of
the
documents
.

Notice
also
that
the
user
has
the
option
of
waiting
in
line
for
the
results
or
asking
for
the
results
to
be
sent
by
e
-
mail
.

Form
Page
for
Detailed
Search

We
are
in
the
process
of
:
Tuning
the
comparison
mechanism
for
detailed
search
,
and
Testing
the
system
with
a
large
number
of
documents
.

Conclusions
<
lb

The
structured
representation
of
the
sentences
extracted
<
lb
>
from
the
documents
and
of
the
user
request
provides
<
lb
>
additional
specific
details
of
their
content.<lb

The
two
-
level
representation
defines
two
dimensions
on
<
lb
>
which
a
document
can
be
judged
,
serving
a
wider
range
of
<
lb
>
interest.<lb
>

Nowadays
,
Information
Retrieval
Systems
available
on
<
lb
>
the
Web
is
not
very
effective
,
since
upon
asking
for
<
lb
>
information
,
they
provide
too
many
documents
;
documents
<
lb
>

that
perhaps
are
unnecessary
because
they
do
not
have
<

>
information
the
user
requested.<lb

The
Search
System
that
we
are
implementing
is
a
<
lb
>
system
that
will
give
the
opportunity
to
the
user
,
not
only
of
<
lb
>
asking
for
information
by
means
of
keywords
,
but
he
/
she
<
lb
>
can
express
essential
specific
characteristic
about
the
<
lb
>

information
needed.<lb

This
system
is
a
tool
that
promises
a
search
of
<
lb
>
documents
with
reduced
irrelevant
information
.

The
more
<
lb
>
the
system
knows
about
the
user
information
needs
,
it
is
<
lb
>
less
likely
to
judge
any
document
as
relevant
.

References
<
lb>[Berghel
,
97
]

Berghel
Hal
,
Cyberspace
2000
:

Dealing
with
<
lb
>
Information
Overload
,
Communications
of
the
ACM,<lb

>
Vol.40
,
No.2
,
February
1997
Chisnell
,
Rama
Srinivasan
,
93
]

Chisnell
Cheryl,<lb
>
Rama
D.V
and
Padmini
Srinivasan
,

Structured
<
lb
>

Representations
of
Empirical
Information
,

In
Case
-
Based
Publicado
en
VIII
Congreso
Internacional
de
ElectrÃ³nica
,
Comunicaciones
y
Computadoras
,
CONILECOMP
98
,
Memorias
,
pp
.

,
Febrero
de
1998,<lb
>
UDLA
-
P
,
Cholula
,
Puebla
,
MÃ©xico
.

Reasoning
Information
Retrieval
,
Technical
Report
55-<lb>93
-
07
AAAI
Press
,
1993
Crismore
,
84
]

Crismore
A
The
Rhetoric
of
Textbooks:<lb

Metadiscourse
,
Journal
of
Curriculum
Studies
,
Vol
.

No
.
3
,
1984
,
pp
.

279
-
296
LÃ³pez
-
LÃ³pez
,
95
]

LÃ³pez
-
LÃ³pez
A

Beyond
Topicality:<lb

>
Exploiting
the
Metadiscourse
of
Abstracts
to
Retrieve
<
lb
>

Documents
with
Analogous
Features
,

Syracuse
University,<lb
>

Unpublished
Ph
.
D.
Dissertation
,
August
1995

LÃ³pez
-
LÃ³pez
Myaeng
,
96
]

LÃ³pez
-
LÃ³pez
A
Myaeng
<
lb

Sung
H
Extending
the
Capabilities
of
Retrieval
System
by
<
lb
>
a
Two
-
Level
Representation
of
Content
,
Procs
,
1st

Australian
Document
Computing
Symposium
,
Part
I
,
1996,<lb
>
pp
15
-
20
LÃ³pez
-
LÃ³pez
Tenorio
-
PÃ©rez
,
97
]
LÃ³pez
-
LÃ³pez
A.
and
<
lb
>

Tenorio
-
PÃ©rez
R
An
Engine
for
Detailed
Analysis
of
<
lb
>
Documents
on
the
WWW
,
In
Procs
.

ENC97
,
Database
<
lb
>

Workshop
,
Queretaro
,
Qro
,
MÃ©xico
,
September
1997
,
pp
<
lb>35
-
38
Salton
McGill
,
83
]
Salton
Gerald
,
McGill
Michael.<lb
>

Introduction
to
Modern
Information
Retrieval
,
McGraw
<

>
Hill
,
1983
SÃ¡nchez
-
LÃ³pez
LÃ³pez
-
LÃ³pez
,
97
]

SÃ¡nchez
-
LÃ³pez
M.<lb
>
H.
and
LÃ³pez
-
LÃ³pez
A

An
Improved
Search
Tool
for
the
<
lb
>
WWW
,
In
Procs
.

CIE97
,
Vol
.

I
,
Cinvestav
-
IPN
,

MÃ©xico,<lb
>
D.F.
MÃ©xico
,
September
1997
,
pp
.

263
-
266
Sowa
,
84
]

Sowa
John
F
Conceptual
Structures:<lb
>
Information
Processing
in
Mind
and
Machine
,
Addison

>
Wesley
,
Reading
MA
,
1984
Strzalkowski
92
]

Strzalkowski
Tomek
,
TTP
:

A
Fast
and
<
lb
>
Robust
Parser
for
Natural
Language
,
PROTEUS
Project

Memorandum
#
43-A
,
March
1992
Tapia
-
Melchor
,
97
]

Tapia
Melchor
,
Ma
.

ExtracciÃ³n
de
InformaciÃ³n
en
Documentos
en
la
WWW

<
lb
>
para
AnÃ¡lisis
Detallado
,
Master
Sc
.

in
Electronics
,
Thesis,<lb
>
INAOE
,
1997
.


-DOCSTART-

One
important
problem
in
Music
Information
Retrieval
is
Automatic
Music
Transcription
,
which
is
an
automated
conversion
process
from
played
music
to
a
symbolic
notation
such
as
sheet
music
.

Since
the
accuracy
of
previous
audiobased
transcription
systems
is
not
satisfactory
,
we
propose
an
innovative
visual
-
based
automatic
music
transcription
system
named
claVision
to
perform
piano
music
transcription
.

Instead
of
processing
the
music
audio
,
the
system
performs
the
transcription
only
from
the
video
performance
captured
by
a
camera
mounted
over
the
piano
keyboard
.

claVision
can
be
used
as
a
transcription
tool
,
but
it
also
has
other
applications
such
as
music
education
.

The
claVision
software
has
a
very
high
accuracy
(
over
95
and
a
very
low
latency
in
real
-
time
music
transcription
,
even
under
di
â†µ
erent
illumination
conditions
.


-DOCSTART-

There
has
been
growing
interest
in
dealing
with
large
-
seale
geographic
data
bases
because
of
the
growing
capacities
of
computer
systems
and
because
of
the
increasing
interest
in
environmental
and
social
systems
.

This
paper
outlines
the
problems
associated
with
these
data
bases
from
data
acquisition
to
simulators
and
information
retrieval
.

It
outlines
an
overall
approach
to
the
problem
using
polygons
,
point
data
,
and
common
coordinate
systems
.

It
explains
a
method
of
grid
independent
data
observation
.

It
also
deals
with
a
variety
of
specific
problems
of
file
management
and
information
handling
which
arise
in
large
-
scale
information
systems
dependent
on
data
of
a
two
-
dimensional
structure
,
as
opposed
to
data
of
a
linear
or
hierarchical
structure
.


-DOCSTART-

Collaborative
tagging
provides
exceptional
performance
in
the
domains
of
IF
(
Information
Filtering
)
and
IR
(
Information
Retrieval
Based
on
various
studies
regarding
the
tagging
behavior
of
users
,
it
can
be
concluded
that
there
is
potential
for
expansion
of
this
domain
to
the
area
of
ratings
.

The
paper
presents
Qtag
,
a
qualitative
tagging
system
that
allows
users
to
tag
in
order
to
rate
and
express
opinions
in
more
sharable
vocabulary
.

A
conceptual
model
and
evaluation
are
presented
.


-DOCSTART-

Private
Information
Retrieval
(
PIR
)
allows
for
retrieval
of
bits
from
a
database
in
a
way
that
hides
a
user
â€™s
access
pattern
from
the
server
.

However
,
its
practicality
in
a
real
-
world
cloud
computing
setting
has
recently
been
questioned
.

In
such
a
setting
,
PIR
â€™s
enormous
computation
and
communication
overhead
is
expected
to
outweigh
any
cost
saving
advantages
of
cloud
computing
.

This
paper
presents
PIRMAP
,
a
practical
,
highly
efficient
protocol
for
PIR
in
MapReduce
,
a
widely
supported
cloud
computing
API
.

PIRMAP
focuses
especially
on
the
retrieval
of
large
files
from
the
cloud
,
where
it
achieves
close
to
optimal
communication
complexity
with
query
times
significantly
faster
than
previous
schemes
.

To
achieve
this
,
PIRMAP
extends
Lipmaa
â€™s
PIR
protocol
to
allow
for
optimal
parallel
computation
during
the
â€œ
Map
â€
phase
of
MapReduce
,
and
homomorphic
aggregation
in
the
â€œ
Reduce
â€
phase
.

To
improve
computational
cost
,
we
employ
a
new
,
faster
â€œ
somewhat
homomorphic
â€
encryption
,
making
our
scheme
practical
for
databases
of
useful
size
while
still
keeping
communication
costs
low
.

PIRMAP
has
been
implemented
and
tested
in
Amazon
â€™s
public
cloud
with
total
database
sizes
of
up
to
1
TByte
.

Our
performance
evaluation
shows
that
nontrivial
PIR
such
as
PIRMAP
can
be
more
than
one
order
of
magnitude
cheaper
and
faster
than
trivial
PIR
in
the
real
-
world
.

PIRMAP
adds
only
20
%
overhead
to
a
theoretical
optimal
PIR
indicating
its
efficiency
.


-DOCSTART-

The
thesis
on
hand
presents
evaluation
procedure
and
evaluation
results
of
a
novel
Information
Retrieval
model

â€”
enhanced
Topic
-
based
Vector
Space
Model
(
eTVSM

Being
a
promising
one
,
this
model
still
lacks
quantitative
evaluations
and
comparisons
to
other
models
.

Therefore
,
this
work
deals
with
closing
this
gap
.

Evaluation
of
any
Information
Retrieval
model
is
a
highly
heuristic
task
,
primary
because
of
human
activities
involved
in
the
process
.

Thus
,
a
formal
procedure
for
evaluation
of
Information
Retrieval
model
is
proposed
.

Then
,
statistical
approaches
for
comparing
Information
Retrieval
models
are
presented
.

Finally
,
proposed
techniques
are
applied
for
performing
eTVSM
evaluations
and
comparisons
to
other
models
,
as
well
as
comparisons
of
eTVSM
under
different
internal
configurations
.

eTVSM
is
the
ontology
driven
model
.

eTVSM
search
results
are
highly
sensitive
to
ontology
configuration
.

Hence
,
a
great
deal
of
effort
is
directed
to
derivation
of
eTVSM
ontology
construction
principles
that
result
in
competitive
eTVSM
as
compared
to
other
state
of
the
art
Information
Retrieval
models
.

Due
to
eTVSM
computational
complexity
,
a
task
of
eTVSM
feasibility
for
large
scale
retrieval
systems
is
addressed
.


-DOCSTART-

Social
media
platforms
have
opened
new
dimensions
within
the
information
retrieval
domain
leading
to
a
novel
concept
known
as
Social
Information
Retrieval
.

We
argue
that
the
concept
of
Social
Information
Retrieval
can
be
extended
by
augmenting
the
huge
amount
of
content
on
the
traditional
Web
with
the
ever
-
growing
rich
Social
Web
content
to
increase
the
information
richness
of
today
â€™s
search
engines
.

This
paper
proposes
a
subjectivity
detection
framework
which
can
lead
towards
a
proposed
emotion
-
aware
search
engine
interface
.

Our
proposed
method
differs
from
previous
subjectivity
analysis
approaches
in
that
it
is
the
first
method
that
takes
into
account
social
features
of
social
media
platforms
for
the
subjectivity
classification
task
.

Through
experimental
evaluations
,
we
observe
the
accuracy
of
the
proposed
method
to
be
86.21
%
which
demonstrates
a
promising
outcome
for
large
-
scale
application
of
our
proposed
subjectivity
analysis
technique
.


-DOCSTART-

The
database
and
information
retrieval
communities
have
long
been
recognized
as
being
irreconcilable
.

Today
,
however
,
we
witness
a
surprising
convergence
of
the
techniques
used
by
both
communities
in
decentralized
,
large
-
scale
environments
.

The
newly
emerging
field
of
reputation
based
trust
management
,
borrowing
techniques
from
both
communities
,
best
demonstrates
this
claim
.

We
argue
that
incomplete
knowledge
and
increasing
autonomy
of
the
participating
entities
are
the
driving
forces
behind
this
convergence
,
pushing
the
adoption
of
probabilistic
techniques
typically
borrowed
from
an
information
retrieval
context
.

We
argue
that
using
a
common
probabilistic
framework
would
be
an
important
step
in
furthering
this
convergence
and
enabling
a
common
treatment
and
analysis
of
distributed
complex
systems
.

We
will
provide
a
first
sketch
of
such
a
framework
and
illustrate
it
with
examples
from
our
previous
work
on
information
retrieval
,
structured
search
and
trust


-DOCSTART-

Several
studies
have
identified
clinical
questions
posed
by
health
care
professionals
to
understand
the
nature
of
information
needs
during
clinical
practice
.

To
support
access
to
digital
information
sources
,
it
is
necessary
to
integrate
the
information
needs
with
a
computer
system
.

We
have
developed
a
conceptual
guidance
approach
in
information
retrieval
,
based
on
a
knowledge
base
that
contains
the
patterns
of
information
needs
.

The
knowledge
base
uses
a
formal
representation
of
clinical
questions
based
on
the
UMLS
knowledge
sources
,
called
the
Generic
Query
model
.

To
improve
the
coverage
of
the
knowledge
base
,
we
investigated
a
method
for
extracting
plausible
clinical
questions
from
the
medical
literature
.

This
poster
presents
the
Generic
Query
model
,
shows
how
it
is
used
to
represent
the
patterns
of
clinical
questions
,
and
describes
the
framework
used
to
extract
knowledge
from
the
medical
literature
.


-DOCSTART-

Zwei
Jahre
nach
ihrer
GrÃ¼ndung
veranstaltete
die
Fachgruppe
Information
Retrieval

der
GI
in
Zusammenarbeit

mit
der
Linguistischen
Informationswissenschaft
der
UniversitÃ¤t
Regensburg
ihre
erste
Fachtagung

i
m
oberpfÃ¤lzischen
Regensburg
.

Etwa
100
Teilnehmer
,
vornehmlich
aus
dem
deutschsprachigen
Raum
,
fanden
sich

i
m
mittelalterlichen
Regensburg
zusammen
,
um
17
VortrÃ¤ge
zu
verfolgen
.

i
m
voraus
sei
gesagt

,
daÃŸ
sowohl
die
3
eingeladenen

Referenten
als
auch
die
14
begutachteten
Vortragenden
Ã¼beraus
interessante
Entwicklungen
aus
dem
Bereich
des
Information
Retrieval
und
angrenzender
Gebiete
,
wie
etwa

Datenbanken
,
Benutzerschnittstellen
oder
Hypertext
vorstellten
.

Karen
Sparck
Jones
von
der
Cambridge
University
(
GB
)
erÃ¶ffnete
die

Tagung
mit
ihrem
Vortrag
"
What
might
be
in
a
summary
Nachdem
sie
einen
ausfÃ¼hrlichen
Bericht
Ã¼ber
Methoden

zur
UnterstÃ¼tzung
von

automatischen

Zusammenfassungen
gegeben
hatte
,
beschrieb
sie
ihr
eigenes
Projektvorhaben
.

Sie
teilte
die
fÃ¼r
die
Generierung
von
Zusammenfassungen
notwendige
Information
in
die
drei
Typen
linguistische
Information
,
Information
Ã¼ber
das

I
m
zweiten
eingeladenen
Vortrag
sprach
Nicholas
J.
Belkin
von
der
Rutgers
University
(
USA
)
Ã¼ber
"
Interaction
with
Texts
:

Information
Retrieval
as
Information
Seeking

Behaviour
Ausgehend
vom
Standard
Information
Retrieval
Modell
zeigte
er
,

daÃŸ
beim
Information
Retrieval
als

Suchvorgang
das
gesamte
Verhalten
eines
Benutzers
bei
der
Informationssuche
berÃ¼cksichtigt
werden
muÃŸ
.

Daraus
folgt
notwendigerweise
,
daÃŸ
ein
Information
Retrieval
System
den
Benutzer
in
allen
Stufen
des
Information
Retrieval
Prozesses
unterstÃ¼tzen
muÃŸ
.

Der
zentrale
ProzeÃŸ
dieser
UnterstÃ¼tzung
ist
die
Interaktion
mit
dem
Nutzer
.

Die
Interaktion
spielt
aber
auch
gegenÃ¼ber
anderen
Prozessen
,
wie
etwa
ReprÃ¤sentation
oder
Vergleich
eine
wichtige
Rolle
.


-DOCSTART-

A
key
component
of
experimentation
in
IR
is
<
i
>
statistical
hypothesis
testing</i
which
researchers
and
developers
use
to
make
inferences
about
the
effectiveness
of
their
system
relative
to
others
.

A
statistical
hypothesis
test
can
tell
us
the
likelihood
that
small
mean
differences
in
effectiveness
(
on
the
order
of
5
say
)
is
due
to
randomness
or
measurement
error
,
and
thus
is
critical
for
making
progress
in
research
.

But
the
tests
typically
used
in
IR
the
t
-
test
,
the
Wilcoxon
signed
-
rank
test
are
very
general
,
not
developed
specifically
for
the
problems
we
face
in
information
retrieval
evaluation
.

A
better
approach
would
take
advantage
of
the
fact
that
the
atomic
unit
of
measurement
in
IR
is
the
<
i
>
relevance
judgment</i
>
rather
than
the
effectiveness
measure
,
and
develop
tests
that
model
relevance
directly
.

In
this
work
we
present
such
an
approach
,
showing
theoretically
that
modeling
relevance
in
this
way
naturally
gives
rise
to
the
effectiveness
measures
we
care
about
.

We
demonstrate
the
usefulness
of
our
model
on
both
simulated
data
and
a
diverse
set
of
runs
from
various
TREC
tracks
.


-DOCSTART-

The
natural
language
processing
has
a
set
of
phases
that
evolves
from
lexical
text
analysis
to
the
pragmatic
one
in
which
the
author
â€™s
intentions
are
shown
.

The
ambiguity
problem
appears
in
all
of
these
tasks
.

Previous
works
tries
to
do
word
sense
disambiguation
,
the
process
of
assign
a
sense
to
a
word
inside
a
specific
context
,
creating
algorithms
under
a
supervised
or
unsupervised
approach
,
which
means
that
those
algorithms
use
or
not
an
external
lexical
resource
.

This
paper
presents
an
approximated
approach
that
combines
not
supervised
algorithms
by
the
use
of
a
classifiers
set
,
the
result
will
be
a
learning
algorithm
based
on
unsupervised
methods
for
word
sense
disambiguation
process
.

It
begins
with
an
introduction
to
word
sense
disambiguation
concepts
and
then
analyzes
some
unsupervised
algorithms
in
order
to
extract
the
best
of
them
,
and
combines
them
under
a
supervised
approach
making
use
of
some
classifiers
.


-DOCSTART-

Activity
recognition
is
becoming
an
important
research
area
,
and
finding
its
way
to
many
application
domains
ranging
from
daily
life
services
to
industrial
zones
.

Sensing
hardware
and
learning
algorithms
are
two
important
components
in
activity
recognition
.

For
sensing
devices
,
we
prefer
to
use
accelerometers
due
to
low
cost
and
low
power
requirement
.

For
learning
algorithms
,
we
propose
a
novel
implementation
of
the
semi
-
Markov
Conditional
Random
Fields
(
semi
-
CRF
)
introduced
by
Sarawagi
and
Cohen
.

Our
implementation
not
only
outperforms
the
original
method
in
terms
of
computation
complexity
(
at
least
10
times
faster
in
our
experiments
)
but
also
is
able
to
capture
the
interdependency
among
labels
,
which
was
not
possible
in
the
previously
proposed
model
.

Our
results
indicate
that
the
proposed
approach
works
well
even
for
complicated
activities
like
eating
and
driving
a
Â 
car
.

The
average
precision
and
recall
are
88.47
%
and
Â 
86.68
respectively
,
which
are
higher
than
results
obtained
by
using
other
methods
such
as
Hidden
Markov
Model
(
HMM
)
or
Topic
Model
(
TM
)
.


-DOCSTART-

In
the
Dempster
-
Shafer
's
theory
of
evidence
,
for
incorporating
uncertainty
,
the
valuation
assigns
to
the
data
tables
the
degrees
of
belief
for
these
data
.

Firstly
,
we
are
looking
for
the
answers
to
the
following
questions
.

Is
there
a
valuation
-
based
system
in
which
combination
and
marginalization
operate
on
valuations
?

Has
this
system
prosperities
analogical
to
the
t
-
norm
system
?

In
the
t
-
norm
system
of
the
valuation
for
the
specific
database
attributes
configuration
can
be
described
the
algebra
of
possible
data
set
in
which
can
be
interpreted
the
Information
Retrieval
Logic
.


-DOCSTART-

Two
tasks
have
been
put
forward
in
the
MSR
-
bing
Grand
Challenge
2015
.

To
address
the
information
retrieval
task
,
we
raise
and
integrate
a
series
of
methods
with
visual
features
obtained
by
convolution
neural
network
(
CNN
)
models
.

In
our
experiments
,
we
discover
that
the
ranking
strategies
of
Hierarchical
clustering
and
PageRank
methods
are
mutually
complementary
.

Another
task
is
fine
-
grained
classification
.

In
contrast
to
basic
-
level
recognition
,
fine
-
grained
classification
aims
to
distinguish
between
different
breeds
or
species
or
product
models
,
and
often
requires
distinctions
that
must
be
conditioned
on
the
object
pose
for
reliable
identification
.

Current
state
-
of
-
the
-
art
techniques
rely
heavily
upon
the
use
of
part
annotations
,
while
the
bing
datasets
suffer
both
abundance
of
part
annotations
and
dirty
background
.

In
this
paper
,
we
propose
a
CNN
-
based
feature
representation
for
visual
recognition
only
using
image
-
level
information
.

Our
CNN
model
is
pre
-
trained
on
a
collection
of
clean
datasets
and
fine
-
tuned
on
the
bing
datasets
.

Furthermore
,
a
multi
-
scale
training
strategy
is
adopted
by
simply
resizing
the
input
images
into
different
scales
and
then
merging
the
soft
-
max
posteriors
.

We
then
implement
our
method
into
a
unified
visual
recognition
system
on
Microsoft
cloud
service
.

Finally
,
our
solution
achieved
top
performance
in
both
tasks
of
the
contest


-DOCSTART-

We
participated
in
the
Cross
-
Language
Information
Retrieval
evaluation
at
NTCIR-3
for
the
EnglishChinese
and
English
-
Japanese
tasks
.

We
examined
several
approaches
to
query
translation
,
including
the
use
of
a
commercial
machine
translation
system
,
a
thesaurus
that
is
automatically
extracted
from
a
parallel
corpus
,
and
a
general
-
purpose
online
dictionary
.

The
MT
-
based
approach
was
most
effective
among
these
alternatives
in
our
experiments
for
English
-
Chinese
retrieval
on
the
NTCIR-2
and
3
data
.

Combined
use
of
machine
translation
and
thesaurus
extraction
yielded
further
improvement
.


-DOCSTART-

The
publishers
will
keep
this
article
on
-
line
on
the
Internet
(
or
its
possible
replacement
network
in
the
future
)
for
a
period

2
5
y
e
ars
from
the
date
of
publication
,
barring
exceptional
circumstances
as
described
s
e
p
arately
.

The
on
-
line
availability
of
the
article
implies
a
p
ermanent
permission
for
anyone
to
read
the
article
on
-
line
,
to
print
out
single
copies
of
it
,
and
to
use
it
unchanged
for
any
non
-
commercial
research
and
educational
purpose
,
including
making
copies
for
classroom
use
.

This
permission
can
not
be
r
evoked
by
subsequent
transfers
of
copyright
.

All
other
uses
of
the
article
are
conditional
on
the
consent
of
the
copyright
owner
.

The
publication
of
the
article
on
the
date
stated
a
b
ove
included
also
the
production
of
a
limited
number
of
copies
on
paper
,
which
were
a
r
chived
i
n
S
w
e
dish
university
libraries
like
all
other
written
works
published
i
n
S

The
publisher
has
taken
technical
and
administrative
measures
to
assure
that
the
on
-
line
version
of
the
article
will
be
permanently
accessible
using
the
URL
stated
a
b
ove
,
unchanged
,
and
permanently
equal
to
the
archived
printed
c
opies
at
least
until
the
expiration
of
the
publication
period
.

For
additional
information
about
the
Linkk
oping
University
Electronic
Pre
s
s
a
n

d
i
t
s
p
r
ocedures
for
publication
and
for
assurance
o
f
d
o
cument
integrity
,
please
refer
to
its
WWW
home
page

:
Abstract
Slicing
is
a
program
analysis
technique
originally
developed
for
imperative
languages
.

This
paper
discusses
slicing
of
CLP
programs
.

As
data
ow
i
n
such
programs
is
not
explicit
,
the
slicing
problem
requires
a
speciic
formulation
and
speciic
techniques
.

The
paper
gives
a
precise
deenition
of
the
slicing
problem
for
CLP
and
presents
a
simple
slicing
method
based
on
variable
sharing
and
groundness
analysis
.

Our
approach
i
s
b
a
s
e
d
on
three
declarative
notions
of
slice
1
)
for
a
set
of
constraints
(
2
)
for
a
derivation
tree
of
a
CLP
program
,
and
(
3
)
for
a
CLP
program
.

The
rst
of
them
is
a
basic
one
,
expressed
in
terms
of
logic
and
used
to
deene
the
remaining
ones
.

i
v
ation
tree
of
a
CLP
program
characterizes
a
state
of
the
computation
of
a
CLP
program
in
terms
of
the
associated
set
of
constraints
.

We
use
the
â€¦


-DOCSTART-

Article
history
:

Received
30
June
2014
Received
in
revised
form
21
December
2014

Accepted
16
January
2015
Available
online
28
January
2015
As
the
rapid
development
of
the
information
science
and
technology
especially
the
network
technique
,
e
-
government
has
a
very
promising
application
foreground
.

E
-
government
acts
as
a
quite
new
formof
government
affairs
and
also
an
effective
exploration
of
government
innovation
.

The
development
and
popularizing
of
the
network
puts
forward
a
new
request
and
challenge
to
the
government
.

This
paper
devotes
to
evaluate
e
-
government
performances
of
31
provincial
governmentwebsites
in
China
using
themethod
of
DEA
.

Research
results
show
thatmost
of
these
provincial
governmentwebsites
operate
at
an
inefficient
level
and
in
a
badmanner
.

Moreover
,
the
e
-
government
efficiency
differences
lie
in
both
different
individual
provinces
and
different
districts
.

It
is
confirmed
that
the
less
developed
western
provinces
achieve
a
higher
efficiency
mean
than
the
eastern
and
middle
ones
2015
Elsevier
Inc.

All
rights
reserved
.


-DOCSTART-

Manfred
Kochen
was
educated
at
MIT
with
a
B.S.
in
Science
(
1950
)
and
at
Columbia
University
in
New
York
,
where
he
received
an
MA
in
1951
and
a
Ph.D.
in
1955
in
Applied
Mathematics
.

Since
1972
he
has
been
Professor
of
Information
Science
,
with
a
joint
appointment
as
Adjunct
Professor
of
Computers
and
Information
Systems
at
the
Graduate
School
of
Business
Administration
and
as
Research
Scientist
at
the
Mental
Health
Research
Institute
.

His
major
research
interests
are
in
problems
of
representation
of
knowledge
that
could
help
people
cope
with
a
greater
variety
of
tasks
in
more
effective
ways
as
well
as
in
organization
of
knowledge
.

He
is
the
author
or
editor
of
six
books
,
including
Information
for
Action
he
has
over
150
scientific
papers
in
technical
journals
or
books
,
including
three
on
the
process
of
referee
-
selection
and
peer
review
.

In
1974
he
was
awarded
the
Award
of
Merit
by
the
American
Society
for
Information
Science
.

He
was
also
named
National
Lecturer
by
the
Association
for
Computing
Machinery
.

He
has
been
an
editor
of
the
Journal
of
the
ACM
,
of
Behavioral
Science
and
is
on
the
editorial
boards
of
several
international
journals
.

He
has
been
very
active
in
the
American
Society
for
Information
Science
.

He
is
increasingly
interested
in
all
aspects
of
'
Coping
with
Social
Complexity
which
is
the
title
of
the
section
of
Human
Systems
Management
that
he
is
responsible
for
as
managing
editor
.

He
also
has
a
long
-
term
interest
in
the
process
of
doing
science
,
of
which
journal
publication
comprises
the
later
stages
.

He
is
therefore
receptive
to
and
interested
in
good
'
new
ideas
for
innovations
in
the
publication
process
leading
toward
improved
papers
and
particularly
toward
helping
authors
to
improve
the
quality
of
manuscripts
they
produce
in
the
first
place
.


-DOCSTART-

The
aim
of
this
work
is
to
present
methods
some
of
the
ongoing
research
done
as
a
part
of
development
of
Semantically
Enhanced
Intellectual
Property
Protection
System
SEIPro2S.
Main
focus
is
on
description
of
methods
that
allow
for
creation
of
more
concise
documents
preserving
semantically
the
same
meaning
as
their
originals
.

Thus
,
compacting
methods
are
denoted
as
a
semantic
compression
.


-DOCSTART-

Concept
-
based
information
retrieval
and
knowledge
representation
are
in
need
of
a
theory
of
concepts
and
semantic
relations
.

Guidelines
for
the
construction
and
maintenance
of
knowledge
organization
systems
(
KOS
such
as
ANSI
/

NISO
Z39.19
-
2005
in
the
U.S.A.
or
DIN
2331:1980
in
Germany
)

do
not
consider
results
of
concept
theory
and
theory
of
relations
to
the
full
extent
.

They
are
not
able
to
unify
the
currently
different
worlds
of
traditional
controlled
vocabularies
,
of
the
social
web
(
tagging
and
folksonomies
)
and
of
the
semantic
web
(
ontologies
Concept
definitions
as
well
as
semantic
relations
are
based
on
epistemological
theories
(
empiricism
,
rationalism
,
hermeneutics
,
pragmatism
,
and
critical
theory

A
concept
is
determined
via
its
intension
and
extension
as
well
as
by
definition
.

We
will
meet
the
problem
of
vagueness
by
introducing
prototypes
.

Some
important
definitions
are
concept
explanations
(
after
Aristotle
)
and
the
definition
of
family
resemblances
(
in
the
sense
of
Wittgenstein
We
will
model
concepts
as
frames
(
according
to
Barsalou
The
most
important
paradigmatic
relation
in
KOS
is
hierarchy
,
which
must
be
arranged
into
different
classes
:

Hyponymy
consists
of
taxonomy
and
simple
hyponymy
,
meronymy
consists
of
many
different
part
-
whole
-
relations
.

For
practical
application
purposes
,
the
transitivity
of
the
given
relation
is
very
important
.

Unspecific
associative
relations
are
of
little
help
to
our
focused
applications
and
should
be
replaced
by
generalizable
and
domain
-
specific
relations
.

We
will
discuss
the
reflexivity
,
symmetry
,
and
transitivity
of
paradigmatic
relations
as
well
as
the
appearance
of
specific
semantic
relations
in
the
different
kinds
of
KOS
(
folksonomies
,
nomenclatures
,
classification
systems
,
thesauri
,
and
ontologies
Finally
,
we
will
pick
out
KOS
as
a
central
theme
of
the
Semantic
Web
.


-DOCSTART-

Named
Entity
Disambiguation
(
NED
)
aims
at
disambiguating
named
entity
mentions
in
a
text
to
their
corresponding
entries
in
a
knowledge
base
such
as
Wikipedia
.

Itis
a
fundamental
task
in
Natural
Language
Processing
(
NLP)and
has
many
applications
such
as
information
extraction
,
information
retrieval
,
and
knowledge
acquisition
.

In
the
past
decade
,
a
number
of
methods
have
been
proposed
for
the
NED
task
.

However
,
most
of
existing
work
focuses
on
exploring
many
more
useful
information
to
help
tackle
this
problem
.

The
effectiveness
of
different
features
proposed
for
the
task
are
not
well
-
studied
in
a
same
platform
.

In
this
paper
,
we
extract
various
remarkable
features
by
leveraging
statistical
,
textual
and
semantic
information
,
and
evaluate
various
combinations
of
the
multiaspect
features
for
the
disambiguation
task
in
the
same
platform
.

Specifically
,
we
utilize
two
learning
to
rank
methods
to
combine
different
features
,
train
and
test
the
combined
methods
on
several
standard
data
sets
.

Through
extensive
experiments
,
we
investigate
the
effects
on
the
quality
of
the
disambiguation
of
exploiting
different
features
and
show
which
combinations
of
features
are
the
best
choices
for
disambiguation
.


-DOCSTART-

As
an
alternative
to
the
usual
Mean
Average
Precision
,
some
use
is
currently
being
made
of
the
Geometric
Mean
Average
Precision
(
GMAP
)
as
a
measure
of
average
search
effectiveness
across
topics
.

GMAP
is
specifically
used
to
emphasise
the
lower
end
of
the
average
precision
scale
,
in
order
to
shed
light
on
poor
performance
of
search
engines
.

This
paper
discusses
the
status
of
this
measure
and
how
it
should
be
understood
.


-DOCSTART-

In
this
paper
,
we
propose
a
retrieval
model
for
news
-
based
information
contents
over
the
WWW
.

We
have
integrated
web
-
tickers
with
a
navigation
browser
.

It
works
as
an
adaptable
dynamic
information
retrieval
tool
embedded
with
the
browser
for
the
web
navigation
.

The
system
has
been
implemented
using
both
mobile
and
static
agents
,
but
provides
inter
-
operability
between
the
agents
and
the
conventional
requestresponse
environment
.


-DOCSTART-

Humanity
is
always
under
the
threat
of
earthquakes
.

Anatolian
peninsula
is
one
of
the
well
-
known
area
which
amongst
the
areas
endangered
by
earthquakes
.

During
the
history
many
dramatic
examples
have
been
occurred
.

In
these
earthquakes
many
people
either
died
or
have
been
injured
.

In
addition
,
lots
of
damage
in
this
area
has
been
occurred
.

More
west
,
in
the
Sea
of
Marmara
,
these
earthquakes
have
also
initiated
Tsunamis
which
hit
the
coastline
and
caused
secondary
damages
.

Modern
technologies
in
combination
with
remotely
sensed
data
in
GIS
environment
open
a
wide
field
for
assisting
in
Crisis
Management
.

The
most
important
component
of
any
Crisis
Management
System
is
a
Crisis
Preparedness
Plan
where
especially
our
disciplines
of
Photogrammetry
,
Remote
Sensing
and
Spatial
Information
Science
can
contribute
in
many
ways
.

Crisis
Preparedness
plays
a
key
role
in
preventing
the
population
against
big
disasters
.

All
Crisis
Management
efforts
need
an
interdisciplinary
cooperation
to
receive
a
sustainable
help
for
all
citizens
.

In
our
paper
,
we
aim
to
highlight
the
possible
contributions
of
our
disciplines
by
examples
of
Earthquakeand
Tsunami
-
Risk
for
Istanbul
.

Part
of
the
discussed
elements
are
referred
to
existing
applications
already
installed
or
under
construction
around
the
world
,
others
are
taken
from
own
studies
in
the
area
of
Istanbul
.

Nowadays
Crisis
Management
System
is
founded
on
3
columns
,
the
Crisis
Preparedness
Plan
,
the
Early
Warning
System
and
the
Rescue
and
Management
Action
.


-DOCSTART-

Sammendrag
/
Abstract
:

This
report
gives
a
brief
overview
of
different
applications
,
problems
and
methods
related
to
pattern
recognition
in
music
.

Many
of
the
applications
of
musical
pattern
recognition
are
connected
to
music
information
retrieval
.

This
area
covers
fields
like
information
retrieval
,
signal
processing
,
pattern
recognition
,
artificial
intelligence
,
databases
,
computer
music
and
music
cognition
.

The
report
focuses
on
problems
and
methods
related
to
signal
processing
and
pattern
recognition
.

Automatic
music
transcription
and
content
-
based
music
retrieval
are
the
problems
that
have
received
the
most
attention
within
this
area
.

For
music
transcription
the
current
state
-
of
-
the
-
art
is
that
monophonic
transcription
for
well
-
defined
musical
instruments
has
to
a
large
degree
been
solved
as
a
research
problem
,
while
transcription
of
polyphonic
music
remains
a
research
issue
for
the
general
case
.

Content
-
based
retrieval
based
on
audio
queries
is
somewhat
dependent
on
the
transcription
,
although
a
full
transcription
may
not
be
necessary
to
find
similarity
.

Other
problems
like
genre
classification
,
music
summarization
and
musical
instrument
recognition
are
also
treated
in
the
report
.

These
are
also
problems
that
are
related
to
music
retrieval
in
that
these
techniques
can
be
used
for
organizing
the
music
databases
and
to
present
the
results
to
users
.

Less
research
has
been
done
in
these
areas
.


-DOCSTART-

Efficient
management
of
a
wireless
local
area
network
is
one
of
the
goals
set
by
faculty
of
information
science
and
technology
(
FTSM
)
UKM
,
in
order
to
provide
wireless
network
connection
to
its
staffs
,
students
and
visitors
.

An
investigation
was
conducted
to
measure
the
signal
strength
and
overlaps
in
order
to
ascertain
the
performance
and
dimension
measures
of
the
current
wireless
network
and
its
future
requirement
.

The
existing
area
under
study
covers
six
buildings
inclusive
of
a
lecture
theatre
and
the
experiments
were
done
on
several
different
days
and
time
slots
.

A
comparison
of
the
network
performance
was
also
measured
by
taking
into
accounts
the
number
of
connected
users
at
any
one
time
.

The
results
from
the
research
shows
that
wireless
access
points
can
be
more
efficiently
located
in
strategic
places
for
continuous
near
excellent
connectivity
and
also
allows
for
more
effective
systematic
planning
and
placement
of
access
points
in
the
future
expansion
of
the
network
.


-DOCSTART-

The
effect
of
using
paat
queries
to
improve
automatic
query
expansion
was
examined
in
the
TREC
environment
.

Automatic
feedback
of
documents
identified
from
similar
past
queries
was
compared
with
standard
top
-
document
feedback
and
with
no
feedback
.

A
new
query
similarity
metric
was
used
based
on
comparing
result
lists
and
using
probability
of
relevance
.

Our
top
-
document
feedback
method
showed
small
improvements
over
no
feedback
method
consistent
with
past
studies
.

On
recall
-
precision
and
average
precision
measures
,
past
query
feedback
yielded
performance
superior
to
that
of
top
-
document
feedback
.

The
past
query
feedback
method
also
lends
itself
to
tunable
thresholds
such
that
better
performance
can
be
obtained
by
automatically
deciding
when
,
and
when
not
,
to
apply
the
expansion
.

Automatic
past
-
query
feedback
actually
improved
top
document
precision
in
this
experiment
.


-DOCSTART-

Term
frequency
normalization
is
a
serious
issue
since
lengths
of
documents
are
various
.

Generally
,
documents
become
long
due
to
two
different
reasons
verbosity
and
multi
-
topicality
.

First
,
verbosity
means
that
the
same
topic
is
repeatedly
mentioned
by
terms
related
to
the
topic
,
so
that
term
frequency
is
more
increased
than
the
well
-
summarized
one
.

Second
,
multi
-
topicality
indicates
that
a
document
has
a
broad
discussion
of
multi
-
topics
,
rather
than
single
topic
.

Although
these
document
characteristics
should
be
differently
handled
,
all
previous
methods
of
term
frequency
normalization
have
ignored
these
differences
and
have
used
a
simplified
length
-
driven
approach
which
decreases
the
term
frequency
by
only
the
length
of
a
document
,
causing
an
unreasonable
penalization
.

To
attack
this
problem
,
we
propose
a
novel
TF
normalization
method
which
is
a
type
of
partially
-
axiomatic
approach
.

We
first
formulate
two
formal
constraints
that
the
retrieval
model
should
satisfy
for
documents
having
verbose
and
multi
-
topicality
characteristic
,
respectively
.

Then
,
we
modify
language
modeling
approaches
to
better
satisfy
these
two
constraints
,
and
derive
novel
smoothing
methods
.

Experimental
results
show
that
the
proposed
method
increases
significantly
the
precision
for
keyword
queries
,
and
substantially
improves
MAP
(
Mean
Average
Precision
)
for
verbose
queries
.


-DOCSTART-

With
Ihe
growth
of
digital
libraries
and
electronic
publishing
,
many
structured
document
sources
are
appearing
and
their
efleclive
mediation
is
an
imporlanl
research
topic
.

In
this
paper
,
we
propose
a
wrapping
architecture
for
externally
maintained
struclured
document
sources
.

Our
wrapping
target
is
information
retrieval
systems
(
IRSs
)
that
provide
access
to
strucaured
documenk
We
describe
a
wrapper
construction
method
for
such
IRSs
with
limited
functionality
.

A
constructed
wrapper
enhances
retneval
facilities
of
Ihe
underlying
IRS
and
provides
an
object
database
view
lo
the
mediator
.

We
focus
on
determining
whether
the
underlying
IRS
can
support
a
given
query
.

Then
we
discuss
some
research
issues
related
to
OUT
wrapping
architecture
.


-DOCSTART-

This
paper
describes
a
model
for
optimum
information
retrieval
over
a
distributed
document
collection
.

The
model
stems
from
Robertson
's
Probability
Ranking
Principle
:
Having
computed
individual
document
rankings
correlated
to
diierent
subcollections
,
these
local
rankings
are
stepwise
merged
into
a
nal
ranking
list
where
the
documents
are
ordered
according
to
their
probability
of
relevance
.

Here
,
a
full
dissemination
of
subcollection
-
wide
information
is
not
required
.

The
documents
of
diierent
subcollec
-
tions
are
assumed
to
be
indexed
using
diierent
indexing
vocabularies
.

Moreover
,
local
rankings
may
be
computed
by
individual
probabilistic
retrieval
methods
.

The
underlying
data
volume
is
arbitrarily
scalable
.

A
criterion
for
eeectively
limiting
the
ranking
process
to
a
subset
of
subcollections
extends
the
model
.


-DOCSTART-

Quantum
entanglement
is
one
of
the
most
remarkable
aspects
of
quantum
physics
.

If
two
particles
are
in
a
entangled
state
,
then
,
even
if
the
particles
are
physically
separated
by
a
great
distance
,
they
behave
in
some
repects
as
a
single
entity
.

Entanglement
is
a
key
resource
for
quantum
information
processing
and
spatially
separated
entangled
pairs
of
particles
have
been
used
for
numerous
purposes
such
as
teleportation
,
superdense
coding
and
cryptography
based
on
Bell
â€™s
theorem
.

Just
as
two
distant
particles
could
be
entangled
,
it
is
also
possible
to
entangle
three
or
more
separated
particles
.

A
well
-
known
application
of
multipartite
entanglement
is
in
testing
nonlocality
from
different
directions
.

Recently
,
it
has
also
been
used
for
many
multi
-
party
computation
and
communication
tasks
and
multi
-
party
cryptography
.

One
of
the
major
issues
in
dealing
with
multi
-
partite
entangled
states
is
of
purification
.

Distilling
pure
maximally
entangled
state
in
this
case
may
not
be
as
simple
as
that
of
bipartite
case
.

But
if
it
is
possible
to
create
multi
-
partite
entangled
states
from
the
bipartite
ones
then
we
can
first
distill
pure
maximal
bipartite
states
and
can
then
prepare
the
multi
-
partite
ones
.

To
this
end
,
we
consider
the
problem
of
creating
maximally
entangled
multi
-
partite
states
out
of
Bell
pairs
distributed
in
a
communication
network
from
a
physical
as
well
as
from
a
combinatorial
perspective
.

We
investigate
the
minimal
combinatorics
of
Bell
pairs
distribution
required
for
this
purpose
and
discuss
how
this
combinatorics
gives
rise
to
resource
minimization
for
practical
implementations
.

We
present
two
protocols
for
this
purpose
.

The
first
protocol
enables
to
prepare
a
GHZ
state
using
two
Bell
pairs
shared
amongst
the
three
users
with
help
of
two
cbits
of
communication
and
local
operations
.

The
protocol
involes
all
the
three
users
dynamically
and
thus
can
find
applications
in
cryptographic
tasks
.

Second
protocol
entails
the
use
of
O(n
)
cbits
of
communication
and
local
operations
to
prepare
an
n
partite
maximally
entangled
state
in
a
distributed
network
of
bell
pairs
along
a
spanning
tree
of
EPR
graph
of
the
n
users
.

We
show
that
this
spanning
tree
structure
is
the
minimal
combinatorial
requirement
.

We
also
characterize
the
minimal
combinatorics
of
agents
in
the
creation
of
pure
maximal
multi
-
partite
entanglement
amongst
the
set
N
of
n
agents
in
a
network
using
apriori
multi
-
partite
entanglement
states
amongst
subsets
of
N
Another
major
and
interesting
issue
is
of
quantifying
multi
-
partite
entangled
states
.

Multi
-
partite
entangled
states
,
unlike
the
bipartite
ones
,
lack
convenient
mathematical
properties
like
Schmidt
decompostion
and
therefore
it
becomes
difficult
to
characterize
them
.

Some
approches
,
essentially
using
the
generalization
of
Schmidt
decomposition
,
have
been
taken
in
this
direction
however
a
general
formulation
in
this
case
is
still
an
outstanding
unresolved
problem
.

State
transformations
under
local
operations
and
classical
communication
(
LOCC
)
are
very
important
while
quantifying
entanglement
because
LOCC
can
at
the
best
increase
classical
correlations
and
therefore
a
good
measure
of
entanglement
is
not
supposed
to
increase
under
LOCC
.

All
the
current
approaches
to
study
the
state
transformation
under
LOCC
are
based
on
entropic
criterion
.

We
present
an
entirely
different
approach
based
on
nice
combinatorial
properties
of
graphs
and
set
systems
.

We
introduce
a
technique
called
bicolored
merging
and
obtain
several
results
about
such
transformations
.

We
demostrate
a
partial
ordering
of
multi
-
partite
states
and
various
classes
of
incomparable
multi
-
partite
states
.

We
utilize
these
results
to
establish
the
impossibility
of
doing
selective
teleportation
in
a
case
where
the
apriori
entanglement
is
in
the
form
of
a
GHZ
state
.

We
also
discuss
the
minimum
number
of
copies
of
a
state
required
to
prepare
another
state
by
LOCC
and
present
bounds
on
this
number
in
terms
of
quantum
distance
between
the
two
states
.

The
ideas
developed
in
this
work
continues
the
combinatorial
setting
mentioned
above
and
can
been
extended
to
incorporate
other
new
kinds
of
multi
-
partite
states
.

Moreover
,
the
idea
of
bicolored
merging
may
also
be
appropriate
to
some
other
areas
of
information
sciences
.

Key
distribution
is
a
fundamental
problem
in
secure
communication
and
quantum
key
distribution
(
QKD
)
protocols
for
key
distribution
between
two
parties
on
the
account
of
quantum
uncertainty
and
no
-
cloning
principles
was
realized
two
decades
ago
,
however
the
more
rigorous
and
comprehensive
proofs
of
this
task
,
taking
into
consideration
source
,
device
and
channel
noise
as
well
as
an
arbitrarily
powerful
eavesdropper
,
have
been
only
recently
studied
by
various
authors
.

We
consider
QKD
between
two
parties
extended
to
that
between
n
trustful
parties
,
that
is
,
how
the
n
parties
may
share
an
identical
secret
key
among
themselves
.

We
propose
a
protocol
for
this
purpose
and
prove
its
unconditional
security
.

The
protocol
is
simple
in
the


-DOCSTART-

In
this
paper
,
we
study
different
applications
of
cross
-
language
latent
topic
models
trained
on
comparable
corpora
.

The
first
focus
lies
on
the
task
of
cross
-
language
information
retrieval
(
CLIR

The
Bilingual
Latent
Dirichlet
Â 
allocation
model
(
BiLDA
)
allows
us
to
create
an
interlingual
,
language
-
independent
representation
of
both
queries
and
documents
.

We
construct
several
BiLDA
-
based
document
models
for
CLIR
,
where
no
additional
translation
resources
are
used
.

The
second
focus
lies
on
the
methods
for
extracting
translation
candidates
and
semantically
related
words
using
only
per
-
topic
word
distributions
of
the
cross
-
language
latent
topic
model
.

As
the
main
contribution
,
we
combine
the
two
former
steps
,
blending
the
evidences
from
the
per
-
document
topic
distributions
and
the
per
-
topic
word
distributions
of
the
topic
model
with
the
knowledge
from
the
extracted
lexicon
.

We
design
and
evaluate
the
novel
evidence
-
rich
statistical
model
for
CLIR
,
and
prove
that
such
a
model
,
which
combines
various
(
only
internal
)
evidences
,
obtains
the
best
scores
for
experiments
performed
on
the
standard
test
collections
of
the
CLEF
2001â€“2003
campaigns
.

We
confirm
these
findings
in
an
alternative
evaluation
,
where
we
automatically
generate
queries
and
perform
the
known
-
item
search
on
a
test
subset
of
Wikipedia
articles
.

The
main
importance
of
this
work
lies
in
the
fact
that
we
train
translation
resources
from
comparable
document
-
aligned
corpora
and
provide
novel
CLIR
statistical
models
that
exhaustively
exploit
as
many
cross
-
lingual
clues
as
possible
in
the
quest
for
better
CLIR
results
,
without
use
of
any
additional
external
resources
such
as
parallel
corpora
or
machine
-
readable
dictionaries
.


-DOCSTART-

This
paper
discusses
the
Visual
Information
Retrieval
(
VIR
)
process
and
emphasizes
the
need
for
more
natural
interfaces
and
a
better
understanding
of
the
user
.

The
central
role
of
user
modelling
and
the
importance
of
the
communication
channel
in
a
VIR
scenario
are
discussed
.

We
identify
areas
of
research
and
possible
approaches
for
dealing
with
the
problem
of
creating
effective
,
user
responsive
VIR
systems
.


-DOCSTART-

By
synthesizing
information
common
to
retrieved
documents
,
multi
-
document
summarization
can
help
users
of
information
retrieval
systems
to
find
relevant
documents
with
a
minimal
amount
of
reading
.

We
are
developing
a
multidocument
summarization
system
to
automatically
generate
a
concise
summary
by
identifying
and
synthesizing
similarities
across
a
set
of
related
documents
.

Our
approach
is
unique
in
its
integration
of
machine
learning
and
statistical
techniques
to
identify
similar
paragraphs
,
intersection
of
similar
phrases
within
paragraphs
,
and
language
generation
to
reformulate
the
wording
of
the
summary
.

Our
evaluation
of
system
components
shows
that
learning
over
multiple
extracted
linguistic
features
is
more
effective
than
information
retrieval
approaches
at
identifying
similar
text
units
for
summarization
and
that
it
is
possible
to
generate
a
fluent
summary
that
conveys
similarities
among
documents
even
when
full
semantic
interpretations
of
the
input
text
are
not
available
.


-DOCSTART-

The
detection
of
synonyms
is
a
challenge
that
has
attracted
many
contributions
for
the
possible
applications
in
many
areas
,
including
Semantic
Web
and
Information
Retrieval
.

An
open
challenge
is
to
identify
synonyms
of
a
term
that
are
appropriate
for
a
specific
domain
,
not
just
all
the
synonyms
.

Moreover
,
the
execution
time
is
critical
when
handling
big
data
.

Therefore
,
it
is
needed
an
algorithm
which
can
perform
accurately
and
fast
in
detecting
domain
-
appropriate
synonyms
on
-
thefly
.

This
contribution
presents
SynFinder
which
uses
WordNet
and
the
web
of
data
.

Given
a
term
and
a
domain
in
input
,
WordNet
is
used
for
the
retrieval
of
all
the
synonyms
of
the
term
.

Then
,
synonyms
which
do
not
appear
in
web
pages
related
to
the
domain
are
eliminated
.

Our
experimentation
shows
a
very
good
accuracy
and
computation
performance
of
SynFinder
,
reporting
a
mean
precision
of
0.94
and
an
average
execution
time
lower
than
1
s.


-DOCSTART-

In
this
survey
,
we
reviewed
the
current
state
of
the
art
in
biomedical
QA
(
Question
Answering
within
a
broader
framework
of
semantic
knowledge
-
based
QA
approaches
,
and
projected
directions
for
the
future
research
development
in
this
critical
area
of
intersection
between
Artificial
Intelligence
,
Information
Retrieval
,
and
Biomedical
Informatics
.

MATERIALS
AND
METHODS

We
devised
a
conceptual
framework
within
which
to
categorize
current
QA
approaches
.

In
particular
,
we
used
"
semantic
knowledge
-
based
QA
"
as
a
category
under
which
to
subsume
QA
techniques
and
approaches
,
both
corpus
-
based
and
knowledge
base
(
KB)-based
,
that
utilize
semantic
knowledge
-
informed
techniques
in
the
QA
process
,
and
we
further
classified
those
approaches
into
three
subcategories
1
)
semantics
-
based
2
)
inference
-
based
,
and
(
3
)
logic
-
based
.

Based
on
the
framework
,
we
first
conducted
a
survey
of
open
-
domain
or
non
-
biomedical
-
domain
QA
approaches
that
belong
to
each
of
the
three
subcategories
.

We
then
conducted
an
in
-
depth
review
of
biomedical
QA
,
by
first
noting
the
characteristics
of
,
and
resources
available
for
,
biomedical
QA
and
then
reviewing
medical
QA
approaches
and
biological
QA
approaches
,
in
turn
.

The
research
articles
reviewed
in
this
paper
were
found
and
selected
through
online
searches
.

Our
review
suggested
the
following
tasks
ahead
for
the
future
research
development
in
this
area
1
)
Construction
of
domain
-
specific
typology
and
taxonomy
of
questions
(
biological
QA
2
)
Development
of
more
sophisticated
techniques
for
natural
language
(
NL
)
question
analysis
and
classification
3
)
Development
of
effective
methods
for
answer
generation
from
potentially
conflicting
evidences
4
)

More
extensive
and
integrated
utilization
of
semantic
knowledge
throughout
the
QA
process
,
and
(
5
)
Incorporation
of
logic
and
reasoning
mechanisms
for
answer
inference
.

CONCLUSION
Corresponding
to
the
growth
of
biomedical
information
,
there
is
a
growing
need
for
QA
systems
that
can
help
users
better
utilize
the
ever
-
accumulating
information
.

Continued
research
toward
development
of
more
sophisticated
techniques
for
processing
NL
text
,
for
utilizing
semantic
knowledge
,
and
for
incorporating
logic
and
reasoning
mechanisms
,
will
lead
to
more
useful
QA
systems
.


-DOCSTART-

The
FIRE
2017
IRLeD
Track
focused
on
creating
a
framework
for
evaluating
different
methods
of
Information
Retrieval
from
legal
documents
.

Therewere
two
tasks
for
this
track
i
)

Catchphrase
Extraction
task
,
and
(
ii
)
Precedence
Retrieval
task
.

In
the
catchphrase
extraction
task
,
the
participants
had
to
extract
catchphrases
(
legal
keywords
)
from
Indian
Supreme
Court
case
documents
.

In
the
second
task
of
Precedence
Retrieval
,
the
participants
were
to
retrieve
relevant
or
cite
-
able
documents
for
particular
Indian
SupremeCourt
cases
from
a
set
of
prior
case
documents
.


-DOCSTART-

In
information
retrieval
systems
search
quality
is
directly
related
to
the
number
of
relevant
retrieved
documents
.

Database
and
data
archive
that
these
systems
are
implemented
on
can
include
variety
of
documents
with
different
sizes
.

In
this
case
,
the
chance
of
retrieving
the
longer
documents
can
be
more
than
the
shorter
ones
.

To
avoid
this
and
giving
equal
chance
to
all
documents
be
retrieved
and
increasing
the
quality
and
integrity
in
information
retrieval
,
the
length
of
documents
must
be
normalized
.

In
this
study
,
both
conventional
and
proposed
methods
for
normalization
,
Cosine
Similarity
normalization
and
Pivoted
Unique
normalization
are
used
for
normalizing
the
length
of
documents
.

Their
performance
is
tested
on
Wikipedia
MM2008
data
archive
and
compared
to
each
other
.

Finally
the
best
model
has
been
introduced
.


-DOCSTART-

Many
studies
have
demonstrated
that
people
engage
in
a
variety
of
different
information
behaviors
when
engaging
in
information
seeking
.

However
,
standard
information
retrieval
systems
such
as
Web
search
engines
continue
to
be
designed
to
support
mainly
one
such
behavior
,
specified
searching
.

This
situation
has
led
to
suggestions
that
people
would
be
better
served
by
information
retrieval
systems
which
support
different
kinds
of
information
-
seeking
strategies
.

This
article
reports
on
an
experiment
comparing
the
retrieval
effectiveness
of
an
integrated
interactive
information
retrieval
(
IIR
)
system
which
adapts
to
support
different
information
-
seeking
strategies
with
that
of
a
standard
baseline
IIR
system
.

The
experiment
,
with
32
participants
each
searching
on
eight
different
topics
,
indicates
that
using
the
integrated
IIR
system
resulted
in
significantly
better
user
satisfaction
with
search
results
,
significantly
more
effective
interaction
,
and
significantly
better
usability
than
that
using
the
baseline
system
.


-DOCSTART-

A
multiple
-
perspective
cocitation
analysis
method
is
introduced
for
characterizing
and
interpreting
the
structure
and
dynamics
of
cocitation
clusters
.

The
method
facilitates
analytic
and
sense
making
tasks
by
integrating
network
visualization
,
spectral
clustering
,
automatic
cluster
labeling
,
and
text
summarization
.

Cocitation
networks
are
decomposed
into
cocitation
clusters
.

The
interpretation
of
these
clusters
is
augmented
by
automatic
cluster
labeling
and
summarization
.

The
method
focuses
on
the
interrelations
between
a
cocitation
cluster
â€™s
members
and
their
citers
.

The
generic
method
is
applied
to
a
three
-
part
analysis
of
the
field
of
information
science
as
defined
by
12
journals
published
between
1996
and
2008
a
)
a
comparative
author
cocitation
analysis
(
ACA
b
)
a
progressive
ACA
of
a
time
series
of
cocitation
networks
,
and
(
c
)
a
progressive
document
cocitation
analysis
(
DCA
Results
show
that
the
multiple
-
perspective
method
increases
the
interpretability
and
accountability
of
both
ACA
and
DCA
networks
.


-DOCSTART-

Modern
industrial
processes
(
nuclear
,
chemical
industry
public
service
needs
(
firefighting
,
rescuing
and
research
interests
(
undersea
,
outer
space
exploration
)
have
established
a
clear
need
to
perform
work
remotely
.

Whereas
a
purely
autonomous
manipulative
capability
would
solve
the
problem
,
its
realization
is
beyond
the
state
of
the
art
in
robotics

[
Stark
et
al.,1988
Some
of
the
problems
plaguing
the
development
of
autonomous
systems
are
:
a
)
anticipation
,
detection
,
and
correction
of
the
multitude
of
possible
error
conditions
arising
during
task
execution
,
b
)
development
of
general
strategy
planning
techniques
transcending
any
particular
limited
task
domain
,
c
)
providing
the
robot
system
with
real
-
time
adaptive
behavior
to
accommodate
changes
in
the
remote
environment
,
d
)
allowing
for
on
-
line
learning
and
performance
improvement
through
"
experience
etc
.

The
classical
approach
to
tackle
some
of
these
problems
has
been
to
introduce
problem
solvers
and
expert
systems
as
part
of
the
remote
robot
workcell
control
system
.

However
,
such
systems
tend
to
be
limited
in
scope
(
to
remain
intellectually
and
implementationally
manageable
too
slow
to
be
useful
in
real
-
time
robot
task
execution
,
and
generally
fail
to
adequately
represent
and
model
the
complexities
of
the
real
world
environment
.

These
problems
become
particularly
severe
when
only
partial
information
about
the
remote
environment
is
available
.

Comments
University
of
Pennsylvania
Department
of
Computer
and
Information
Science
Technical
Report

MSCIS-90
-
35
.

This
technical
report
is
available
at
ScholarlyCommons
:

http
repository.upenn.edu/cis_reports/550
Teleprogramming
:

Overcoming
Communication
Delays

In
Remote
Manipulation
MS
-
CIS-90
-
35
GRASP
-
LAB
217


-DOCSTART-

We
present
an
approach
for
picture
indexing
and
abstraction
.

Picture
indexing
facilitates
information
retrieval
from
a
pictorial
database
consisting
of
picture
objects
and
picture
relations
.

To
construct
picture
indexes
,
abstraction
operations
to
perform
picture
object
clustering
and
classification
are
formulated
.

To
substantiate
the
abstraction
operations
,
we
also
formalize
syntactic
abstraction
rules
and
semantic
abstraction
rules
.

We
then
illustrate
by
examples
how
to
apply
these
abstraction
operations
to
obtain
various
picture
indexes
,
and
how
to
construct
icons
to
facilitate
accessing
of
pictorial
data
.


-DOCSTART-

This
paper
describes
our
participation
in
monolingual
tasks
at
CLEF-2005
.

In
this
research
we
have
worked
in
the
following
languages
:
English
,
French
,
Portuguese
,
Bulgarian
and
Hungarian
.

Our
task
has
been
focused
on
using
combined
different
size
passages
to
improve
the
Information
Retrieval
process
.

Once
we
have
studied
the
experiments
which
have
been
carried
out
and
the
official
results
at
CLEF
,
we
have
realized
that
this
combining
model
gets
better
the
achieved
scores
considerably
.


-DOCSTART-

The
paper
presents
the
innovative
use
of
cluster
based
search
for
e
cient
mathematical
information
retreival
.

The
search
is
realized
by
applying
multiple
clustering
techniques
on
the
mathematical
markup
documents
.

The
technique
makes
use
of
cluster
oriented
search
to
speed
up
math
information
retrieval
.

Impressive
results
have
been
obtained
as
compared
to
similarity
based
search
.

With
the
use
of
cluster
based
search
,
the
retrieval
time
has
been
reduced
from
multiple
seconds
to
about
1
second
.

The
quality
of
the
result
set
has
been
found
to
be
comparable
to
similarity
based
search
.


-DOCSTART-

This
report
consists
of
three
parts
.

Part
I
presents
the
Workshop
Report
and
Part
II
presents
a
proposal
for
a
National
Electronic
Library
,
the
ideas
for
which
originated
during
the
workshop
.

However
,
the
details
presented
in
this
part
were
not
actually
discussed
at
the
workshop
,
but
were
worked
out
via
post
-
workshop
meetings
and
discussions
.

Part
III
is
a
collection
of
Position
Papers
prepared
for
and
presented
at
the
workshop
.

Executive
Summary

In
recent
years
,
the
Division
of
Information
,
Robotics
and
Intelligent
systems(IRIS
)
at
the
National
Science
Foundation
(
NSF
)
has
sponsored
a
number
of
workshops
to
identify
future
directions
for
research
in
a
number
of
disciplines
within
Computer
and
Information
Sciences
.

The
focus
of
the
Chicago
workshop
was
on
information
as
opposed
to
data
,
and
on
supporting
users
'
needs
for
nding
and
organizing
information
contained
in
document
databases
.

In
particular
,
the
impact
and
future
directions
in
the
areas
of
Information
Retrieval
(
IR
Document
Analysis
,
Natural
Language
Processing
(
NLP
and
Intelligent
Document
Management
Systems
were
explored
.

In
this
report
,
a
system
that
handles
these
functions
is
,
collectively
,
referred
to
as
a
Information
Resources
and
Document
Management
System
(
IRDMS

The
NSF
was
given
a
broad
directive
in
its
enabling
act
of
1950
\to
foster
the
interchange
of
scientiic
information
among
scientists
The
OOce
of
Science
Information
Service
at
the
NSF
was
responsible
for
providing
research
funding
toward
improved
methods
and
techniques
for
handling
scientiic
information
,
including
mechanized
systems
,
through
the
Documentation
Research
Program
.

The
activities
of
the
program
was
expanded
considerably
around
1956
under
the
directorship
of
Mrs.
Helen
Brownson
.

A
number
of
conferences
were
sponsored
by
NSF
in
the
early
1960s
to
chart
a
plan
for
research
activities
in
the
eld
of
scientiic
documentation
.

In
the
1970s
,
the
signiicance
of
the
area
was
further
emphasized
at
the
NSF
by
the
establishment
of
,
for
example
,
the
Division
for
Information
Science
and
Technology
.

At
present
,
IR
and
related
areas
are
included
within
Database
and
Expert
Systems
Program
(
Director
:
Dr.
M.
Ze
-
mankova
which
comes
under
the
IRIS
Division
of
the
Directorate
for
Computer
Information
Science
Engineering
.

The
continued
support
of
the
NSF
,
industry
and
professional
societies
from
the
1960s
through
the
1980s
helped
in
the
emergence
of
a
multi
-
billion
dollar
information
industry
,
which
was
founded
on
early
research
ndings
.

More
recently
,
as
the
result
of
keen
competition
for
available
resources
as
well
as
changes
in
the
responsibility
of
the
NSF
visa
-vis
â€¦


-DOCSTART-

In
an
increasingly
digital
urban
setting
,
connected
38
;
concerned
Citizens
typically
voice
their
opinions
on
various
civic
topics
via
social
media
.

Efficient
and
scalable
analysis
of
these
citizen
voices
on
social
media
to
derive
actionable
insights
is
essential
to
the
development
of
smart
cities
.

The
very
nature
of
the
data
:
heterogeneity
and
dynamism
,
the
scarcity
of
gold
standard
annotated
corpora
,
and
the
need
for
multi
-
dimensional
analysis
across
space
,
time
and
semantics
,
makes
urban
social
media
analytics
challenging
.

This
workshop
is
dedicated
to
the
theme
of
social
media
analytics
for
smart
cities
,
with
the
aim
of
focusing
the
interest
of
CIKM
research
community
on
the
challenges
in
mining
social
media
data
for
urban
informatics
.

The
workshop
hopes
to
foster
collaboration
between
researchers
working
in
information
retrieval
,
social
media
analytics
,
linguistics
;
social
scientists
,
and
civic
authorities
,
to
develop
scalable
and
practical
systems
for
capturing
and
acting
upon
real
world
issues
of
cities
as
voiced
by
their
citizens
in
social
media
.

The
aim
of
this
workshop
is
to
encourage
researchers
to
develop
techniques
for
urban
analytics
of
social
media
data
,
with
specific
focus
on
applying
these
techniques
to
practical
urban
informatics
applications
for
smart
cities
.


-DOCSTART-

Successful
development
of
software
systems
involves
the
efficient
navigation
of
software
artifacts
.

However
,
as
artifacts
are
continuously
produced
and
modified
,
engineers
are
typically
plagued
by
challenging
information
landscapes
.

One
stateof
-
practice
approach
to
structure
information
is
to
establish
trace
links
between
artifacts
;
a
practice
that
is
also
enforced
by
several
development
standards
.

Unfortunately
,
manually
maintaining
trace
links
in
an
evolving
system
is
a
tedious
task
.

To
tackle
this
issue
,
several
researchers
have
proposed
treating
the
capture
and
recovery
of
trace
links
as
an
Information
Retrieval
(
IR
)
problem
.

The
goal
of
this
thesis
is
to
contribute
to
the
evaluation
of
IR
-
based
trace
recovery
,
both
by
presenting
new
empirical
results
and
by
suggesting
how
to
increase
the
strength
of
evidence
in
future
evaluative
studies
.

This
thesis
is
based
on
empirical
software
engineering
research
.

In
a
Systematic
Literature
Review
(
SLR
)
we
show
that
a
majority
of
previous
evaluations
of
IR
-
based
trace
recovery
have
been
technology
-
oriented
,
conducted
in
â€œ
the
cave
of
IR
evaluation
using
small
datasets
as
experimental
input
.

Also
,
software
artifacts
originating
from
student
projects
have
frequently
been
used
in
evaluations
.

We
conducted
a
survey
among
traceability
researchers
and
found
that
while
a
majority
consider
student
artifacts
to
be
only
partly
representative
of
industrial
counterparts
,
such
artifacts
were
typically
not
validated
for
industrial
representativeness
.

Our
findings
call
for
additional
case
studies
to
evaluate
IR
-
based
trace
recovery
within
the
full
complexity
of
an
industrial
setting
.

Thus
,
we
outline
future
research
on
IR
-
based
trace
recovery
in
an
industrial
study
on
safety
-
critical
impact
analysis
.

Also
,
this
thesis
contributes
to
the
body
of
empirical
evidence
of
IR
-
based
trace
recovery
in
two
experiments
with
industrial
software
artifacts
.

The
technologyoriented
experiment
highlights
the
clear
dependence
between
datasets
and
the
accuracy
of
IR
-
based
trace
recovery
,
in
line
with
findings
from
the
SLR
.

The
humanoriented
experiment
investigates
how
different
quality
levels
of
tool
output
affect
the
tracing
accuracy
of
engineers
.

While
the
results
are
not
conclusive
,
there
are
indications
that
it
is
worthwhile
further
investigating
into
the
actual
value
of
improving
tool
support
for
IR
-
based
trace
recovery
.

Finally
,
we
present
how
tools
and
methods
are
evaluated
in
the
general
field
of
IR
research
,
and
propose
a
taxonomy
of
evaluation
contexts
tailored
for
IR
-
based
trace
recovery
in
software
engineering
.


-DOCSTART-

In
ranking
with
the
pairwise
classification
approach
,
the
loss
associated
to
a
predicted
ranked
list
is
the
mean
of
the
pairwise
classification
losses
.

This
loss
is
inadequate
for
tasks
like
information
retrieval
where
we
prefer
ranked
lists
with
high
precision
on
the
top
of
the
list
.

We
propose
to
optimize
a
larger
class
of
loss
functions
for
ranking
,
based
on
an
ordered
weighted
average
(
OWA
Yager
,
1988
)
of
the
classification
losses
.

Convex
OWA
aggregation
operators
range
from
the
max
to
the
mean
depending
on
their
weights
,
and
can
be
used
to
focus
on
the
top
ranked
elements
as
they
give
more
weight
to
the
largest
losses
.

When
aggregating
hinge
losses
,
the
optimization
problem
is
similar
to
the
SVM
for
interdependent
output
spaces
.

Moreover
,
we
show
that
OWA
aggregates
of
margin
-
based
classification
losses
have
good
generalization
properties
.

Experiments
on
the
Letor
3.0
benchmark
dataset
for
information
retrieval
validate
our
approach
.


-DOCSTART-

Text
classification
is
the
process
of
automatically
assigning
predefined
categories
to
free
text
,
which
is
very
important
to
information
retrieval
and
many
other
applications
.

Of
it
,
the
first
important
thing
is
to
effectively
represent
a
text
to
characterize
it
as
belonging
to
a
specified
category
based
on
its
content
and
thus
make
the
following
phase
of
classifier
training
and
using
more
effective
and
efficient
regarding
to
the
final
classification
performance
.

In
this
paper
,
an
effective
and
efficient
new
method
called
variance
-
mean
based
feature
filtering
method
of
feature
selection
to
do
feature
reduction
in
the
representation
phase
for
text
classification
is
proposed
.

It
keeps
the
best
features
,
and
thus
improves
the
final
performance
,
e.g.
macro
-
f1
to
0.92
and
simultaneously
decreases
the
computing
time
for
representing
the
incoming
text
waiting
to
be
classified
dramatically
,
which
is
important
because
it
occurs
on
line
and
is
time
-
critical
.

The
effectiveness
and
efficiency
are
especially
obvious
when
applied
to
Chinese
language
text
.


-DOCSTART-

This
paper
proposes
a
novel
Chinese
-
English
Cross
-
Lingual
Information
Retrieval
(
CECLIR
)
model
PME
,
in
which
bilingual
dictionary
and
comparable
corpora
are
used
to
translate
the
query
terms
.

The
proximity
and
mutual
information
of
the
term
-
pairs
in
the
Chinese
and
English
comparable
corpora
are
employed
not
only
to
resolve
the
translation
ambiguities
but
also
to
perform
the
query
expansion
so
as
to
deal
with
the
out
-
of
-
vocabulary
issues
in
the
CECLIR
.

The
evaluation
results
show
that
the
query
precision
of
PME
algorithm
is
about
84.4
%
of
the
monolingual
information
retrieval
.


-DOCSTART-

The
labyrinthine
abundance
of
educational
resources
on
the
Web
has
greatly
expanded
the
challenge
of
helping
students
find
,
organize
,
and
use
resources
that
will
best
match
their
individual
goals
,
interests
,
and
current
knowledge
.

Map
-
based
navigation
,
using
technologies
such
as
the
Self
-
Organizing
Maps

(
SOM
is
one
solution
to
this
growing
challenge
.

However
,
as
the
number
of
documents
organized
by
SOM
increases
,
the
number
of
documents
within
each
cell
becomes
too
large
for
the
user
to
make
meaningful
choices
,
overwhelming
his
ability
to
make
accurate
decisions
.

Combining
interactivity
with
the
ability
to
organize
a
large
number
of
documents
,
we
have
developed
two
-
level
heterogeneous
maps
that
are
augmented
with
social
navigation
support
.

We
implemented
our
idea
within
the
Knowledge
Sea
II
system
and
ran
a
pilot
study
using
this
system
in
an
Information
Retrieval
course
.


-DOCSTART-

World
Wide
Web
is
considered
the
most
valuable
place
for
Information
Retrieval
and
Knowledge
Discovery
.

While
retrieving
information
through
user
queries
,
a
search
engine
results
in
a
large
and
unmanageable
collection
of
documents
.

A
more
efficient
way
to
organize
the
documents
can
be
a
combination
of
clustering
and
ranking
,
where
clustering
can
group
the
documents
and
ranking
can
be
applied
for
ordering
the
pages
within
each
cluster
.

This
paper
proposes
an
approach
to
co
-
clustering
web
documents
and
queries
.

When
user
issues
a
query
,
we
construct
a
Query
-
Document
Bipartite
Graph
from
click
log
data
.

Then
,
we
co
-
cluster
the
web
documents
and
queries
simultaneous
based
on
the
bipartite
spectral
graph
partitioning
which
uses
the
second
singular
vectors
of
an
appropriately
scaled
query
-
document
matrix
to
yield
good
bipartition
and
rank
the
queries
and
documents
on
the
bipartite
graph
via
an
iterative
process
like
HITS
.

The
results
of
experiments
show
promising
improvement
.


-DOCSTART-

We
developed
a
music
information
retrieval
system
based
on
singing
voice
timbre
,
i.e
a
system
that
can
search
for
songs
in
a
database
that
have
similar
vocal
timbres
.

To
achieve
this
,
we
developed
a
method
for
extracting
feature
vectors
that
represent
characteristics
of
singing
voices
and
calculating
the
vocal
-
timbre
similarity
between
two
songs
by
using
a
mutual
information
content
of
their
feature
vectors
.

We
operated
the
system
using
75
songs
and
confirmed
that
the
system
worked
appropriately
.

According
to
the
results
of
a
subjective
experiment
,
80
%
of
subjects
judged
that
compared
with
a
conventional
method
using
MFCC
,
our
method
finds
more
appropriate
songs
that
have
similar
vocal
timbres
.


-DOCSTART-

The
inverted
file
is
the
most
popular
indexing
mechanism
used
for
document
search
in
an
information
retrieval
system
(
IRS

However
,
the
disk
I
/
O
for
accessing
the
inverted
file
becomes
a
bottleneck
in
an
IRS
.

To
avoid
using
the
disk

I
/
O
,
we
propose
a
caching
mechanism
for
accessing
the
inverted
file
,
called
the
inverted
file
cache
(
IF
cache
In
this
cache
,
a
proposed
hashing
scheme
using
a
linked
list
structure
to
handle
collisions
in
the
hash
table
speeds
up
entry
indexing
.

Furthermore
,
the
replacement
and
storage
mechanisms
of
this
cache
are
designed
specifically
for
the
inverted
file
structure
.

We
experimentally
verify
our
design
,
based
on
documents
collected
from
the
TREC
(
Text
REtrieval
Conference
)
and
search
requests
generated
by
the
Zipf
-
like
distribution
.

Simulation
results
show
that
the
IF
cache
can
improve
the
performance
of
a
test
IRS
by
about
60
%
in
terms
of
the
average
searching
response
time
.


-DOCSTART-

In
recent
years
,
information
retrieval
and
information
seeking
have
moved
beyond
their
single
-
user
roots
and
are
becoming
multi
-
user
endeavors
.

However
,
there
are
multiple
visions
for
how
best
to
design
multi
-
user
interactions
:
social
search
versus
collaborative
search
.

The
terms
"
social
"
and
"
collaborative
"
are
overloaded
with
meaning
,
having
been
used
to
describe
a
wide
variety
of
systems
,
user
needs
and
goals
,
interaction
styles
,
and
algorithms
.

In
this
panel
we
adopt
the
following
primary
definitions
:
Information
seeking
tasks
in
which
there
are
two
or
more
people
who
lack
the
same
information
(
share
the
same
information
need
)
and
explicitly
set
out
together
to
satisfy
that
need
are
known
as
collaborative
.

A
collaborative
information
retrieval
system
provides
mechanisms
interfaces
and
mediation
algorithms
that
allow
the
team
to
work
together
to
find
information
that
neither
individual
would
have
found
when
working
alone
.

There
is
an
inherent
division
of
labor
in
collaborative
work
On
the
other
hand
,
information
seeking
tasks
in
which
only
a
single
individual
lacks
information
,
but
is
willing
or
able
to
let
an
larger
group
assist
in
the
satisfaction
of
that
need
,
is
known
as
social
search
.

The
larger
group
may
be
an
community
of
like
-
minded
individuals
,
or
it
might
be
a
social
network
of
friends
and
associates
.

But
either
way
,
the
assumption
is
that
someone
in
that
community
or
network
already
possesses
the
information
that
the
initial
individual
seeks
.

The
goal
of
the
system
is
therefore
to
correctly
propagate
or
diffuse
that
existing
knowledge
throughout
the
network
,
to
amplify
and
repeat
information
that
has
already
been
discovered
by
at
least
one
person
Despite
these
fundamental
differences
between
collaborative
(
team
-
oriented
,
jointly
-
held
information
need
)
and
social
(
network-
and
community
-
augmented
,
though
ultimately
solitary
need
there
are
similarities
in
process
.

This
panel
will
explore
both
these
similarities
and
differences
,
and
provide
insight
about
whether
one
type
of
multi
-
user
information
seeking
vision
will
ultimately
eclipse
the
other
,
or
whether
each
will
remain
separate
but
complementary
.


-DOCSTART-

In
visual
data
exploration
with
scatter
plots
,
no
single
plot
is
sufficient
to
analyze
complicated
high
-
dimensional
data
sets
.

Given
numerous
visualizations
created
with
different
features
or
methods
,
meta
-
visualization
is
needed
to
analyze
the
visualizations
together
.

We
solve
how
to
arrange
numerous
visualizations
onto
a
meta
-
visualization
display
,
so
that
their
similarities
and
differences
can
be
analyzed
.

We
introduce
a
machine
learning
approach
to
optimize
the
meta
-
visualization
,
based
on
an
information
retrieval
perspective
:
two
visualizations
are
similar
if
the
analyst
would
retrieve
similar
neighborhoods
between
data
samples
from
either
visualization
.

Based
on
the
approach
,
we
introduce
a
nonlinear
embedding
method
for
meta
-
visualization
:
it
optimizes
locations
of
visualizations
on
a
display
,
so
that
visualizations
giving
similar
information
about
data
are
close
to
each
other
.


-DOCSTART-

The
effectiveness
of
information
retrieval
(
IR
)
systems
is
influenced
by
the
degree
of
term
overlap
between
user
queries
and
relevant
documents
.

Query
-
document
term
mismatch
,
whether
partial
or
total
,
is
a
fact
that
must
be
dealt
with
by
IR
systems
.

Query
Expansion
(
QE
)
is
one
method
for
dealing
with
term
mismatch
.

IR
systems
implementing
query
expansion
are
typically
evaluated
by
executing
each
query
twice
,
with
and
without
query
expansion
,
and
then
comparing
the
two
result
sets
.

While
this
measures
an
overall
change
in
performance
,
it
does
not
directly
measure
the
effectiveness
of
IR
systems
in
overcoming
the
inherent
issue
of
term
mismatch
between
the
query
and
relevant
documents
,
nor
does
it
provide
any
insight
into
how
such
systems
would
behave
in
the
presence
of
query
-
document
term
mismatch
.

In
this
paper
,
we
propose
a
new
approach
for
evaluating
query
expansion
techniques
.

The
proposed
approach
is
attractive
because
it
provides
an
estimate
of
system
performance
under
varying
degrees
of
query
-
document
term
mismatch
,
it
makes
use
of
readily
available
test
collections
,
and
it
does
not
require
any
additional
relevance
judgments
or
any
form
of
manual
processing
.


-DOCSTART-

In
this
decade
the
information
retrieval
research
community
has
started
new
relevant
efforts
to
improve
previously
available
evaluation
techniques
and
methods
and
to
address
new
ones
.

Evaluation
techniques
are
critical
to
research
in
all
areas
of
science
and
engineering
,
so
they
are
for
the
information
retrieval
area
.

The
evaluation
of
information
retrieval
systems
can
take
different
approaches
,
face
different
issues
,
and
propose
or
use
different
methods
,
as
it
has
been
addressed
by
Harter
and
Hert
in
[
1
The
two
orthogonal
themes
they
propose
to
consider
are
the
Different
IR
Theoretical
Perspectives
and
the
Diversification
and
Hybridisation
of
IR
Systems

These
two
themes
are
compreh
ensive
of
many
aspects
and
efforts
that
most
of
IR
researchers
believe
are
important
for
conducting
future
IR
evaluation
efforts
.

This
framework
was
proposed
to
the
thirty
-
three
people
who
attended
the
workshop
.

Fourteen
people
were
from
United
States
,
thirteen
from
Europe
,
three
,
two
,
and
one
people
respectively
were
from
Korea
,
Japan
and
Australia
.


-DOCSTART-

In
this
paper
we
describe
our
Oromo
-
English
retrieval
experiments
that
we
have
conducted
at
IIITHyderabad
(
India
)
and
submitted
to
the
ad
hoc
retrieval
task
of
CLEF
2007
.

We
participated
in
the
bilingual
subtask
of
CLEF
campaign
for
the
second
time
by
designing
and
submitting
four
official
runs
.

The
experiments
differ
from
one
another
in
terms
of
topic
fields
used
for
query
construction
and
the
application
of
stemmer
for
normalization
of
query
terms
.

One
of
our
major
objectives
was
to
assess
the
overall
performance
of
our
dictionary
-
based
Oromo
-
English
CLIR
system
on
a
new
English
test
collection
that
has
been
provided
by
CLEF
this
year
.

We
are
also
interested
in
exploring
and
assessing
the
impacts
of
Afaan
Oromo
light
stemmer
on
the
overall
performances
of
our
experimental
CLIR
system
.

After
a
brief
description
of
the
research
contexts
of
our
Oromo
-
English
CLIR
system
,
we
will
present
and
discuss
the
evaluation
results
of
our
official
runs
.


-DOCSTART-

In
order
to
explicate
the
applicability
of
importing
Web
Service
to
digital
library
information
retrieval
system
,
this
paper
firstly
briefly
introduces
Web
Service
.

Then
the
authors
construct
an
information
retrieval
model
based
on
Web
Service
to
improve
the
current
retrieval
system
.

In
this
paper
,
we
comprehensively
present
the
model
from
overall
designing
,
process
designing
,
interface
implementation
and
others
.


-DOCSTART-

Motivated
by
the
hypothesis
that
the
retrieval
performance
of
a
weighting
model
is
independent
of
the
language
in
which
queries
and
collection
are
expressed
,
we
compared
the
retrieval
performance
of
three
weighting
models
,
i.e
Okapi
,
statistical
language
modeling
(
SLM
and
deviation
from
randomness
(
DFR
on
three
monolingual
test
collections
,
i.e
French
,
Italian
,
and
Spanish
.

The
DFR
model
was
found
to
consistently
achieve
better
results
than
both
Okapi
and
SLM
,
whose
performance
was
comparable
.

We
also
evaluated
whether
the
use
of
retrieval
feedback
improved
retrieval
performance
;
retrieval
feedback
was
beneficial
for
DFR
and
Okapi
and
detrimental
for
SLM
.

Besides
relative
performance
,
DFR
with
retrieval
feedback
achieved
excellent
absolute
results
:
best
run
for
Italian
and
Spanish
,
third
run
for
French
.


-DOCSTART-

We
have
developed
efficient
methods
to
score
structured
hypotheses
from
technologies
that
fuse
evidence
from
massive
data
streams
to
detect
threat
phenomena
.

We
have
generalized
metrics
(
precision
,
recall
,
F
-
value
,
and
area
under
the
precision
-
recall
curve
)
traditionally
used
in
the
information
retrieval
and
machine
learning
communities
to
realize
object
-
oriented
versions
that
accommodate
inexact
matching
over
structured
hypotheses
with
weighted
attributes
.

We
also
exploit
the
object
-
oriented
precision
and
recall
metrics
in
additional
metrics
that
account
for
the
costs
of
false
-
positive
and
false
-
negative
threat
reporting
.

We
have
reported
on
our
scoring
methods
more
fully
previously
;
the
present
brief
presentation
is
offered
to
help
make
this
work
accessible
to
the
machine
learning
community
.


-DOCSTART-

The
use
of
logic
in
Information
Retrieval
(
IR
)
enables
one
to
formulate
models
that
are
more
general
than
other
well
known
IR
models
.

Indeed
,
some
logical
models
are
able
to
represent
,
within
a
uniform
framework
,
various
features
of
IR
systems
,
such
as
hypermedia
links
,
multimedia
content
,
and
users
knowledge
.

Logic
also
provides
a
common
approach
to
the
integration
of
IR
systems
with
logical
database
systems
.

Finally
,
logic
makes
it
possible
to
reason
about
an
IR
model
and
its
properties
.

This
latter
possibility
is
becoming
increasingly
important
since
conventional
evaluation
methods
,
although
good
indicators
of
the
e
ectiveness
of
IR
systems
,
often
give
results
which
can
not
be
predicted
,
or
for
that
matter
satisfactorily
explained
.

However
,
logic
by
itself
can
not
fully
model
IR
.

In
determining
the
relevance
of
a
document
to
a
query
the
truth
value
or
the
validity
of
a
logical
formula
relating
the
two
is
not
enough
.

It
is
necessary
to
take
into
account
the
uncertainty
inherent
in
such
a
formulation
.

This
paper
gives
an
overview
of
how
past
and
current
research
have
combined
the
use
of
logical
and
uncertainty
theories
for
the
formulation
of
more
advanced
models
for
the
representation
and
retrieval
of
information
.


-DOCSTART-

In
urban
areas
including
shopping
malls
and
stations
with
many
people
,
it
is
important
to
utilize
various
information
which
those
people
have
obtained
.

In
this
paper
,
we
propose
a
method
for
information
registration
and
retrieval
in
MANET
which
achieves
small
communication
cost
and
short
response
time
.

In
our
method
,
we
divide
the
whole
application
field
into
multiple
sub
-
areas
and
classify
records
into
several
categories
so
that
mobile
terminals
in
an
area
holds
records
with
a
category
.

Each
area
is
associated
with
a
category
so
that
the
number
of
queries
for
the
category
becomes
the
largest
in
the
area
.

Thus
,
mobile
users
search
records
with
a
certain
category
by
sending
a
query
to
nodes
in
the
particular
area
using
existing
protocol
such
as
LBM
(
Location
-
Based
Multicast
Through
simulations
supposing
actual
urban
area
near
Osaka
station
,
we
have
confirmed
that
our
method
achieves
practical
communication
cost
and
performance
for
information
retrieval
in
MANET
.


-DOCSTART-

In
applications
including
chemoinformatics
,
bioinformatics
,
information
retrieval
,
text
classification
,
computer
vision
and
others
,
a
variety
of
common
issues
have
been
identified
involving
frequency
of
occurrence
,
variation
and
similarities
of
instances
,
and
lack
of
precise
class
labels
.

These
issues
continue
to
be
important
hurdles
in
machine
intelligence
and
my
doctoral
thesis
focuses
on
developing
robust
machine
learning
models
that
address
the
same
.


-DOCSTART-

College
of
Library
and
Information
Services
University
of
Maryland
College
Park
,
MD
20742
Organizing
Information
Organizing
information
is
at
the
heart
of
information
science
and
is
important
in
many
other
areas
as
well
.

In
bibliographic
and
similar
information
systems
it
involves
classification
as
well
as
the
description
of
documents
or
other
entities
;
in
database
management
it
is
known
as
data
modeling
;
in
artificial
intelligence
,
as
knowledge
representation
for
expert
systems
,
natural
language
understanding
,
and
other
purposes
;
in
psychology
,
as
the
structure
of
memory
and
cognition
;
in
linguistics
,
as
syntax
and
semantics
and
structure
of
discourse
;
in
technical
writing
,
as
the
structure
of
a
composition
;
in
biology
it
is
used
on
two
levels
:
in
the
classification
of
organisms
and
in
the
study
of
information
transferred
through
genes
.

In
all
scholarly
and
scientific
fields
,
organizing
information
is
important
for
establishing
frameworks
for
thought
used
in
research
and
teaching
.

It
assists
in
the
formation
of
useful
concepts
and
it
serves
to
clarify
terminology
to
assist
both
authors
and
readers
.

Many
of
these
topics
are
coming
together
in
the
emerging
discipline
of
cognitive
science
.

Finally
,
philosophy
of
knowledge
is
concerned
with
the
clarification
of
many
of
these
issues
.


-DOCSTART-

Merging
search
results
from
different
servers
is
a
major
problem
in
Distributed
Information
Retrieval
.

We
used
Regression
-
SVM
and
Ranking
-
SVM
which
would
learn
a
function
that
merges
results
based
on
information
that
is
readily
available
:
i.e.
the
ranks
,
titles
,
summaries
and
URLs
contained
in
the
results
pages
.

By
not
downloading
additional
information
,
such
as
the
full
document
,
we
decrease
bandwidth
usage
.

CORI
and
Round
Robin
merging
were
used
as
our
baselines
;
surprisingly
,
our
results
show
that
the
SVMmethods
do
not
improve
over
those
baselines
.


-DOCSTART-

The
paper
describes
feature
subset
selection
used
in
learning
on
text
data
(
text
learning
)
and
gives
a
brief
overview
of
feature
subset
selection
commonly
used
in
machine
learning
.

Several
known
and
some
new
feature
scoring
measures
appropriate
for
feature
subset
selection
on
large
text
data
are
described
and
related
to
each
other
.

Experimental
comparison
of
the
described
measures
is
given
on
real
-
world
data
collected
from
the
Web
.

Machine
learning
techniques
are
used
on
data
collected
from
Yahoo
,
a
large
text
hierarchy
of
Web
documents
.

Our
approach
includes
some
original
ideas
for
handling
large
number
of
features
,
categories
and
documents
.

The
high
number
of
features
is
reduced
by
feature
subset
selection
and
additionally
by
using
â€˜
stop
-
list
pruning
low
-
frequency
features
and
using
a
short
description
of
each
document
given
in
the
hierarchy
instead
of
using
the
document
itself
.

Documents
are
represented
as
feature
-
vectors
that
include
word
sequences
instead
of
including
only
single
words
as
commonly
used
when
learning
on
text
data
.

An
efficient
approach
to
generating
word
sequences
is
proposed
.

Based
on
the
hierarchical
structure
,
we
propose
a
way
of
dividing
the
problem
into
subproblems
,
each
representing
one
of
the
categories
included
in
the
Yahoo
hierarchy
.

In
our
learning
experiments
,
for
each
of
the
subproblems
,
naive
Bayesian
classifier
was
used
on
text
data
.

The
result
of
learning
is
a
set
of
independent
classifiers
,
each
used
to
predict
probability
that
a
new
example
is
a
member
of
the
corresponding
category
.

Experimental
evaluation
on
real
-
world
data
shows
that
the
proposed
approach
gives
good
results
.

The
best
performance
was
achieved
by
the
feature
selection
based
on
a
feature
scoring
measure
known
from
information
retrieval
called
Odds
ratio
and
using
relatively
small
number
of
features
.

Elsevier
Science
B.V.

All
rights
reserved
.


-DOCSTART-

The
new
frontier
of
mobile
information
retrieval
will
combine
context
awareness
and
content
adaptation
.


-DOCSTART-

Research
on
Web
searching
is
at
an
incipient
stage
.

This
aspect
provides
a
unique
opportunity
to
review
the
current
state
of
research
in
the
field
,
identify
common
trends
,
develop
a
methodological
framework
,
and
define
terminology
for
future
Web
searching
studies
.

In
this
article
,
the
results
from
published
studies
of
Web
searching
are
reviewed
in
order
to
present
the
current
state
of
research
.

The
analysis
of
the
limited
Web
searching
studies
available
indicates
that
research
methods
and
terminology
are
already
diverging
.

A
framework
is
proposed
for
future
studies
that
will
facilitate
comparison
of
results
.

The
advantages
of
such
a
framework
are
presented
,
and
the
implications
for
the
design
of
Web
information
retrieval
systems
studies
are
discussed
.

Additionally
,
the
searching
characteristics
of
Web
users
are
compared
and
contrasted
with
users
of
traditional
information
retrieval
and
online
public
access
systems
to
discover
if
there
is
a
need
for
more
studies
that
focus
predominantly
or
exclusively
on
Web
searching
.

The
comparison
indicates
that
Web
searching
differs
from
searching
in
other
environments
.


-DOCSTART-

Generic
object
detection
is
confronted
by
dealing
with
different
degrees
of
variations
,
caused
by
viewpoints
or
deformations
in
distinct
object
classes
,
with
tractable
computations
.

This
demands
for
descriptive
and
flexible
object
representations
which
can
be
efficiently
evaluated
in
many
locations
.

We
propose
to
model
an
object
class
with
a
cascaded
boosting
classifier
which
integrates
various
types
of
features
from
competing
local
regions
,
each
of
which
may
consist
of
a
group
of
subregions
,
named
as
regionlets
.

A
regionlet
is
a
base
feature
extraction
region
defined
proportionally
to
a
detection
window
at
an
arbitrary
resolution
(
i.e
size
and
aspect
ratio
These
regionlets
are
organized
in
small
groups
with
stable
relative
positions
to
be
descriptive
to
delineate
fine
-
grained
spatial
layouts
inside
objects
.

Their
features
are
aggregated
into
a
one
-
dimensional
feature
within
one
group
so
as
to
be
flexible
to
tolerate
deformations
.

The
most
discriminative
regionlets
for
each
object
class
are
selected
through
a
boosting
learning
procedure
.

Our
regionlet
approach
achieves
very
competitive
performance
on
popular
multi
-
class
detection
benchmark
datasets
with
a
single
method
,
without
any
context
.

It
achieves
a
detection
mean
average
precision
of
41.7
percent
on
the
PASCAL
VOC
2007
dataset
,
and
39.7
percent
on
the
VOC
2010
for
20
object
categories
.

We
further
develop
support
pixel
integral
images
to
efficiently
augment
regionlet
features
with
the
responses
learned
by
deep
convolutional
neural
networks
.

Our
regionlet
based
method
won
second
place
in
the
ImageNet
Large
Scale
Visual
Object
Recognition
Challenge
(
ILSVRC
2013
)
.


-DOCSTART-

Marsyas
,
is
an
open
source
audio
processing
framework
with
specific
emphasis
on
building
Music
Information
Retrieval
systems
.

It
has
been
been
under
development
since
1998
and
has
been
used
for
a
variety
of
projects
in
both
academia
and
industry
.

In
this
chapter
,
the
software
architecture
of
Marsyas
will
be
described
.

The
goal
is
to
highlight
design
challenges
and
solutions
that
are
relevant
to
any
MIR
software
.

Introduction
Music
has
always
been
transformed
by
advances
in
technology
.

Examples
of
technologies
that
transformed
the
way
music
was
produced
,
distributed
and
consumed
include
musical
instruments
,
music
notation
,
recording
and
more
recently
digital
music
storage
and
distribution
.

Recently
portable
digital
music
players
have
become
a
familiar
sight
and
online
music
sales
have
been
steadily
increasing
.

It
is
likely
that
in
the
near
future
anyone
will
be
able
to
access
digitally
all
of
recorded
music
in
human
history
.

In
order
to
efficiently
interact
with
the
rapidly
growing
collections
of
digitally
available
music
it
is
necessary
to
develop
tools
that
have
some
understanding
of
the
actual
musical
content
.

Music
Information
Retrieval
(
MIR
)
is
an
emerging
research
area
that
deals
with
all
aspects
of
organizing
and
extracting
information
from
music
signals
.

In
the
past
few
years
,
interest
in
Music
Information
Retrieval
(
MIR
)
has
been
steadily
increasing
.

MIR
algorithms
,
especially
when
analyzing
music
signals
in
audio
format
,
typically
utilize
state
-
of
-
the
-
art
signal
processing
and
machine
learning
algorithms
.

The
large
amounts
of
data
that
is
processed
together
with
the
huge
computational
requirements
of
audio
processing
can
stress
current
hardware
to
its
limits
.

Therefore
efficient
processing
is
critical
for
building
functional
MIR
systems
that
scale
to
large
collections
of
music
and
eventually
to
all
of
recorded
music
.

Moreover
,
MIR
is
an
inherently
interdisciplinary
field
with
practitioners
with
varying
degrees
of
computer
and
programming
expertise
(
examples
of
fields
involved
include
musicology
,
information
science
,
and
cognitive
psychology

Therefore
it
is
desirable
for
MIR
systems
to
support
multiple
hierarchical
levels
of
usage
and
extensibility
.

These
issues
make
the
design
and
development
of
MIR
systems
and
frameworks
especially
challenging
.

Music
Analysis
,
Retrieval
and
SYnthesis
for
Audio
Signals
is
an
open
source
audio
processing
framework
with
specific
emphasis
on
building
MIR
systems
.

It
has
been
under
development
since
1998
and
has
been
used
for
a
variety
of
projects
both
in
academia
and
industry
.

The
guiding
principle
behind
the
design
of
MARSYAS
has
always
been
to
provide
a
flexible
,
expressive
and
extensive
framework
without
sacrificing
computational
efficiency
.

Addressing
these
conflicting
requirements
is
the
major
challenge
facing
the
software
engineer
of
MIR
systems
.

The
main
objective
of
this
chapter
is
to
describe
the
software
architecture
of
MARSYAS
using
examples
from
specific
MIR
applications
.

We
highlight
the
design
challenges
and
corresponding
solutions
that
are
probably
relevant
to
any
MIR
software
system
.

In
many
cases
these
solutions
are
informed
by
ideas
originating
from
other
fields
of
Computer
Science
and
Software
Engineering
but
have
to
be
adapted
to
the
particular
needs
and
constraints
of
MIR
research
.

After
reviewing
related
work
and
background
information
,
MARSYAS
is
described
in
the
following
subsections
:
History
,
Requirements
,
Architecture
and
Projects
.

The
next
section
(
Specific
Topics
)
describes
in
more
detail
specific
topics
that
we
believe
are
especially
important
for
audio
processing
and
how
we
have
tried
to
address
them
in
the
framework
.

The
chapter
concludes
with
a
description
of
future
trends
in
MARSYAS
and
audio
processing
software
frameworks
in
general
.

One
of
the
major
dilemmas
facing
any
MIR
researcher
is
whether
to
use
existing
tools
or
develop
their
own
.

By
describing
the
tradeoffs
and
challenges
we
have
faced
with
the
design
of
our
system
we
hope
to
help
researchers
make
more
informed
decisions
.

Finally
an
underlying
theme
of
this
chapter
is
the
importance
of
open
source
software
for
research
and
how
it
is
different
from
other
areas
of
open
source
development
.


-DOCSTART-

Human
action
recognition
from
videos
is
a
challenging
machine
vision
task
with
multiple
important
application
domains
,
such
as
humanrobot
/
machine
interaction
,
interactive
entertainment
,
multimedia
information
retrieval
,
and
surveillance
.

In
this
paper
,
we
present
a
novel
approach
to
human
action
recognition
from
3D
skeleton
sequences
extracted
from
depth
data
.

We
use
the
covariance
matrix
for
skeleton
joint
locations
over
time
as
a
discriminative
descriptor
for
a
sequence
.

To
encode
the
relationship
between
joint
movement
and
time
,
we
deploy
multiple
covariance
matrices
over
sub
-
sequences
in
a
hierarchical
fashion
.

The
descriptor
has
a
fixed
length
that
is
independent
from
the
length
of
the
described
sequence
.

Our
experiments
show
that
using
the
covariance
descriptor
with
an
off
-
the
-
shelf
classification
algorithm
outperforms
the
state
of
the
art
in
action
recognition
on
multiple
datasets
,
captured
either
via
a
Kinect
-
type
sensor
or
a
sophisticated
motion
capture
system
.

We
also
include
an
evaluation
on
a
novel
large
dataset
using
our
own
annotation
.


-DOCSTART-

This
paper
studies
the
problem
of
end
-
to
-
end
windows
mining
directly
from
detection
output
.

Traditional
object
detection
systems
approach
this
problem
in
an
ad
-
hoc
manner
,
say
,
Non
-

Maximum
Suppression

(
NMS
Beyond
NMS
,
multi
-
class
context
modeling
has
been
explored
thoroughly
recent
years
.

But
all
these
methods
put
their
emphasis
on
eliminating
false
positive
windows
rather
than
improving
recall
.

To
address
this
problem
,
we
firstly
study
this
problem
and
propose
semantic
windows
mining
.

To
improve
recall
,
we
propose
Selective
Forward
Search
(
SFS
)
which
keeps
most
of
the
semantic
windows
while
substantially
reduces
the
number
of
false
positives
.

After
SFS
,
to
improve
precision
,
we
present
the
end
-
to
-
end
windows
mining
by
means
of
similarity
refining
optimized
for
mean
Average
Precision
(
mAP
)
and
overlap
regression
.

We
show
a
noticeable
improvement
on
the
PASCAL
VOC
datasets
in
both
recall
and
precision
.


-DOCSTART-

In
FIPA
-
style
multi
-
agent
systems
,
agents
coordinate
their
activities
by
sending
messages
representing
particular
communicative
acts
(
or
performatives
Agent
communication
languages
must
strike
a
balance
between
simplicity
and
expressiveness
by
defining
a
limited
set
of
communicative
act
types
that
fit
the
communication
needs
of
a
wide
set
of
problems
.

More
complex
requirements
for
particular
problems
must
then
be
handled
by
defining
domain
-
specific
predicates
and
actions
within
ontologies
.

This
paper
examines
the
communication
needs
of
a
multi
-
agent
distributed
information
retrieval
system
and
discusses
how
well
these
are
met
by
the
FIPA
ACL
.


-DOCSTART-

The
Knowledge
Base
Discovery
Tool
(
KBDT
)
is
a
suite
of
tools
and
components
to
improve
the
indexing
of
and
search
for
documents
.

KBDT
extracts
and
displays
content
from
documents
and
builds
knowledge
indexes
based
on
meaning
,
rather
than
keywords
.

KBDT
uses
the
indexes
to
perform
more
intelligent
searches
.

It
also
includes
visualization
technology
to
display
relevant
results
using
multi
-
media
,
rather
than
plain
text
.

This
paper
describes
prototypes
of
two
tools
in
this
suite
that
use
components
for
searching
,
extraction
,
and
display
of
requested
information
.

The
tools
are
the
Knowledge
Base
Editor
and
the
Intelligent
Information
Retrieval
Engine
.


-DOCSTART-

The
development
and
documentation
of
software
for
the
analysis
of
geographical
data
is
maturing
,
and
the
needs
and
desires
of
varying
user
communities
are
becoming
clearer
.

Certainly
today
there
are
more
users
in
more
communities
,
and
in
general
much
more
data
than
before
,
even
though
data
is
more
accessible
in
some
countries
than
in
others
.

Many
more
users
are
now
meeting
geographical
data
through
geographical
information
systems
software
(
GIS
GIS
are
general
-
purpose
environments
for
handling
geographical
data
,
and
do
not
assume
that
the
user
will
need
to
make
predictions
or
draw
inferences
from
the
data
,
or
error
propagation
in
geographical
data
analysis
.

Indeed
,
much
of
current
progress
in
GIS
is
in
making
it
easier
for
users
to
construct
maps
at
the
front
end
and
in
providing
open
and
consistent
data
base
support
at
the
back
end
.

Neither
of
these
two
areas
lie
close
to
the
central
concerns
of
statistical
data
analysts
,
such
as
making
predictions
with
associated
uncertainties
,
but
can
be
of
great
value
to
them
.

In
meeting
and
undertaking
dialogues
with
users
and
developers
,
it
seems
both
valid
and
important
to
attempt
to
explore
some
of
the
assumptions
the
different
communities
hold
themselves
,
have
about
each
other
,
and
the
tasks
they
undertake
separately
and
jointly
.

Some
of
the
points
to
be
made
will
draw
in
the
ontology
discourse
in
geographical
information
science
(
GIScience
which
may
be
helpful
in
throwing
light
of
different
assumptions
made
by
different
communities
,
not
just
technical
/
motivational
,
but
also
related
to
the
sociology
of
organizations
and
of
scientific
disciplines
.

The
paper3
discusses
these
issues
in
general
terms
,
but
more
specifically
touching
on
tools
and
methods
that
may
propagate
between
communities
of
users
,
and
on
difficulties
associated
with
the
use
of
inference
in
inappropriate
settings
.

In
particular
,
we
will
present
and
discuss
selected
examples
of
analytical
practice
that
are


-DOCSTART-

In
2001
,
JISC
research
project
INSPIRAL
investigated
non
-
technical
,
institutional
and
end
user
issues
involved
in
linking
information
resource
provision
and
elearning
in
UK
higher
education
.

Stakeholder
communities
were
identified
and
surveyed
.

Prior
research
and
developments
,
current
practice
and
thinking
,
and
future
plans
were
investigated
.

The
resulting
learner
-
centred
vision
for
the
future
also
identified
success
factors
for
and
barriers
to
an
integrated
online
learning
environment
.

Four
case
studies
offered
exemplars
of
good
practice
for
information
professionals
and
other
stakeholders
.

Overcoming
inter
-
professional
difficulties
through
high
-
level
strategy
and
collaboration
,
supported
by
learner
-
centred
evaluation
,
and
driven
by
proactive
library
participation
,
is
key
to
success
.

E
-
Learning
and
Libraries
in
UK
HE

From
the
nineties
,
through
the
turn
of
the
Millennium
,
there
were
two
key
developments
in
UK
higher
education
(
HE
the
adoption
of
online
teaching
and
learning
through
the
medium
of
virtual
learning
environments
(
VLEs
11
and
the
evolution
of
hybrid
libraries
[
17
VLEs
,
e.g.
WebCT
and
Blackboard
,
are
tools
that
support
elearning
through
integrated
provision
of
learning
materials
,
and
communication
,
administration
,
and
assessment
tools
.

VLE
developments
evolved
in
HE
teaching
and
learning
communities
,
and
within
national
funding
body
JISC
â€™s
(
Joint
Information
Systems
Committee
)

VLE
programmes
.

Meanwhile
,
most
university
libraries
had
begun
to
offer
their
catalogues
online
,
many
through
a
standard
web
-
browser
.

They
also
offered
access
to
increasing
numbers
of
electronic
journals
and
other
online
information
sources
.

In
addition
,
virtual
versions
of
library
services
,
such
as
reservations
,
registration
and
reference
,
were
starting
to
be
offered
,
particularly
to
distance
learners
[
15
These
developments
collectively
represented
the
hybrid
library
,
drawing
together
online
and
physical
collections
and
services
.

JISC
also
gave
support
in
this
area
,
through
the
eLib
Programme
and
various
infrastructure
and
collections
initiatives
.

JISC
recognised
in
2001
that
its
developments
in
these
areas
required
integration
,
and
funded
development
projects
accordingly
,
including
INSPIRAL
(

INveStigating
Portals
for
Information
Resources
And
Learning
8
12
which
aimed
to
identify
and
analyse
,
from
the
perspective
of
the
UK

HE
learner
,
the
non
-
technical
,
institutional
and
enduser
issues
with
regard
to
linking
VLEs
and
hybrid
libraries
,
and
to
make
recommendations
for
JISC
strategic
planning
and
investment
.

INSPIRAL
â€™s
Methodology
INSPIRAL
was
designed
to
provide
an
overview
of
prior
and
current
research
and
practice
,
as
well
as
key
stakeholder
communitiesâ€™
plans
,
priorities
,
and
visions
for
the
future
.

The
study
utilised
a
literature
review
,
stakeholder
interviews
,
focus
groups
,
an
online
discussion
forum
,
workshops
and
case
studies
.

It
was
supported
by
ongoing
dissemination
throughout
the
stakeholder
communities
to
enable
continuous
discussion
and
feedback
on
findings
.

When
INSPIRAL
began
,
there
had
been
little
research
or
writing
specifically
on
integrating
library
resources
and
services
with
VLEs
.

However
,
there
had
been
a
wealth
of
research
and
thinking
around
the
wider
context
,
particularly
on
elearning
related
institutional
changes
within
HE
,
and
the
needs
of
learners
.

Because
an
integrated
elearning
environment
requires
the
bringing
together
of
pedagogical
,
technological
,
and
information
expertise
,
a
multidisciplinary
range
of
literature
was
reviewed
,
mainly
in
library
and
information
science
,
educational
development
,
and
learning
technology
[
2
Stakeholders
were
identified
as
first
and
foremost
)
learners
secondly
)
educational
institutions
and
their
staff
,
including
:
academic
departments
,
libraries
,
hybrid
and
digital
library
projects
,
staff
developers
,
learning
technologists
,
educational
developers
,
distance
,
flexible
,
open
,
and
lifelong
learning
departments
,
systems
support
staff
,
researchers
and
administration
departments
;
and
(
finally
)
other
education
-
related
bodies
and
commercial
organisations
.

Twenty
individuals
from
these
stakeholder
groups
were
initially
consulted
via
detailed
,
anonymous
interviews
following
a
structured
interview
plan
[
7
Three
focus
groups
were
held
,
following
a
similar
format
,
including
one
specifically
for
learners
.

In
total
this
phase
of
the
study
involved
43
individuals
,
including
5
learners
with
elearning
experience
.

In
Proceedings
of
the
International
Conference
on
Computers
in
Education
(
ICCEâ€™02
)
0
-
7695
-
1509
-
6/02
$
17.00
2002
IEEE


-DOCSTART-

Ranking
in
information
retrieval
has
been
traditionally
approached
as
a
pursuit
of
relevant
information
,
under
the
assumption
that
the
usersâ€™
information
needs
are
unambiguously
conveyed
by
their
submitted
queries
.

Nevertheless
,
as
an
inherently
limited
representation
of
a
more
complex
information
need
,
every
query
can
arguably
be
considered
ambiguous
to
some
extent
.

In
order
to
tackle
query
ambiguity
,
search
result
diversification
approaches
have
recently
been
proposed
to
produce
rankings
aimed
to
satisfy
the
multiple
possible
information
needs
underlying
a
query
.

In
this
survey
,
we
review
the
published
literature
on
search
result
diversification
.

In
particular
,
we
discuss
the
motivations
for
diversifying
the
search
results
for
an
ambiguous
query
and
provide
a
formal
definition
of
the
search
result
diversification
problem
.

In
addition
,
we
describe
the
most
successful
approaches
in
the
literature
for
producing
and
evaluating
diversity
in
multiple
search
domains
.

Finally
,
we
also
discuss
recent
advances
as
well
as
open
research
directions
in
the
field
of
search
result
diversification
.

R.
L.
T.
Santos
,
C.
Macdonald
and
I.
Ounis
.

Search
Result
Diversification
.

Foundations
and
TrendsR
in
Information
Retrieval
,
vol
.

9
,
no
.
1
,
pp
.

1â€“90
,
2015
.

10.1561/1500000040
.


-DOCSTART-

In
October
1991
the
National
Science
Foundation
(
NSF
)
sponsored
a
workshop
to
examine
the
role
of
the
Information
Retrieval
research
community
in
the
emerging
environment
of
Internet
,
high
performance
text
processing
capabilities
and
ever
-
increasing
volumes
of
digitized
documents
.

Ed
Fox
,
Michael
Lesk
and
Michael
McGill
drafted
a
White
Paper
,
calling
for
a
National
Electronic
Science
,
Engineering
,
and
Technology
Library
.

The
term
"
Digital
Library
"
was
adopted
and
for
follow
-
up
workshops
with
the
goal
to
identify
research
directions
,
leading
to
National
Science
Foundation
(
NSF)/Defense
Advanced
Research
Projects
Agency
(
DARPA)/National
Aeronautics
and
Space
Administration
(

NASA
)
Research
in
Digital
Libraries
Initiative
announced
in
late
1993
.

Now
,
in
2016
,
25
years
after
the
first
workshop
,
15
years
after
the
Joint
Conference
on
Digital
Libraries
has
been
established
,
and
many
initiatives
and
developments
around
the
world
,
what
is
the
state
of
Digital
Libraries
?

What
items
should
be
in
digital
libraries
,
who
should
their
custodians
,
how
can
the
items
be
organized
to
support
knowledge
discovery
,
how
can
the
contents
be
safeguarded
and
preserved
?

Ebla
,
Syria
(
2500
B.C
2250
B.C
constitutes
the
oldest
organized
library
of
tables
yet
discovered
.

What
will
the
archaeologists
discover
in
year
4400
about
the
world
,
politics
,
economies
,
technologies
,
science
,
climate
,
species
,
health
,
food
,
culture
,
art
,
entertainment
and
everyday
life
through
the
ages
?

The
talk
will
examine
what
we
can
do
to
support
innovative
research
and
design
and
implementation
of
lasting
,
informative
Digital
Libraries
that
will
promote
global
goals
of
knowledge
discovery
and
international
understanding
and
personal
needs
to
organize
and
selectively
share
important
facts
,
creations
,
and
memories
.


-DOCSTART-

We
investigate
the
problem
of
describing
languages
compactly
in
different
grammatical
formalisms
for
natural
languages
.

In
particular
,
the
problem
is
studied
from
the
point
of
view
of
some
newly
developed
natural
language
formalisms
like
linear
control
grammars
(
LCGs
)
and
tree
adjoining
grammars
(
TAGs
these
formalisms
not
only
generate
non
-
context
-
free
languages
that
capture
a
wide
variety
of
syntactic
phenomena
found
in
natural
language
,
but
also
have
computationally
efficient
polynomial
time
recognition
algorithms
.

We
prove
that
the
formalisms
enjoy
the
property
of
unbounded
succinctness
over
the
family
of
context
-
grammars
,
i.e.
they
are
,
in
general
,
able
to
provide
more
compact
representations
of
natural
languages
as
compared
to
standard
context
-
free
grammars
.

Comments
University
of
Pennsylvania
Department
of
Computer
and
Information
Science
Technical
Report

MSCIS-90
-
82
.

This
technical
report
is
available
at
ScholarlyCommons
:
http
repository.upenn.edu/cis_reports/443
Descriptional
Succinctness
of
Some
Grammatical
Formalisms
for
Natural
Language
MS
-
CIS-90
-
82
LINC
LAB

187
Michael
A.
Palis
University
of
Pennsylvania
Sunil
Shende
University
of
Nebraska
Department
of
Computer
and
Information
Science
School
of
Engineering
and
Applied
Science
University
of
Pennsylvania
Philadelphia
,
PA
191
04
-
6389


-DOCSTART-

For
the
last
few
years
peer
-
to
-
peer
(
p2p
)
networks
have
become
widely
used
tools
for
sharing
any
kind
of
information
from
multimedia
data
to
text
documents
.

The
vast
amount
of
shared
information
leads
issues
on
finding
relevant
information
over
p2p
networks
.

Existing
p2p
file
search
and
information
retrieval
techniques
are
based
on
the
name
of
files
,
which
is
insufficient
when
searching
relevant
documents
.

In
this
paper
we
present
a
method
to
perform
semantic
information
retrieval
over
p2p
networks
.

Our
method
semantically
inspects
the
content
of
shared
data
in
peers
to
generate
conceptual
information
about
documents
and
general
information
about
the
peer
.


-DOCSTART-

Yu
Yang
a
c
Hongbo
Liu
a
b
Hua
Wang
c
Hongsun
Fu

d
a
School
of
Information
Science
and
Technology
,
Dalian
Maritime
University
,
Dalian
116026
,
China
b
Institute
for
Neural
Computation
,
University
of
California
,
San
Diego
,
La
Jolla
,
CA
92093
,
USA
c
Department
of
Mathematical
Sciences
,
Georgia
Southern
University
,
Statesboro
,
GA
30460
,
USA
d
Department
of
Mathematics
,
Dalian
Maritime
University
,
Dalian
116026
,
China


-DOCSTART-

Bipartite
matching
problems
characterize
many
situations
,
ranging
from
ranking
in
information
retrieval
to
correspondence
in
vision
.

Exact
inference
in
realworld
applications
of
these
problems
is
intractable
,
making
efficient
approximation
methods
essential
for
learning
and
inference
.

In
this
paper
we
propose
a
novel
sequential
matching
sampler
based
on
a
generalization
of
the
PlackettLuce
model
,
which
can
effectively
make
large
moves
in
the
space
of
matchings
.

This
allows
the
sampler
to
match
the
difficult
target
distributions
common
in
these
problems
:
highly
multimodal
distributions
with
well
separated
modes
.

We
present
experimental
results
with
bipartite
matching
problems

â€”
ranking
and
image
correspondence
â€”
which
show
that
the
sequential
matching
sampler
efficiently
approximates
the
target
distribution
,
significantly
outperforming
other
sampling
approaches
.


-DOCSTART-

As
Internet
resources
become
accessible
to
more
and
more
countries
,
there
is
a
need
to
develop
efficient
methods
for
information
retrieval
across
languages
.

In
the
present
paper
,
we
focus
on
query
expansion
techniques
to
improve
the
effectiveness
of
an
information
retrieval
.

A
combination
to
a
dictionary
-
based
translation
and
statistical
-
based
disambiguation
is
indispensable
to
overcome
translation
â€™s
ambiguity
.

We
propose
a
model
using
multiple
sources
for
query
reformulation
and
expansion
to
select
expansion
terms
and
retrieve
information
needed
by
a
user
.

Relevance
feedback
,
thesaurus
-
based
expansion
,
as
well
as
a
new
feedback
strategy
,
based
on
the
extraction
of
domain
keywords
to
expand
user
â€™s
query
,
are
introduced
and
evaluated
.

We
evaluated
the
effectiveness
of
the
proposed
combined
method
,
by
an
application
to
a
FrenchEnglish
Information
Retrieval
.


-DOCSTART-

Quantum
information
science
is
a
source
of
task
-
related
axioms
whose
consequences
can
be
explored
in
general
settings
encompassing
quantum
mechanics
,
classical
theory
,
and
more
.

Quantum
states
are
compendia
of
probabilities
for
the
outcomes
of
possible
operations
we
may
perform
on
a
system
operational
states
I
discuss
general
frameworks
for
operational
theories
sets
of
possible
operational
states
of
a
system
in
which
convexity
plays
key
role
.

The
main
technical
content
of
the
paper
is
in
a
theorem
that
any
such
theory
naturally
gives
rise
to
a
weak
effect
algebra
when
outcomes
having
the
same
probability
in
all
states
are
identified
and
in
the
introduction
of
a
notion
of
operation
algebra
that
also
takes
account
of
sequential
and
conditional
operations
.

Such
frameworks
are
appropriate
for
investigating
what
things
look
like
from
an
inside
view
i.e
for
describing
perspectival
information
that
one
subsystem
of
the
world
can
have
about
another
.

Understanding
how
such
views
can
combine
,
and
whether
an
overall
geometric
picture
outside
view
coordinating
them
all
can
be
had
,
even
if
this
picture
is
very
different
in
structure
from
the
perspectives
within
it
,
is
the
key
to
whether
we
may
be
able
to
achieve
a
unified
objective
physical
view
in
which
quantum
mechanics
is
the
appropriate
description
for
certain
perspectives
,
or
whether
quantum
mechanics
is
truly
telling
us
we
must
go
beyond
this
geometric
conception
of


-DOCSTART-

Participation
from
music
librarians
has
been
sparse
in
the
first
six
ISMIR
conferences
,
despite
many
potential
areas
of
common
interest
.

This
paper
makes
an
argument
for
the
benefit
to
both
the
library
and
Music
IR
communities
of
increased
representation
of
librarians
at
ISMIR
.

An
analysis
of
conference
programs
and
primary
publications
of
two
music
library
organizations
to
determine
topics
from
the
library
literature
relevant
to
Music
IR
research
is
presented
.

A
discussion
follows
of
expertise
music
librarians
could
potentially
contribute
to
Music
IR
research
and
the
ways
in
which
Music
IR
research
could
further
the
work
of
music
librarians
,
in
each
of
the
topics
represented
in
the
library
literature
.


-DOCSTART-

In
this
paper
we
address
the
combination
of
query
translation
approaches
for
cross
-
language
information
retrieval
(
CLIR

We
translate
queries
with
Google
Translate
and
extend
them
with
new
translations
obtained
by
mapping
noun
phrases
in
the
query
to
concepts
in
the
target
language
using
Wikipedia
.

For
two
CLIR
collections
,
we
show
that
the
proposed
model
provides
meaningful
translations
that
improve
the
strong
baseline
CLIR
model
based
on
a
top
performing
SMT
system
.


-DOCSTART-

The
digital
image
over
the
network
is
inevitably
affected
by
the
channel
additive
noise
.

However
the
existing
fragile
watermarking
techniques
with
recovery
are
susceptible
to
random
noise
.

To
overcome
this
problem
,
this
paper
presents
a
chaos
-
based
fragile
watermarking
scheme
with
recovery
.

In
the
proposed
algorithm
,
the
original
image
is
divided
into
2times2
blocks
.

The
watermark
embedding
position
of
every
image
block
is
randomly
generated
based
on
chaotic
system
.

These
strategies
can
effectively
improve
the
ability
of
our
algorithm
against
not
only
random
noise
but
also
the
synchronous
counterfeiting
attack
.

The
experiment
demonstrates
that
the
proposed
scheme
can
detect
and
localize
any
malicious
alterations
and
can
recover
a
tampered
image
to
an
intelligible
one
even
if
the
tested
image
is
polluted
by
the
random
noise
.


-DOCSTART-

A
large
amount
of
information
in
the
form
of
text
,
audio
,
video
and
other
documents
is
available
on
the
web
.

Users
should
be
able
to
find
relevant
information
in
these
documents
.

Information
Retrieval
(
IR
)
refers
to
the
task
of
searching
relevant
documents
and
information
from
the
contents
of
a
data
set
such
as
the
World
Wide
Web
(

WWW
A
web
search
engine
is
an
IR
system
that
is
designed
to
search
for
information
on
the
World
Wide
Web
.

There
are
various
components
involved
in
information
retrieval
.

IR
system
has
following
components
:


-DOCSTART-

With
advances
in
the
field
of
digitization
of
printed
documents
and
several
mass
digitization
projects
underway
,
information
retrieval
and
document
search
have
emerged
as
key
research
areas
.

However
,
most
of
the
current
work
in
these
areas
is
limited
to
English
and
a
few
oriental
languages
.

The
lack
of
efficient
solutions
for
Indic
scripts
and
languages
such
as
Sanskrit
has
hampered
information
extraction
from
a
large
body
of
documents
of
cultural
and
historical
importance
.

This
chapter
presents
two
relevant
topics
in
this
area
.

First
,
we
describe
the
use
of
a
script
specific
Keyword
Spotting
for
Sanskrit
documents
that
makes
use
of
domain
knowledge
of
the
script
.

Second
,
we
address
the
needs
of
a
digital
library
to
provide
access
to
a
collection
of
documents
from
multiple
scripts
.

This
requires
intelligent
solutions
which
scale
across
different
scripts
.

We
present
a
script
independent
Keyword
Spotting
approach
for
this
purpose
.

Experimental
results
illustrate
the
efficacy
of
our
methods
.


-DOCSTART-

The
Information
Retrieval
in
the
World
Wide
Web
(
Web
IR
)
plays
an
important
role
in
the
use
of
the
Internet
.

An
information
retrieval
process
begins
when
a
user
enters
a
formal
statements
of
information
needs
,
known
as
query
,
into
the
system
and
ends
when
results
are
given
.

Results
consist
of
several
objects
that
match
the
query
with
different
degrees
of
relevancy
.

The
relevancy
of
the
results
is
still
an
important
concern
and
is
often
associated
with
the
volume
of
the
results
:
getting
better
relevancy
involves
bigger
volume
of
returned
information
while
lesser
volume
involves
lower
relevancy
.

This
issue
makes
the
Web
IR
an
active
domain
of
research
and
development
.

Since
a
decade
,
an
interest
was
given
to
software
agents
technology
and
its
applications
.

Agents
have
been
successfully
used
in
many
fields
:
agents
-
based
market
systems
prove
to
be
efficient
for
implementing
e
-
commerce
or
B2B
applications
on
the
Internet
,
thanks
to
inherent
properties
such
as
prominency
of
interactions
,
collaboration
between
agents
,
autonomy
,
scalability
,
flexibility
,
interoperability
,
etc
.

Although
the
use
of
agents
in
other
application
domains
is
not
yet
widespread
,
the
integration
of
agents
into
market
mechanisms
bring
clear
and
efficient
solutions
to
Quality
of
Service
(
QoS
)
issues
encountered
in
most
distributed
applications
and
notably
in
Web
IR
systems
.

Mobility
allows
defining
a
new
interaction
model
,
where
agents
act
on
behalf
of
final
users
or
devices
providing
resources
,
while
Market
Places
provide
an
organizational
setting
for
the
matching
of
demands
and
offers
.

This
paper
,
a
renewed
form
of
[
1
will
show
how
market
mechanisms
and
mobility
can
ease
collaboration
between
agents
to
deliver
a
service
with
relevant
results
in
Web
IR
.


-DOCSTART-

Title
of
dissertation
:
PHOTON
PAIR
PRODUCTION
FROM
A
HOT
ATOMIC
ENSEMBLE
IN
THE
DIAMOND
CONFIGURATION
Richard
Thomas
Willis
,
Doctor
of
Philosophy
,

2009
Dissertation
directed
by
:
Professor
Steven
Rolston
Department
of
Physics

This
thesis
discusses
four
-
wave
mixing
(
4WM
)
in
a
warm
ensemble
of
rubidium
using
the
diamond
configuration
level
structure
.

Both
classical
4WM
and
nonclassical
photon
-
pair
production
are
investigated
.

Quantum
information
science
has
spawned
a
great
amount
of
experimental
work
on
the
interaction
of
light
with
collective
modes
of
excitation
in
atomic
ensembles
.

Plans
to
build
quantum
networks
and
quantum
repeaters
with
atom
ensembles
take
advantage
of
nonlinear
interactions
to
produce
and
store
non
-
classical
states
of
light
.

These
technologies
will
require
photon
sources
that
not
only
generate
nonclassical
light
,
but
also
resonant
,
narrow
band
light
.

Here
we
investigate
a
system
which
could
be
used
as
such
a
source
.

We
take
advantage
of
the
4WM
interaction
in
a
warm
ensemble
of
Rubidium
atoms
.

Our
scheme
utilizes
the
diamond
energy
level
configuration
which
,
in
rubidium
,
allows
for
correlated
pairs
at
telecommunications
wavelengths
.

We
start
by
examining
the
properties
of
classical
4WM
in
the
system
.

We
measure
the
resonance
structure
and
see
that
it
can
be
understood
in
terms
of
velocity
class
selective
resonant
enhancement
and
power
splitting
effects
.

The
efficiency
of
the
process
is
low
and
limited
by
linear
absorption
of
the
pumps
.

Our
observations
agree
with
a
semi
-
classical
Maxwell
-
Bloch
theoretical
treatment
.

Next
we
observe
pair
generation
by
spontaneous
4WM
from
the
warm
ensemble
.

The
temporal
profile
of
the
cross
-
correlation
function
(
CCF
)
for
the
photons
depends
on
pump
-
laser
power
and
detuning
.

This
allows
us
to
produce
biphotons
with
controllable
spectra
.

A
simple
quantum
optical
theoretical
treatment
based
on
linear
filtering
gives
qualitative
agreement
with
the
data
.

We
show
that
the
photon
pairs
are
polarization
entangled
,
clearly
violating
Bell
â€™s
Inequality
.

A
perturbative
quantum
optical
treatment
predicts
the
polarization
state
of
the
pairs
and
agrees
with
our
measurements
.

We
analyze
the
photon
statistics
of
the
source
and
find
the
largest
violation
of
the
two
beam
Cauchy
-
Schwarz
inequality
from
a
warm
atomic
source
yet
.

We
cast
the
system
as
a
heralded
single
photon
source
at
telecommunications
wavelengths
and
see
that
it
is
competitive
with
other
systems
in
terms
of
spectral
brightness
.

PHOTON
PAIR
PRODUCTION
FROM
A
HOT
ATOMIC
ENSEMBLE
IN
THE
DIAMOND
CONFIGURATION
by
Richard
Thomas
Willis
Dissertation
submitted
to
the
Faculty
of
the
Graduate
School
of
the
University
of
Maryland
,
College
Park
in
partial
fulfillment
of
the
requirements
for
the
degree
of
Doctor
of
Philosophy
2009
Advisory
Committee
:
Professor
Steven
Rolston
,
Chair
/
Advisor
Professor
Luis
Orozco
Dr.
Charles
Clark
Professor
Mario
Dagenais
Dr.
Paul
Lett
c
Copyright
by
Richard
Thomas
Willis
2009


-DOCSTART-

Visual
information
retrieval
(
VIR
)
is
an
active
and
vibrant
research
area
,
which
attempts
at
providing
means
for
organizing
,
indexing
,
annotating
,
and
retrieving
visual
information
(
images
and
videos
)
form
large
,
unstructured
repositories
.

The
goal
of
VIR
is
to
retrieve
the
highest
number
of
relevant
matches
to
a
given
query
(
often
expressed
as
an
example
image
and/or
a
series
of
keywords
In
its
early
years
(
1995
-
2000
)
the
research
efforts
were
dominated
by
content
-
based
approaches
contributed
primarily
by
the
image
and
video
processing
community
.

During
the
past
decade
,
it
was
widely
recognized
that
the
challenges
imposed
by
the
semantic
gap
(
the
lack
of
coincidence
between
an
image
's
visual
contents
and
its
semantic
interpretation
)
required
a
clever
use
of
textual
metadata
(
in
addition
to
information
extracted
from
the
image
's
pixel
contents
)
to
make
image
and
video
retrieval
solutions
efficient
and
effective
.

The
need
to
bridge
(
or
at
least
narrow
)
the
semantic
gap
has
been
one
of
the
driving
forces
behind
current
VIR
research
.

Additionally
,
other
related
research
problems
and
market
opportunities
have
started
to
emerge
,
offering
a
broad
range
of
exciting
problems
for
computer
scientists
and
engineers
to
work
on
.

In
this
tutorial
,
we
present
an
overview
of
visual
information
retrieval
(
VIR
)
concepts
,
techniques
,
algorithms
,
and
applications
.

Several
topics
are
supported
by
examples
written
in
Java
,
using
Lucene
(
an
open
-
source
Java
-
based
indexing
and
search
implementation
)
and
LIRE
(
Lucene
Image
REtrieval
an
open
-
source
Java
-
based
library
for
content
-
based
image
retrieval
(
CBIR
)
written
by
Mathias
Lux

After
motivating
the
topic
,
we
briefly
review
the
fundamentals
of
information
retrieval
,
present
the
most
relevant
and
effective
visual
descriptors
currently
used
in
VIR
,
the
most
common
indexing
approaches
for
visual
descriptors
,
the
most
prominent
machine
learning
techniques
used
in
connection
with
contemporary
VIR
solutions
,
as
well
as
the
challenges
associated
with
building
real
-
world
,
large
scale
VIR
solutions
,
including
a
brief
overview
of
publicly
available
datasets
used
in
worldwide
challenges
,
contests
,
and
benchmarks
.

Throughout
the
tutorial
,
we
integrate
examples
using
LIRE
,
whose
main
features
and
design
principles
are
also
discussed
.

Finally
,
we
conclude
the
tutorial
with
suggestions
for
deepening
the
knowledge
in
the
topic
,
including
a
brief
discussion
of
the
most
relevant
advances
,
open
challenges
,
and
promising
opportunities
in
VIR
and
related
areas

The
tutorial
is
primarily
targeted
at
experienced
Information
Retrieval
researchers
and
practitioners
interested
in
extending
their
knowledge
of
document
-
based
IR
to
equivalent
concepts
,
techniques
,
and
challenges
in
VIR
.

The
acquired
knowledge
should
allow
participants
to
derive
insightful
conclusions
and
promising
avenues
for
further
investigation
.


-DOCSTART-

This
paper
reports
our
experimental
investigation
into
the
use
of
a
reinforcement
learning
strategy
to
learn
weights
of
(
formal
)
concepts
and
keywords
to
support
Information
Retrieval
.

This
work
is
a
part
of
our
main
research
objective
of
using
more
elegant
construct
of
a
concept
rather
than
simple
keywords
as
the
basic
element
of
representation
and
matching
.

The
framework
used
for
achieving
this
was
based
on
the
theory
of
Formal
Concept
Analysis
(
FCA
)
and
Lattice
theory
.

Features
or
concepts
(
formulated
according
to
FCA
)
of
each
document
(
and
query
)
are
represented
in
a
separate
concept
lattice
and
are
weighted
separately
with
respect
to
the
document
.

The
document
retrieval
process
is
viewed
as
a
continuous
conversation
between
queries
and
documents
,
during
which
documents
are
allowed
to
learn
a
consistent
set
of
significant
concepts
to
help
their
retrieval
.

The
learning
strategy
used
was
based
on
relevance
feedback
information
that
makes
the
similarity
of
relevant
documents
stronger
and
nonrelevant
documents
weaker
.

Test
results
obtained
on
the
Cranfield
collection
show
a
significant
increase
of
average
precisions
as
the
system
gains
more
experience
.


-DOCSTART-

Multilingual
Digital
Libraries
ask
for
Cross
-
Language
search
engines
able
to
disambiguate
the
search
terms
across
languages
.

In
this
paper
,
we
report
on
the
development
and
first
evaluation
of
a
Cross
-
Language
Information
Retrieval
(
CLIR
)
system
enriched
with
a
Word
To
Category
Module
:
we
automatically
extract
a
mapping
from
words
in
the
meta
data
of
the
Catalog
â€™s
records
to
the
associated
Classification
Categories
and
exploit
it
to
help
the
CLIR
system
retrieve
only
books
in
different
languages
about
the
topic
actually
queried
by
the
user
.


-DOCSTART-

In
this
work
,
we
investigate
utilizing
the
structure
of
a
website
to
increase
the
effectiveness
of
document
retrieval
within
a
structured
domain
.

In
particular
we
examine
various
methods
to
combine
evidence
within
the
website
in
order
to
improve
the
quality
of
pages
returned
.


-DOCSTART-

This
paper
introduces
a
novel
evaluation
framework
for
question
series
and
employs
it
to
explore
the
effectiveness
of
QA
and
IR
systems
at
addressing
usersâ€™
information
needs
.

The
framework
is
based
on
the
notion
of
recall
curves
,
which
characterize
the
amount
of
relevant
information
contained
within
a
fixed
-
length
text
segment
.

Although
it
is
widely
assumed
that
QA
technology
provides
more
efficient
access
to
information
than
IR
systems
,
our
experiments
show
that
a
simple
IR
baseline
is
quite
competitive
.

These
results
help
us
better
understand
the
role
of
NLP
technology
in
QA
systems
and
suggest
directions
for
future
research
.


-DOCSTART-

There
is
a
significant
amount
of
interest
in
combining
and
extending
database
and
information
retrieval
technologies
to
manage
textual
data
.

The
challenge
is
becoming
more
relevant
due
to
the
increased
availability
of
documents
in
digital
form
.

Document
data
has
a
natural
hierarchical
structure
,
which
may
be
made
explicit
due
to
the
use
of
markup
conventions
(
as
it
is
the
case
with
SGML
An
important
aspect
of
managing
structured
and
semi
-
structured
textual
data
consists
of
supporting
the
efficient
retrieval
of
text
components
based
both
on
their
content
and
structure
.

In
this
paper
we
study
issues
related
to
the
expressive
power
and
optimization
of
a
class
of
algebras
that
support
combining
string
(
or
pattern
)
searches
with
queries
on
the
hierarchical
structure
of
the
text
.

The
region
algebra
studied
is
a
set
-
at
-
a
-
time
algebra
for
manipulating
-tezt
regions
(
substrings
of
the
text
)
that
supports
finding
out
nesting
and
ordering
properties
of
the
text
regions
.

The
region
algebra
is
part
of
the
language
in
use
in
commercial
text
retrieval
systems
,
and
can
be
implemented
very
efficiently
.

The
results
in
this
work
are
obtained
by
showing
a
close
relationship
between
the
region
algebra
and
the
monadic
first
order
theory
of
binary
trees
.

We
show
that
queries
in
the
algebra
can
be
optimized
,
but
the
optimization
can
be
difficult
(
Co
-
NP
-
Hard
in
the
general
case
,
although
there
is
an
important
class
of
queries
that
can
be
optimized
in
polynomial
time

On
the
negative
side
,
we
show
that
the
language
is
incapable
of
capturing
some
important
properties
of
the
text
structure
,
related
to
the
nesting
and
ordering
of
text
regions
.

We
conclude
by
suggesting
possible
extensions
*
Research
done
while
the
author
was
at
the
University
of
Toronto
.

Permission
to
copy
without
fee
all
or
part
of
this
material
is
granted
provided
that
the
copies
are
not
made
or
distributed
for
direct
commercial
advantage
,
the
ACM
copyright
notice
and
the
title
of
the
publication
and
its
date
appear
,
and
notice
ISgiven
that
copying
is
by
permission
of
the
Association
of
Computing
Machinery
.

To
copy
otherwise
,
or
to
republish
,
requires
a
fee
and/or
specific
permission
,
PODS

â€™95
San
Jose
CA
USA
(
3
1995
ACM
0
-
89791
-730
-
8/95/0005
3.50
Tova
Mile
*
Department
of
Computer
Science


-DOCSTART-

Trapped
atomic
ions
are
a
well
-
advanced
physical
system
for
investigating
fundamental
questions
of
quantum
physics
and
for
quantum
information
science
and
its
applications
.

When
contemplating
the
scalability
of
trapped
ions
for
quantum
information
science
one
notes
that
the
use
of
laser
light
for
coherent
operations
gives
rise
to
technical
and
also
physical
issues
that
can
be
remedied
by
replacing
laser
light
by
long
wavelength
radiation
in
the
microwave
(
MW
)
and
radio
-
frequency
(
RF
)
regime
radiation
employing
suitably
modified
ion
traps
.


-DOCSTART-

In
Information
Retrieval
System
(
IRS
the
Automatic
Relevance
Feedback
(
ARF
)
is
a
query
reformulation
technique
that
modifies
the
initial
one
without
the
user
intervention
.

It
is
applied
mainly
through
the
addition
of
terms
coming
from
the
external
resources
such
as
the
ontologies
and
or
the
results
of
the
current
research
.

In
this
context
we
are
mainly
interested
in
the
local
analysis
technique
for
the
ARF
in
ad
-
hoc
IRS
on
Arabic
documents
.

In
this
article
,
we
have
examined
the
impact
of
the
variation
of
the
two
parameters
implied
in
this
technique
,
that
is
to
say
,
the
number
of
the
documents
Â«
D
Â»
and
the
number
of
terms
Â«
T
on
an
Arabic
IRS
performance
.

The
experimentation
,
carried
out
on
an
Arabic
corpus
text
,
enables
us
to
deduce
that
there
are
queries
which
are
not
easily
improvable
with
the
query
reformulation
.

In
addition
,
the
success
of
the
ARF
is
due
mainly
to
the
selection
of
a
sufficient
number
of
documents
D
and
to
the
extraction
of
a
very
reduced
set
of
relevant
terms
T
for
retrieval
.


-DOCSTART-

Cognitive
regions
are
regions
in
the
mind
,
reflecting
informal
ways
individuals
and
cultural
groups
organize
their
understanding
of
earth
landscapes
.

Cognitive
region
boundaries
are
typically
substantially
vague
and
their
membership
functions
are
substantially
variable
the
transition
from
outside
to
inside
the
region
is
imprecise
or
vague
,
and
different
places
within
the
region
are
not
equally
strong
or
clear
as
exemplars
of
the
region
.

Methods
for
assessing
and
cartographically
depicting
cognitive
regions
,
as
with
other
vague
geographic
regions
,
have
traditionally
implied
an
inappropriate
level
of
boundary
sharpness
and
membership
uniformity
,
such
as
when
boundaries
are
mapped
as
precise
lines
.

Research
in
recent
decades
has
explored
methods
for
assessing
and
depicting
boundary
vagueness
and
membership
variability
,
either
within
or
across
individuals
,
but
has
still
assumed
homogeneity
and
regularity
in
the
vagueness
and
variability
.

In
this
article
,
we
present
two
studies
that
assess
the
cognitive
regions
of
â€˜
Northernâ€™
and
â€˜
Southernâ€™
California
,
and
,
for
comparison
Northernâ€™
and
â€˜
Southernâ€™
Alberta
.

The
first
study
uses
a
standard
boundary
-
drawing
task
;
the
second
uses
a
novel
task
in
which
participants
rate
cells
of
a
high
-
resolution
grid
laid
over
an
outline
map
.

This
technique
allows
us
to
assess
and
depict
vagueness
and
nonuniformity
that
is
heterogeneous
and
irregular
across
different
areas
.

Differences
in
the
conceptualization
of
â€˜
Northernâ€™
and
â€˜
Southernâ€™
regions
in
California
,
as
compared
to
those
in
Alberta
,
point
to
thematic
influences
on
cognitive
regions
in
California
but
not
in
Alberta
.

As
is
often
true
with
cognitive
regions
,
Northern
and
Southern
California
are
about
attitude
,
not
just
latitude
.


-DOCSTART-

University
at
Buffalo
(
UB
)
participated
in
TREC-12
in
Genomics
and
High
Accuracy
Retrieval
from
Documents
(
HARD
)
tracks
.

We
explored
some
techniques
that
combine
Information
Retrieval
and
Information
Extraction
to
perform
the
TREC
tasks
.

We
used
an
Information
Extraction
engine
InfoXtract
[
3
]
from
Cymfony
Inc.
to
enhance
retrieval
results
.

For
the
Genomics
primary
task
,
documents
retrieved
using
a
vector
space
model
with
relevance
feedback
are
reweighted
based
on
the
biomedical
named
entities
discovered
by
InfoXtract
.

For
the
secondary
task
,
extracted
information
along
with
cue
words
for
text
snippets
that
describe
functionality
is
used
for
generating
GeneRIFs
for
given
Gene
name
and
PubMed
abstract
.

A
language
modeling
approach
that
incorporates
keyword
and
non
-
keyword
features
are
used
for
the
HARD
task
.

Features
extracted
by
InfoXtract
from
the
HARD
corpus
are
used
to
rank
documents
and/or
passages
as
answers
to
the
HARD
queries
.

Cymfony
â€™s
InfoXtract
[
3
]
is
a
customizable
Information
Extraction
engine
that
performs
syntactic
and
semantic
parsing
of
a
document
to
identify
features
like
named
entities
,
relationships
and
events
in
them
.

The
baseline
InfoXtract
engine
has
been
trained
for
the
general
English
and
news
domain
,
It
can
be
customized
to
recognize
new
named
entities
like
Gene
Names
and
Gene
Function
.

Biomedical
Customization
of
InfoXtract
is
briefly
presented
in
Section
2.2
.


-DOCSTART-

While
similarity
has
gained
in
importance
in
research
about
information
retrieval
on
the
(
geospatial
)
semantic
Web
,
information
retrieval
paradigms
and
their
integration
into
existing
spatial
data
infrastructures
have
not
been
examined
in
detail
so
far
.

In
this
paper
,
intensional
and
extensional
paradigms
for
similarity
-
based
information
retrieval
are
introduced
.

The
differences
between
these
paradigms
with
respect
to
the
query
and
results
are
pointed
out
.

Web
user
interfaces
implementing
two
of
these
paradigms
are
presented
,
and
steps
towards
the
integration
of
the
SIM
-
DL
similarity
theory
into
a
spatial
data
infrastructure
are
discussed
.

Remaining
difficulties
are
highlighted
and
directions
of
further
work
are
given
.


-DOCSTART-

Increasing
the
growth
rates
of
websitesâ€™
number
has
led
to
the
challenge
of
assisting
Web
customers
in
finding
appropriate
details
from
the
Internet
using
an
intelligent
search
engine
.

Information
retrieval
(
IR
)
is
an
essential
and
useful
strategy
for
Web
users
;
thus
,
different
strategies
and
techniques
are
designed
for
such
purpose
.

Currently
,
the
focus
on
the
usefulness
of
Artificial
Intelligence
(
AI
)
has
been
improved
with
IR
.

One
AI
area
is
Evolutionary
Computation
(
EC
which
is
based
on
designs
of
natural
selection
.

A
traditional
and
important
strategy
in
EC
is
Genetic
Algorithm
(
GA
this
paper
adopts
the
GA
technique
to
enhance
the
retrieval
of
HTML
documents
.

This
improvement
is
obtained
by
creating
a
modern
evaluation
function
and
applying
a
hybrid
crossover
operator
.

The
proposed
evaluation
function
is
based
on
term
proximity
,
keyword
probability
within
the
document
,
and
HTML
tag
weight
query
.

Experimental
results
are
compared
with
two
well
known
evaluation
function
functions
applied
in
IR
domain
which
are
Okapi
-
BM25
and
Bayesian
interface
network
model
.

The
results
demonstrate
a
good
level
of
enhancement
to
the
recall
and
precision
.

In
addition
,
the
documents
retrieved
by
the
proposed
system
were
more
accurate
and
relevant
to
the
queries
than
that
retrieved
by
other
models
.

10.4018/jalr.2012040101
2
International
Journal
of
Artificial
Life
Research
,
3(2
1
-
14
,
April
-
June
2012

Copyright
2012
,
IGI
Global
.

Copying
or
distributing
in
print
or
electronic
forms
without
written
permission
of
IGI
Global
is
prohibited
.

This
in
turn
led
to
encourage
researchers
for
using
this
algorithm
in
IR
.

Besides
,
GA
plays
an
important
approach
to
provide
suitable
information
for
the
user
â€™s
needs
.

IR
and
GA
integrated
to
avoid
web
users
suffering
from
specific
problems
when
trying
to
retrieve
useful
information
such
that
Many
of
the
retrieved
documents
are
not
related
to
the
user
query
Some
of
relevant
documents
have
not
been
retrieved
yet
(
Picarougne
et
al
2002

However
,
retrieving
relevant
information
is
not
a
simple
process
.

However
,
the
complexity
of
this
process
is
further
increased
by
the
fact
that
more
and
more
of
this
information
appears
in
natural
language
and
not
in
structured
formats
(
Liu
,
2006

The
rest
of
this
paper
is
organized
as
follows
:
in
section
2
,
we
describe
problem
statement
.

Section
3
describes
the
main
objectives
of
this
research
.

Section
4
discusses
related
works
.

Document
representation
is
introduced
in
section
5
.

Section
6
introduces
the
proposed
approach
.

Section
7
represents
the
experimental
results
of
the
proposed
method
.

Section
8
gives
the
conclusions
of
this
study
.

PROBLEM
STATEMENT
AND
OBJECTIVES

Every
internet
user
wishes
to
have
satisfied
results
when
using
any
web
search
engine
.

Satisfaction
in
sense
that
all
the
retrieved
results
are
relevant
,
and
all
relevant
documents
are
retrieved
;
in
another
words
,
the
web
user
is
satisfied
when
the
search
engine
retrieves
all
and
only
the
relevant
documents
.

In
spite
of
several
enhancements
are
achieved
in
such
searching
techniques
,
still
web
users
suffer
from
two
major
problems
when
trying
to
retrieve
useful
information
.

The
objective
of
this
paper
is
integrate
IR
and
GA
to
avoid
web
users
suffering
from
such
problems
when
trying
to
retrieve
useful
information
such
that
:
Many
of
the
retrieved
documents
are
not
related
to
the
user
query
which
is
called
low
precision
,
and
many
of
relevant
documents
have
not
been
retrieved
yet
which
is
called
low
recall
.

In
order
to
enhance
precision
and
recall
,
this
paper
proposes
a
novel
information
retrieval
GA
based
searching
mechanism
.

In
addition
this
paper
explains
how
the
new
developed
crossover
operator
can
affect
the
efficiency
of
the
proposed
GA
system
.

Another
objective
presented
in
this
paper
is
to
develop
a
new
evaluation
function
.

This
new
function
will
be
used
to
evaluate
documents
and
retrieve
the
related
documents
at
high
rank
.


-DOCSTART-

This
document
describes
work
undertaken
as
part
of
a
programme
of
study
at
the
Faculty
of
Geo
-
Information
Science
and
Earth
Observation
of
the
University
of
Twente
.

All
views
and
opinions
expressed
therein
remain
the
sole
responsibility
of
the
author
,
and
do
not
necessarily
represent
those
of
the
Faculty
.

i
ABSTRACT
Location
allocation
is
a
combinatorial
optimization
problem
.

Traditional
exact
method
can
not
solve
location
allocation
problem
efficiently
.

This
problem
does
not
limit
itself
in
a
small
spectrum
rather
it
has
been
grown
with
lots
of
branches
for
around
a
century
.

Capacity
,
cost
,
different
type
facilities
,
demands
,
time
,
different
type
of
distances
and
mixing
with
diverse
real
world
problems
have
made
location
allocation
problem
much
complex
.

Metaheuristic
solutions
like
genetic
algorithm
and
simulated
annealing
have
been
exploited
for
long
time
to
solve
location
allocation
problem
.

In
several
researches
of
location
allocation
problem
,
these
two
algorithms
have
been
bought
into
one
umbrella
and
have
been
proved
efficient
.

But
location
allocation
problem
with
integrated
GIS
,
genetic
algorithm
and
simulated
annealing
was
not
much
explored
.

This
research
has
explored
location
allocation
problem
by
both
genetic
algorithm
and
simulated
annealing
with
GIS
integration
.

To
achieve
this
,
two
case
studies
based
on
Enschede
schools
have
been
performed
.

Location
allocation
problem
usually
considers
nearest
distance
.

Through
these
case
studies
,
location
allocation
problem
also
considers
nearest
distance
with
various
criteria
like
capacity
,
user
preference
,
existing
facility
etc
.

ii
ACKNOWLEDGEMENTS
I
would
like
to
thank
both
of
my
supervisors
for
their
support
and
guidance
all
through
this
work
.

I
would
like
to
thank
my
parents
and
family
for
their
blessings
.

I
would
like
to
thank
almighty
Allah
.


-DOCSTART-

The
performance
evaluation
of
information
retrieval
systems
has
achieved
a
high
momentum
in
the
last
few
years
.

Basic
performance
measures
of
information
retrieval
systems
include
precision
and
recall
.

While
these
measures
work
well
in
monolingual
web
retrieval
,
they
are
not
suitable
for
CLIR
(
Cross
-
lingual
Information
Retrieval
)
and
MLIR
(
Multilingual
Information
Retrieval
)
where
two
or
more
languages
are
involved
respectively
.

Many
measures
were
proposed
to
improve
over
the
precision
-
recall
measures
but
they
are
inadequate
to
exhibit
the
language
wise
performance
evaluations
.

Precision
metric
variants
for
evaluating
the
performance
over
the
retrieval
of
the
documents
in
various
languages
have
been
proposed
in
this
research
.

This
paper
also
identifies
the
major
strengths
and
shortcomings
of
some
of
the
existing
IR
performance
evaluation
measures
.

This
paper
concentrates
on
the
metric
based
performance
evaluation
on
two
variants
of
IR
.

Experiments
are
conducted
in
two
phases
(
CLIR
and
MLIR

These
two
phases
of
experiments
have
been
done
on
practical
web
search
systems
and
proved
that
the
proposed
measures
are
necessary
to
reveal
the
importance
of
language
wise
comparisons
.


-DOCSTART-

The
state
-
of
-
the
-
art
solutions
to
the
vocabulary
mismatch
in
information
retrieval
(
IR
)
mainly
aim
at
leveraging
either
the
relational
semantics
provided
by
external
resources
or
the
distributional
semantics
,
recently
investigated
by
deep
neural
approaches
.

Guided
by
the
intuition
that
the
relational
semantics
might
improve
the
effectiveness
of
deep
neural
approaches
,
we
propose
the
Deep
Semantic
Resource
Inference
Model
(
DSRIM
)
that
relies
on
:
1
)
a
representation
of
raw
-
data
that
models
the
relational
semantics
of
text
by
jointly
considering
objects
and
relations
expressed
in
a
knowledge
resource
,
and
2
)
an
end
-
to
-
end
neural
architecture
that
learns
the
query
-
document
relevance
by
leveraging
the
distributional
and
relational
semantics
of
documents
and
queries
.

The
experimental
evaluation
carried
out
on
two
TREC
datasets
from
TREC
Terabyte
and
TREC
CDS
tracks
relying
respectively
on
WordNet
and
MeSH
resources
,
indicates
that
our
model
outperforms
state
-
of
-
the
-
art
semantic
and
deep
neural
IR
models
.


-DOCSTART-

Time
is
an
important
dimension
of
any
information
space
and
can
be
very
useful
in
information
retrieval
.

Current
information
retrieval
systems
and
applications
do
not
take
advantage
of
all
the
time
information
available
in
the
content
of
documents
to
provide
better
search
results
and
user
experience
.

In
this
paper
we
show
some
of
the
areas
that
can
benefit
from
exploiting
such
temporal
information
.


-DOCSTART-

Plagiarism
is
an
increasing
problem
in
the
digital
world
.

The
sheer
amount
of
digital
data
calls
for
automation
of
plagirism
discovery
.

In
this
paper
we
evaluate
an
Information
Retrieval
approach
of
dealing
with
plagiarism
through
Vector
Spaces
.

This
will
allow
us
to
detect
similarities
that
are
not
result
of
naive
copy&paste
.

We
also
consider
the
extension
of
Vector
Spaces
where
input
documents
are
analyzed
for
term
co
-
occurence
,
allowing
us
to
introduce
some
semantics
into
our
approach
beyond
mere
word
matching
.

The
approach
is
evaluated
on
a
real
-
world
collection
of
mathematical
documents
as
part
of
the
DML
-
CZ
project
.


-DOCSTART-

A
fuzzy
bibliographic
information
retrieval
based
on
a
fuzzy
thesaurus
or
on
a
fuzzy
pseudothesaurus
is
described
.

A
fuzzy
thesaurus
consists
of
two
fuzzy
relations
defined
on
a
set
of
keywords
for
the
bibliography
.

The
fuzzy
relations
are
generated
based
on
a
fuzzy
set
model
,
which
describes
association
of
a
keyword
to
its
concepts
.

If
the
set
of
concepts
in
the
fuzzy
set
model
is
replaced
by
the
set
of
documents
,
the
fuzzy
relations
are
called
a
pseudothesaurus
,
which
is
automatically
generated
by
using
occurrence
frequencies
of
the
keywords
in
the
set
of
documents
.

The
fuzzy
retrieval
uses
two
fuzzy
relations
in
addition
,
that
is
,
a
fuzzy
indexing
and
a
fuzzy
inverted
file
:
the
latter
is
the
inverse
relation
of
the
former
.

They
are
,
however
,
related
to
different
algorithms
for
indexing
and
retrieval
,
respectively
.

An
algorithm
of
ordering
retrieved
documents
according
to
the
values
of
the
fuzzy
thesaurus
is
proposed
.

This
method
of
the
ordering
is
optimal
in
the
sense
that
one
can
obtain
documents
of
maximum
relevance
in
a
fixed
time
interval
.

An
example
of
the
fuzzy
retrieval
is
shown
on
a
prototype
database
.

This
method
shows
one
of
the
simplest
way
to
realize
fuzzy
retrieval
in
practical
database
systems
.


-DOCSTART-

Standards
are
important
because
they
make
a
field
more
open
to
small
and
medium
businesses
and
to
academic
players
.

We
review
a
number
of
standards
that
apply
to
information
retrieval
and
web
search
,
and
discuss
the
role
that
they
play
.

We
also
discuss
some
areas
where
there
is
potential
for
the
development
of
standards
,
where
for
instance
information
retrieval
would
benefit
,
and
where
standards
development
appears
feasible
.


-DOCSTART-

The
aim
of
this
study
was
to
implement
a
supervised
codebook
learning
methodology
for
optimizing
the
feature
based
Bag
-
of
-
Words(Bow
)
representation
towards
data
Retrieval
.

Following
the
cluster
hypothesis
,
that
states
that
points
within
the
same
cluster
square
measure
seemingly
to
fulfill
a
similar
data
.

The
Bow
model
are
often
applied
to
different
domains
furthermore
,
so
also
judge
our
approach
employing
a
assortment
of
for
five
time
series
datasets
,
a
text
dataset
and
a
video
dataset
.

The
gains
square
measure
three
fold
since
the
EO
-
Bow
will
improve
the
mean
Average
preciseness
,
whereas
reducing
the
coding
time
and
therefore
the
info
storage
requirements
The
Bow
model
treats
each
imageas
a
document
that
contains
a
number
of
different
â€œ
visual
â€œ
words
.

Then
an
image
is
represented
as
a
histogram
over
aset
of
representative
words
,
known
as
dictionary
or
codebook
.

These
histograms
describe
the
corresponding
images
andthey
can
be
used
for
the
subsequent
retrieval
tasks
.


-DOCSTART-

We
present
four
approaches
to
the
Amharic
French
bilingual
track
at
CLEF
2005
.

All
experiments
use
a
dictionary
based
approach
to
translate
the
Amharic
queries
into
French
Bags
-
of
-
words
,
but
while
one
approach
uses
word
sense
discrimination
on
the
translated
side
of
the
queries
,
the
other
one
includes
all
senses
of
a
translated
word
in
the
query
for
searching
.

We
used
two
search
engines
:
The
SICS
experimental
engine
and
Lucene
,
hence
four
runs
with
the
two
approaches
.
Non
-
content
bearing
words
were
removed
both
before
and
after
the
dictionary
lookup
.

TF
/
IDF
values
supplemented
by
a
heuristic
function
was
used
to
remove
the
stop
words
from
the
Amharic
queries
and
two
French
stopwords
lists
were
used
to
remove
them
from
the
French
translations
.

In
our
experiments
,
we
found
that
the
SICS
search
engine
performs
better
than
Lucene
and
that
using
the
word
sense
discriminated
keywords
produce
a
slightly
better
result
than
the
full
set
of
non
discriminated
keywords
.


-DOCSTART-

With
the
emergence
of
mobile
devices
,
Web
services
are
made
available
for
users
in
nearly
all
situations
of
every
day
life
.

While
the
WWW
offers
a
multitude
of
Web
services
there
is
a
lack
of
search
engines
which
support
the
users
in
finding
adequate
services
out
of
the
bulk
of
available
Web
services
.

Within
this
paper
,
we
will
introduce
a
context
-
sensitive
approach
for
effective
Web
service
discovery
.

As
a
huge
amount
of
context
information
could
be
gathered
by
the
use
of
mobile
devices
,
this
approach
is
especially
directed
towards
users
of
these
mobile
devices
.

Examples
for
context
information
are
the
actual
visited
place
,
the
actual
temperature
,
the
recent
illumination
,
etc
.

Also
communities
of
users
who
share
common
contexts
,
interests
and
service
needs
arise
.

While
existing
systems
for
Web
service
discovery
generally
do
not
consider
context
and
community
information
of
their
users
,
we
will
focus
on
these
topics
,
taking
advantage
of
an
novel
adaption
of
information
retrieval
techniques


-DOCSTART-

Industry
Abstracts
;
Australian
Education
Index
;
Bacon
â€™s
Media
Directory
;
Burrelle
â€™s
Media
Directory
;
Cabell
â€™s

Directories
;
Ceramic
Abstracts
;
Compendex
(
Elsevier
Engineering
Index
Computer
Information
Systems
Abstracts
;
Corrosion
Abstracts
;
CSA
Civil
Engineering
Abstracts
;
CSA
Illumina
;
CSA
Mechanical
Transportation
Engineering
Abstracts
;
DBLP
;
DEST
Register
of
Refereed
Journals
;
EBSCOhost
â€™s
Academic
Search
;
EBSCOhost
â€™s
Academic
Source
;
EBSCOhost
â€™s
Business
Source
;
EBSCOhost
â€™s
Computer
Applied
Sciences
Complete
;
EBSCOhost
â€™s
Computer
Science
Index
;
EBSCOhost
â€™s
Computer
Source
;
EBSCOhost
â€™s
Current
Abstracts
;
EBSCOhost
â€™s
Science
Technology
Collection
;
Electronics
Communications
Abstracts
;
Engineered
Materials
Abstracts
;
ERIC
Education
Resources
Information
Center
;
GetCited
;
Google
Scholar
;
INSPEC
;
JournalTOCs
;
KnowledgeBoard
;
Library
Information
Science
Abstracts
(

LISA
Materials
Business
File
Steels
Alerts
;
MediaFinder
;
Norwegian
Social
Science
Data
Services
(
NSD
PsycINFO
PubList.com
;
SCOPUS
;
Solid
State
Superconductivity
Abstracts
;
The
Index
of
Information
Systems
Journals
;
The
Standard
Periodical
Directory
;
Ulrich
â€™s
Periodicals
Directory

The
International
Journal
of
Distance
Education
Technologies
(
IJDET
ISSN
1539
-
3100
;
eISSN
1539
-
3119
Copyright
2014
IGI
Global
.

All
rights
,
including
translation
into
other
languages
reserved
by
the
publisher
.

No
part
of
this
journal
may
be
reproduced
or
used
in
any
form
or
by
any
means
without
written
permission
from
the
publisher
,
except
for
noncommercial
,
educational
use
including
classroom
teaching
purposes
.

Product
or
company
names
used
in
this
journal
are
for
identification
purposes
only
.

Inclusion
of
the
names
of
the
products
or
companies
does
not
indicate
a
claim
of
ownership
by
IGI
Global
of
the
trademark
or
registered
trademark
.

The
views
expressed
in
this
journal
are
those
of
the
authors
but
not
necessarily
of
IGI
Global
.

Research
Articles


-DOCSTART-

Research
on
ontology
is
becoming
increasingly
widespread
in
the
computer
science
community
,
and
its
importance
is
being
recognized
in
a
multiplicity
of
research
fields
and
application
areas
,
including
knowledge
engineering
,
database
design
and
integration
,
information
retrieval
and
extraction
.

We
shall
use
the
generic
term
â€œ
information
systems
in
its
broadest
sense
,
to
collectively
refer
to
these
application
perspectives
.

We
argue
in
this
paper
that
so
-
called
ontologies
present
their
own
methodological
and
architectural
peculiarities
:
on
the
methodological
side
,
their
main
peculiarity
is
the
adoption
of
a
highly
interdisciplinary
approach
,
while
on
the
architectural
side
the
most
interesting
aspect
is
the
centrality
of
the
role
they
can
play
in
an
information
system
,
leading
to
the
perspective
of
ontology
-
driven
information
systems
.


-DOCSTART-

In
the
age
of
digital
information
more
and
more
digital
libraries
and
historical
archives
are
using
information
systems
in
order
to
facilitate
the
document
retrieval
and
provide
better
visualization
of
the
search
results
and
document
presentation
.

Much
research
has
been
done
in
the
field
of
digital
libraries
,
but
in
the
case
of
historical
archives
,
which
have
particular
needs
,
this
is
not
the
case
.

To
this
end
,
we
investigate
the
use
of
new
tools
,
which
are
based
on
the
ontology
of
the
historical
archive
in
order
to
provide
a
new
and
effective
method
for
document
retrieval
in
a
dynamic
environment
which
will
take
into
account
the
collaboration
needs
of
the
users
.


-DOCSTART-

Finding
software
for
reuse
is
a
problem
that
programmers
face
.

To
reuse
code
that
has
been
proven
to
work
can
increase
any
programmer
â€™s
productivity
,
benefit
corporate
productivity
,
and
also
increase
the
stability
of
software
programs
.

This
paper
shows
that
fuzzy
retrieval
has
an
improved
retrieval
performance
over
typical
Boolean
retrieval
.

Various
methods
of
fuzzy
information
retrieval
implementation
and
their
use
for
software
reuse
will
be
examined
.

A
deeper
explanation
of
the
fundamentals
of
designing
a
fuzzy
information
retrieval
system
for
software
reuse
will
be
examined
.

Future
research
options
and
necessary
data
storage
systems
are
explored
.


-DOCSTART-

A
q
query
Locally
Decodable
Code
(
LDC
)
encodes
an
n
-
bit
message
x
as
an
N
-bit
codeword
C(x
such
that
one
can
probabilistically
recover
any
bit
xi
of
the
message
by
querying
only
q
bits
of
the
codeword
C(x
even
after
some
constant
fraction
of
codeword
bits
has
been
corrupted
.

We
give
new
constructions
of
three
query
LDCs
of
vastly
shorter
length
than
that
of
previous
constructions
.

Specifically
,
given
any
Mersenne
prime
p
2
t
1
,
we
design
three
query
LDCs
of
length

N
exp
n1/t
for
every
n.

Based
on
the
largest
known
Mersenne
prime
,
this
translates
to
a
length
of
less
than
exp
(


-DOCSTART-

Correlation
study
is
at
the
heart
of
time
-
varying
multivariate
volume
data
analysis
and
visualization
.

Document
clustering
is
related
to
data
clustering
concept
which
is
one
of
data
mining
tasks
and
unsupervised
classification
.

Clustering
is
a
useful
technique
that
organizes
a
large
quantity
of
unordered
text
documents
into
a
small
number
of
meaningful
and
coherent
clusters
,
thereby
providing
a
basis
for
intuitive
and
informative
navigation
and
browsing
mechanisms
.

It
is
often
applied
to
the
huge
data
in
order
to
make
a
partition
based
on
their
similarity
.

Initially
,
it
used
for
Information
Retrieval
in
order
to
improve
the
precision
and
recall
from
query
.

It
is
very
easy
to
cluster
with
small
data
attributes
which
contains
of
important
items
.

Furthermore
,
document
clustering
is
very
useful
in
retrieve
information
application
in
order
to
reduce
the
consuming
time
and
get
high
precision
and
recall
.

Therefore
,
we
propose
to
integrate
the
information
retrieval
method
and
document
clustering
as
concept
space
approach
.

The
method
is
known
as
Latent
Semantic
Index
(
LSI
)
approach
which
used
Singular
Vector
Decomposition
(
SVD
)
or
Principle
Component
Analysis
(
PCA
The
aim
of
this
method
is
to
reduce
the
matrix
dimension
by
finding
the
pattern
in
document
collection
with
refers
to
concurrent
of
the
terms
.

Each
method
is
implemented
to
weight
of
termdocument
in
vector
space
model
(
VSM
)
for
document
clustering
.


-DOCSTART-

s
Service
and
providing
the
first
on
-
line
implementations
of
several
of
these
.

Chemical
Industry
Notes
and
Patent
Concordance
were
first
made
available
in
1976
.

Also
,
the
CA
CONDENSATES
file
for
several
collective
index
periods
and
the
CASIA
indexing
file
were
first
made
available
on
-
line
in
1976
by
DIALOG
Information
Services
.

In
that
same
year
,
the
CHEMNAME
chemical
-
substance
file
was
created
from
CASIA
and
the
CA
Index
Guide
.

As
a
result
of
that
work
,
the
Index
Guide
was
also
used
to
enrich
the
combined
CA
CONDENSATES
/
CASIA
file
first
offered
on
-
line
in
1977
by
DIALOG
Information
Services
.

REFERENCES
AND
NOTES
Summit
,
R.
K
Lockheed
Experience
in
Processing
Large
Databases
for
its
Commercial
Information
Retrieval
Service
J
Chem
.

InJ
Comput
.

1975
,
15
,
40
-
42
.

Schultheisz
,
R.
J
Walker
,
D.
F
Kannan
,
K.
L
Design
and
Implementation
of
an
On
-
Line
Chemical
Dictionary
(
CHEMLINE
J
Chem
.

InJ
Comput
.

Sci
.
1978
,
29
,
173
-
179
.

Callahan
,
M.
V
Rusch
,
P.
F
Online
Implementation
of
the
CA
SEARCH
File
and
the
CAS
Registry
Nomenclature
File
Online
Reu
.

1981,5,377
-
393
.

Fisanick
,
W
Mitchell
,
L.
D
Scott
,
J.
A
Vander
Stouw
,
G.
G
Substructure
Searching
of
Computer
-
Readable
Chemical
Abstracts
Service
.

Ninth
Collective
Index
Chemical
Nomenclature
Files
J.
Chem
.

InJ
Comput
.

1975
,
15
,
73
-
84
.

Fernelius
,
W.
C
Powell
,
W.
H
Confusion
in
the
Periodic
Table
of
the
Elements
J.
Chem
.

1982
,
59
,
504
-
508
.

Evolution
of
Industrial
Chemical
Information
Systems
CARLOS
M.
BOWMAN
*
and
PAULA
B.
MOSES

The
Dow
Chemical
Company
,
Midland
,
Michigan
48674

Received
January
21
,
1985
A
modern
industrial
chemical
information
system
is
an
integrated
system
that
provides
for
the
storage
,
retrieval
,
and
manipulation
of
internal
and
external
information
to
meet
the
needs
of
the
organization
.

It
incorporates
some
of
the
most
modern
tools
and
methods
available
in
an
effort
to
be
cost
effective
.

The
various
components
of
an
industrial
chemical
information
system
are
enumerated
,
and
their
evolution
is
described
and
anticipated
.


-DOCSTART-

This
paper
presents
our
contribution
to
enhance
literature
-
based
discovery
with
information
retrieval
techniques
.

We
propose
an
approach
that
combines
flexible
Information
Retrieval
and
Concepts
indexing
for
knowledge
discovery
in
biomedical
literature
.

Flexible
information
retrieval
contributes
to
filter
MEDLINE
biomedical
literature
to
the
most
relevant
documents
,
whereas
concept
-
based
indexing
allows
to
quickly
identifying
candidate
concepts
that
could
potentially
validate
a
hypothesis
.

We
have
tested
our
approach
by
replicating
the
Swanson
â€™s
first
discovery
on
fish
oil
and
Raynaud
's
disease
correlation
.

The
obtained
results
show
the
effectiveness
of
our
approach
.

MOTS
-
CLÃ‰S
recherche
dâ€™information
,
dÃ©couverte
de
connaissances
dans
la
littÃ©rature
biomÃ©dicale
MeSH
;
hypothÃ¨se
de
Swanson
.


-DOCSTART-

Rank
fusion
is
the
task
of
combining
multiple
ranked
document
lists
(
ranks
)
into
a
single
ranked
list
.

It
is
a
late
fusion
approach
designed
to
improve
the
rankings
produced
by
individual
systems
.

Rank
fusion
techniques
have
been
applied
throughout
multiple
domains
:
e.g.
combining
results
from
multiple
retrieval
functions
,
or
multimodal
search
where
several
feature
spaces
are
common
.

In
this
paper
,
we
present
the
Inverse
Square
Rank
fusion
method
family
,
a
set
of
novel
fully
unsupervised
rank
fusion
methods
based
on
quadratic
decay
and
on
logarithmic
document
frequency
normalization
.

Our
experiments
created
with
standard
Information
Retrieval
datasets
(
image
and
text
fusion
)
and
image
datasets
(
image
features
fusion
show
that
ISR
outperforms
existing
rank
fusion
algorithms
.

Thus
,
the
proposed
technique
has
comparable
or
better
performance
than
existing
state
-
of
-
the
-
art
approaches
,
while
maintaining
a
low
computational
complexity
and
avoiding
the
need
for
document
scores
or
training
data
.


-DOCSTART-

This
paper
will
first
briefly
survey
the
existing
impact
of
multimedia
information
retrieval
(
MIR
)
in
applications
.

It
will
then
analyze
the
current
trends
of
MIR
research
which
can
have
an
influence
on
future
applications
.

It
will
then
detail
the
future
possibilities
and
bottlenecks
in
applying
the
MIR
research
results
in
the
main
target
application
areas
,
such
as
the
consumer
(
e.g
personal
video
recorders
,
web
information
retrieval
public
safety
(
e.g
automated
smart
surveillance
systems
and
professional
world
(
e.g
automated
meeting
capture
and
summarization

In
particular
,
recommendations
will
be
made
to
the
research
community
regarding
the
challenges
that
need
to
be
met
to
make
the
knowledge
transfer
towards
the
applications
more
efficient
and
effective
.

It
will
also
attempt
to
study
the
trends
in
the
applications
which
can
inform
the
MIR
community
on
directing
intellectual
resources
towards
MIR
problems
which
can
have
a
maximal
real
-
world
impact
.


-DOCSTART-

Much
of
the
research
in
Information
Retrieval
has
concerned
improvements
to
similarity
computations
,
statistics
gathering
,
and
term
extraction
,
with
the
goal
of
improving
effectiveness
.

However
,
a
simple
examination
of
user
characteristics
can
readily
show
,
the
method
of
computing
similarity
is
less
important
than
the
behaviour
of
the
system
interface
and
environmental
factors
.

It
is
hypothesised
that
there
must
be
knowledge
of
the
relationship
between
a
query
,
its
user
,
the
environment
,
and
the
query
and
user
instantiation
in
the
real
world
.

This
hypothesis
and
others
are
demonstrated
.

With
facilities
for
interaction
and
feedback
appropriately
incorporated
,
effectiveness
of
100
%
can
be
achieved
.


-DOCSTART-

In
this
paper
,
we
introduce
an
approach
for
training
a
Named
Entity
Recognizer
(
NER
)
from
a
set
of
seed
entities
on
the
web
.

Creating
training
data
for
NERs
is
tedious
,
time
consuming
,
and
becomes
more
difficult
with
a
growing
set
of
entity
types
that
should
be
learned
and
recognized
.

Named
Entity
Recognition
is
a
building
block
in
natural
language
processing
and
is
widely
used
in
fields
such
as
question
answering
,
tagging
,
and
information
retrieval
.

Our
NER
can
be
trained
on
a
set
of
entity
names
of
different
types
and
can
be
extended
whenever
a
new
entity
type
should
be
recognized
.

This
feature
increases
the
practical
applications
of
the
NER
.


-DOCSTART-

The
heuristics
employed
in
information
retrieval
systems
have
traditionally
been
document
-
based
,
and
have
judged
similarity
holistically
based
upon
entire
documents
.

In
this
work
we
present
a
locality
-
based
paradigm
for
information
retrieval
,
in
which
every
word
location
in
each
document
is
scored
.

The
locality
-
based
similarity
heuristic
provides
retrieval
e
ectiveness
as
good
as
the
documentbased
technique
,
and
has
the
additional
advantage
of
allowing
the
matching
section
or
sections
of
retrieved
documents
to
be
shown
to
the
user
when
they
are
sifting
the
results
of
their
query
.

This
is
a
considerable
improvement
upon
the
conventional
presentation
mechanism
,
in
which
the
user
must
manually
search
each
document
for
the
passage
if
any
such
passage
exists
at
all
that
suggested
to
the
retrieval
mechanism
that
this
document
is
an
answer
.

We
also
describe
an
improved
index
representation
that
supports
the
required
operations
.


-DOCSTART-

The
lackness
of
a
formal
account
is
probably
one
of
the
most
evident
of
the
shortcomings
of
information
retrieval
:
concepts
like
information
,
information
need
,
and
relevance
are
neither
well
understood
nor
formally
defined
.

This
paper
sketches
a
cognitive
framework
that
permits
to
analyze
these
three
central
concepts
of
the
information
retrieval
scenario
.

The
framework
consists
of
concepts
as
cognitive
agents
acting
in
the
world
,
knowledge
states
possessed
by
the
cognitive
agents
,
transitions
among
knowledge
states
,
and
inferences
.

On
the
basis
of
the
framework
,
information
is
formally
defined
as
a
pair
representing
the
difference
between
two
knowledge
states
;
this
definition
permits
to
clarify
the
distinction
among
data
,
knowledge
,
and
information
and
to
discuss
the
subjectiveness
of
information
.

On
this
ground
,
the
concept
of
information
need
is
examined
:
it
is
defined
,
it
is
studied
in
the
context
of
the
interaction
between
an
information
retrieval
system
and
a
user
,
and
the
well
known
classification
in
verificative
,
conscious
topical
,
and
muddled
needs
is
analyzed
.

On
the
basis
of
the
above
definitions
of
information
and
information
need
,
relevance
is
formally
defined
,
and
some
critical
features
of
this
concept
are
discussed
.


-DOCSTART-

The
advent
of
appropriate
computer
technology
has
led
to
the
design
and
implementation
of
sophisticated
on
-
line
information
retrieval
systems
which
answer
the
need
for
fast
and
accurate
retrieval
of
information
.

These
systems
which
are
now
being
offered
to
the
public
may
have
been
developed
for
private
/e.g

The
New
York
Times
Information
Bank
governmental

(
NASA
â€™s
RECON
or
commercial
(
SDC
â€™s
ORBIT
)
purposes
;
but
working
with
such
systems
is
no
longer
the
exclusive
domain
of
the
information
specialist
or
the
librarian
.

Many
systems
are
being
offered
for
direct
access
by
the
ultimate
user
population
:
the
scientist
,
engineer
,
scholar
,
manager
and
student
.

More
and
more
emphasis
is
placed
upon
the
importance
of
direct
use
;
the
need
to
eliminate
,
or
at
least
lessen
,
the
role
of
the
intermediary
.

This
trend
must
by
its
nature
lead
system
designers
to
realize
the
need
for
effective
user
training
programs
.

In
the
1967
National
Colloquium
on
Information
Retrieval
,
Cavanagh
(
1
)
stated
that
Without
thorough
training
the
user
may
misuse
the
system
or
fail
t
o
exploit
its
potential
,
thereby
effectively
degrading
system
performance
Lancaster
(
2
)
rates
training
alongside
systems
design
,
hardware
and
the
data
base
as
one
of
the
four
major
factors
contributing
to
the
effectiveness
of
a
search
in
an
on
-
line
system
.

In
a
more
specific
instance
,
Rae
(
3
)
has
identified
user
training
as
a
major
obstacle
to
the
use
of
the
SUNY
system
at
the
Parkinson
Information
Center
.

It
is
surprising
,
then
,
to
view
the
general
lack
of
help
,
audiovisual
presentations
and
on
-
line
instruction
have
all
been
used
with
varying
degrees
of
success
.

The
author
contends
that
the
use
of
computer
-
assisted
instruction
in
conjunction
with
the
on
-
line
information
retrieval
system
is
the
most
promising
form
of
instruction
in
that
the
medium
itself
,
as
well
as
the
message
,
may
be
used
to
acquaint
the
novice
searcher
with
an
interactive
userhystem
interface
.


-DOCSTART-

This
paper
presents
the
results
of
an
experimental
investigation
into
the
use
of
Neural
Networks
for
implementing
Relevance
Feedback
in
an
interactive
Information
Retrieval
System
.

The
most
advance
Relevance
Feedback
technique
used
in
operative
Interactive
Information
Retrieval
systems
,
Probabilistic
Relevance
Feedback
,
is
compared
with
a
Neural
Networks
based
technique
.

The
latest
uses
the
learning
and
generalisation
capabilities
of
a
3{layer
feedforward
Neural
Network
with
the
Backpropagation
learning
procedure
to
learn
distinguishing
between
relevant
and
non
-
relevant
documents
.

A
comparative
evaluation
between
the
two
techniques
is
reported
using
an
advance
Information
Retrieval
System
,
a
Neural
Network
simulator
,
and
an
IR
test
document
collection
.

The
results
are
reported
and
explained
from
an
Information
Retrieval
point
of
view
.

1
Information
Retrieval
Information
Retrieval
(
IR
)
is
a
science
that
aims
at
storing
and
allowing
fast
access
to
a
large
amount
of
information
.

This
information
can
be
of
any
kind
:
textual
,
visual
,
or
auditory
.

An
Information
Retrieval
System
(
IRS
)
is
a
computing
tool
which
stores
this
information
to
be
retrieved
for
future
use
.

Most
actual
IR
systems
store
and
enable
the
retrieval
of
only
textual
information
or
documents
.

However
,
this
is
not
an
easy
task
,
just
to
give
a
clue
to
its
size
,
it
must
be
noticed
that
often
the
collections
of
documents
an
IRS
has
to
deal
with
contain
several
thousands
or
even
millions
of
documents
.

A
user
accesses
the
IRS
by
submitting
a
query
,
the
IRS
then
tries
to
retrieve
all
documents
that
\satisfy
"
the
query
.

As
opposed
to
database
systems
,
and
IRS
does
not
provide
an
exact
answer
but
produce
a
ranking
of
documents
that
appear
to
contain
information
relevant
to
the
query
.

Queries
and
documents
are
usually
expressed
in
natural
language
and
to
be
processed
by
the
IRS
they
are
passed
through
a
query
and
a
document
processors
which
breaks
them
into
their
constituents
words
.

Non
-
content
-
bearing
words

the
but
and
etc
are
discarded
,
and
su
xes
are
removed
,
so
that
what
remains
to
represent
query
and
documents
are
lists
of
terms
that
can
be
compared
using
some
similarity
evaluation
algorithms
.


-DOCSTART-

Based
on
bibliometric
methods
,
the
article
makes
an
analysis
on
455
papers
published
by
the
scholars
of
Mainland
China
(
ML
)
in
66
foreign
periodicals
in
the
field
of
Information
Science
&
amp
;
Library
Science
(
ISLS
)
included
in
Social
Sciences
Citation
Index
from
2000
to
2010
.

A
series
of
quantitative
analysis
has
been
made
from
the
aspects
of
publications
,
periodicals
,
affiliation
to
reflect
the
current
research
status
and
academic
ability
of
Mainland
China
in
the
field
of
ISLS
.


-DOCSTART-

The
central
issue
in
language
model
estimation
is
smoothing
,
which
is
a
technique
for
avoiding
zero
probability
estimation
problem
and
overcoming
data
sparsity
.

There
are
three
representative
smoothing
methods
:
Jelinek
-
Mercer
(
JM
)
method
;
Bayesian
smoothing
using
Dirichlet
priors
(
Dir
)
method
;
and
absolute
discounting
(
Dis
)
method
,
whose
parameters
are
usually
estimated
empirically
.

Previous
research
in
information
retrieval
(
IR
)
on
smoothing
parameter
estimation
tends
to
select
a
single
value
from
optional
values
for
the
collection
,
but
it
may
not
be
appropriate
for
all
the
queries
.

The
effectiveness
of
all
the
optional
values
should
be
considered
to
improve
the
ranking
performance
.

Recently
,
learning
to
rank
has
become
an
effective
approach
to
optimize
the
ranking
accuracy
by
merging
the
existing
retrieval
methods
.

In
this
article
,
the
smoothing
methods
for
language
modeling
in
information
retrieval
(
LMIR
)
with
different
parameters
are
treated
as
different
retrieval
methods
,
then
a
learning
to
rank
approach
to
learn
a
ranking
model
based
on
the
features
extracted
by
smoothing
methods
is
presented
.

In
the
process
of
learning
,
the
effectiveness
of
all
the
optional
smoothing
parameters
is
taken
into
account
for
all
queries
.

The
experimental
results
on
the
Learning
to
Rank
for
Information
Retrieval
(
LETOR
)

LETOR3.0
and
LETOR4.0
data
sets
show
that
our
approach
is
effective
in
improving
the
performance
of
LMIR
.


-DOCSTART-

This
paper
presents
the
results
obtained
by
our
group
at
the
GeoCLEF
2006
.

Our
system
used
a
method
based
on
the
expansion
of


-DOCSTART-

A
GENERATIVE
THEORY
OF
RELEVANCE
SEPTEMBER
2004

VICTOR
LAVRENKO
B.Sc
UNIVERSITY
OF
MASSACHUSETTS
AMHERST
M.Sc
UNIVERSITY
OF
MASSACHUSETTS
AMHERST

Ph
.
D
UNIVERSITY
OF
MASSACHUSETTS
AMHERST
Directed
by
:
Professor
W.
Bruce
Croft
and
Professor
James
Allan

We
present
a
new
theory
of
relevance
for
the
field
of
Information
Retrieval
.

Relevance
is
viewed
as
a
generative
process
,
and
we
hypothesize
that
both
user
queries
and
relevant
documents
represent
random
observations
from
that
process
.

Based
on
this
view
,
we
develop
a
formal
retrieval
model
that
has
direct
applications
to
a
wide
range
of
search
scenarios
.

The
new
model
substantially
outperforms
strong
baselines
on
the
tasks
of
ad
-
hoc
retrieval
,
cross
-
language
retrieval
,
handwriting
retrieval
,
automatic
image
annotation
,
video
retrieval
,
and
topic
detection
and
tracking
.

Empirical
success
of
our
approach
is
due
to
a
new
technique
we
propose
for
modeling
exchangeable
sequences
of
discrete
random
variables
.

The
new
technique
represents
an
attractive
counterpart
to
existing
formulations
,
such
as
multinomial
mixtures
,
pLSI
and
LDA
:
it
is
effective
,
easy
to
train
,
and
makes
no
assumptions
about
the
geometric
structure
of
the
data
.


-DOCSTART-

With
the
rapid
development
of
artificial
intelligence
and
natural
language
processing
,
text
similarity
calculation
has
become
the
core
module
of
many
applications
such
as
semantic
disambiguation
,
information
retrieval
,
automatic
question
answering
and
data
mining
etc
.

Most
of
the
existing
semantic
similarity
algorithms
are
based
on
statistical
methods
or
rule
based
methods
that
are
conducted
on
ontology
dictionaries
and
some
kind
of
knowledge
bases
.

Wherein
the
rule
-
based
methods
usually
use
the
dictionary
,
the
ontology
tree
or
graph
,
or
the
co
-
occurrence
number
of
attributes
,
while
the
statistical
methods
may
choose
to
use
or
not
use
a
knowledge
base
.

While
a
statistical
method
of
using
a
knowledge
base
incorporates
more
comprehensive
knowledge
and
has
the
capability
of
reduces
knowledge
noise
,
it
usually
obtains
better
performance
.

Nevertheless
,
due
to
the
imbalanced
distribution
of
different
items
in
a
knowledge
base
,
the
semantic
similarity
calculation
results
for
low
-
frequency
words
are
usually
poor
.


-DOCSTART-

Recently
,
information
retrieval
based
on
Peer
-
to
-
Peer
(
P2P
)
networks
is
becoming
a
popular
and
dynamic
research
topic
.

In
this
work
,
we
study
two
scenarios
with
their
own
problems
and
we
solve
them
by
our
proposed
Site
-
to
-
Site
(
S2S
)
Searching
in
a
pure
P2P
network
and
GAroute
in
a
hybrid
P2P
network
respectively
.

The
first
scenario
is
that
Web
information
retrieval
by
Centralized
Search
Engines
(
CSEs
)
like
Google
have
three
shortcomings
.

First
,
CSEs
are
centralized
so
that
they
require
expensive
resources
to
handle
search
requests
.

Second
,
the
search
results
are
not
always
up
-
to
-
date
.

Third
,
website
owners
have
no
control
over
their
shared
contents
such
as
preventing
published
contents
from
being
searched
.

To
circumvent
the
aforementioned
shortcomings
,
we
refer
to
Gnutella
to
distribute
search
engines
called
S2S
search
engines
over
websites
(
peers
)
in
a
pure
P2P
network
,
which
maintain
their
updated
local
contents
with
full
control
by
their
owners
.

Hence
,
each
website
becomes
an
autonomous
search
engine
and
they
join
together
to
form
a
S2S
Searching
network
.

In
this
thesis
,
we
show
the
system
architecture
,
indexing
and
matching
algorithms
of
S2S
search
engines
.

In
addition
,
we
explain
our
query
routing
algorithm
based
on
distributed
registrars
which
prevents
the
query
flooding
problem
existing
in
Gnutella
.

Furthermore
,
we
describe
our
S2S
communication
protocol


-DOCSTART-

The
use
of
informetric
analyses
has
had
profound
effects
on
the
development
of
powerful
information
retrieval
systems
.

The
informetric
properties
of
melodies
represented
as
simple
collections
of
intervallic
n
-
grams
exhibit
some
remarkable
similarities
to
the
well
-
known
informetric
properties
of
text
.

Understanding
these
similarities
can
play
a
vital
role
in
the
creation
of
a
successful
Music
Information
Retrieval
(
MIR
)
system
.

This
paper
examines
the
informetric
properties
of
intervallic
n
-
grams
and
shows
how
this
information
can
be
used
to
give
theoretical
justifications
for
the
application
of
traditional
IR
methodologies
in
constructing
MIR
systems
.

Introduction
Automating
access
to
musical
information
through
the
use
of
digital
computers
has
intrigued
musicologists
,
computer
scientists
,
librarians
and
music
lovers
alike
.

Before
the
advent
of
digital
computers
the
principal
method
of
accessing
musical
information
was
the
thematic
catalogue
.

Scholars
and
musicians
have
consulted
these
printed
volumes
for
over
a
thousand
years
(
Brook
1980

In
them
they
have
found
fragments
of
musical
works
called
incipits
that
represent
the
beginnings
of
the
work
,
or
significant
parts
(
themes
These
incipits
have
taken
on
various
forms
including
conventional
notes
,
neumes
,
tablatures
,
numbers
,
letters
or
computer
codes

The
amount
of
information
conveyed
by
incipits
can
vary
depending
on
its
representation
.

Sometimes
these
incipits
have
been
simply
extracted
from
a
musical
score
and
contain
pitch
,
harmonic
,
rhythmic
,
editorial
,
textual
and
timbral
information
.

Other
times
,
the
authors
of
thematic
catalogues
have
seen
fit
to
greatly
reduce
the
amount
of
information
presented
by
representing
only
select
aspects
of
a
melody
,
usually
pitch
names
(
e.g.
Barlow
and
Morgenstern
1949
Information
has
been
further
reduced
by
representing
incipits
through
the
use
of
the
intervals
(
i.e
the
signed
distance
between
notes
,
usually
measured
in
semitones
)
that
make
up
the
general
shape
of
a
melody
(
e.g.
Keller
and
Rabson
;
Parsons
1975
Despite
the
method
of
representation
,
thematic
catalogues
are
especially
noteworthy
because
they
attempt
to
give
users
the
ability
to
access
musical
information
on
its
own
terms
;
that
is
to
say
,
thematic
catalogues
give
users
the
ability
to
frame
musical
queries
as
music
.

There
seems
to
be
as
many
approaches
to
developing
Music
Information
Retrieval
(
MIR
)
systems
as
there
are
developers
.

Some
have
designed
complex
suites
of
computer
tools
to
analyse
all
the
varied
facets
of
music
(
e.g
Huron
1991
Others
have
tried
to
automate
the
thematic
catalogue
by
including
incipits
as
part
of
a
bibliographic
record
(
e.g
RISM
1997
Still
others
have
explored
the
idea
of
using
sophisticated
approximate
string
-
matching
techniques
(
e.g.
Ghias
et
al
1995
;
McNab
et
al
.

One
thing
that
unites
all
of
these
approaches
is
that
they
have
some
kind
of
shortcoming
.

The
more
powerful
analytic
systems
can
be
very
difficult
to
use
,
incipit
indexes
leave
out
large
amounts
of
music
that
might
be
of
interest
,
and
approximate
stringmatching
can
be
computationally
expensive
without
necessarily
giving
better
results
(
Downie
1997
Taking
our
cue
from
those
thematic
catalogues
that
have
reduced
the
amount
of
musical
information
represented
(
e.g.
Parsons
1975
;
Keller
and
Rabson
1980
)
we
have
been
developing
prototype
a
MIR
system

(
Tague
et
al
.

1993
;
Downie
1994
)
based
upon
the
intervals
found
within
the
melodies
of
a
test
collection
of
100
folksongs
(
Downie
1995
We
believe
that
there
is
enough
information
contained
within
an
interval
-
only
representation
of
monophonic
melodies
that
effective
retrieval
of
musical
information
can
be
achieved
.

We
aim
to
extend
the
thematic
catalogue
model
by
affording
access
to
melodic
segments
found
anywhere
within
a
melody
.

To
achieve
this
extension
we
fragment
the
melodies
into
length
-
n
subsections
called
n
-
grams
.

The
length
of
these
n
-
grams
and
the
degree
to
which
we
precisely
represent
the
intervals
are
variables
to
be
analysed
.

For
example
,
consider
Figure
1
where
Ix
represents
an
interval

I
â€
at
location

x
in
a
melodic
string
.

The
Contiguous
String
represents
the
sequence
of
intervals
as
extracted
from
a
melody
.

The
next
three
items
show
how
we
partition
the
Contiguous
String
into
substrings
of
length-4
,
5
,
and
6
,
called
4-grams
,
5-grams
,
and
6-grams
,
respectively
.

In
our
nomenclature
we
denote
these
n
-
grams
as
L4
,
L5
and
L6
,
respectively
Insert
Figure
1
around
here
]

Depending
on
how
precisely
one
wished
to
represent
the
intervals
in
a
melody
I
â€
can
take
on
many
different
value
ranges
.

If
one
wanted
to
represent
only
the
direction
of
the
intervals
one
could
group
all
the
intervals
into
one
of
three
classes
(
e.g
Same
,
Up
,

In
our
nomenclature
we
call
such
a
classification
scheme
C3
,
where
â€œ
3
"
indicates
the
number
of
elements
in
the
scheme
.

The
C3
scheme
is
considered
very
forgiving
because
a
user
would
only
have
to
remember
the
contour
of
a
melody
and
not
its
exact
intervals
.

A
much
more
precise
classification
scheme
is
C23
where
each
of
the
intervals
within
the
range
of
an
octave
(
either
direction
)
is
represented
.

The
C23
scheme
is
not
very
forgiving
but
it
does
give
a
very
accurate
representation
of
most
melodic
lines
.

On
a
recall
-
precision
continuum
,
C3
is
designed
to
enhance
recall
(
at
the
expense
of
precision
)
while
C23
is
designed
to
enhance
precision
(
at
the
expense
of
recall
In
our
experiments
,
we
have
looked
at
the
C7
,
C12
,
C14
,
and
C23
classification
schemes
.

These
n
-
grams
form
discrete
units
of
melodic
information
much
in
the
same
manner
as
words
are
discrete
units
of
language
.

Thus
,
we
have
come
to
consider
them
â€œ
musical
words
We
hypothesize
that
,
for
the
purposes
of
music
information
retrieval
,
we
can
treat
them
as
â€œ
real
words
â€
and
thus
apply
tradition
text
-
based
information
retrieval
techniques
.

In
Downie
(
1995
)
we
examined
the
retrieval
characteristics
of
four
n
-
gram
/
classification
scheme
(
CxLx
)
combinations
as
applied
to
a
collection
of
100
folksongs
:
C7L6
,
C12L5
,
C14L5
and
C23L4
.

A
group
of
simulated
queries
was
run
against
these
databases
.

Retrieval
effectiveness
was
evaluated
using
a
variation
on
the
traditional
precision
measure
.

The
results
of
the
experiments
were
encouraging
.

Now
,
in
this
paper
,
we
will
present
the
results
of
a
small
set
of
informetric
analyses
on
the
same
four
databases
designed
to
explore
the
informetric
properties
of
the
â€œ
musical
words
â€
found
within
them
We
will
also
present
some
of
the
implications
of
these
properties
for
the
purposes
of
music
information
retrieval
.

Informetric
Analyses

The
best
justification
for
performing
informetric
analyses
in
conjunction
with
the
simulation
and
evaluation
of
IR
systems
can
be
found
in
Wolfram
â€™s
Applying
informetric
characteristic
of
databases
to
IR
system
file
design
,
Parts
I
and
II
(
1992a
/
b
Knowledge
of
the
informetric
properties
of
an
IR
system
has
been
shown
to
assist
developers
predict
and
model
:
1
)
storage
requirements
(
Tague
1988
;

Tague
and
Nicholls
1987
2
)
index
growth
(
Heaps
1978
;
Wolfram
et
al
.

1990
3
)
optimal
indexing
and
search
strategies
(
Sampson
and
Bendell
1985
;
Wolfram
1992b
and
,
4
)
query
usage
(
Nelson
1988
;

Wolfram
1992b
Of
these
four
,
we
will
be
making
comments
about
the
first
three
based
upon
the
results
of
this
study
.

When
performing
informetric
analyses
it
is
customary
to
make
the
distinction
between
types
and
tokens
.

Types
are
the
unique
entities
(
e.g
words
,
symbols
,
n
-
grams
,
etc
that
make
up
a
corpus
of
interest
.

Tokens
are
the
instances
of
the
types
.

For
example
,
the
collection
[
dog
,
dog
,
cat
]
contains
two
types
(
dog
and
cat
)
and
three
tokens
.

In
this
study
the
types
are
the
distinct
n
-
grams
and
the
tokens
the
instances
of
same
.

Three
distinct
informetric
analyses
will
be
presented
here
.

First
,
we
will
look
at
the
summary
statistics
that
describe
the
basic
properties
of
our
data
(
e.g
type
counts
,
token
counts
,
etc
from
two
distinct
â€œ
views
1)ViewA
,
where
we
consider
each
database
to
be
one
large
corpus
(
i.e
all
tokens
are
used
to
represent
the
songs
and
,
2
)
ViewB
,
where
each
database
is
partitioned
into
the
100
songs
and
only
one
token
per
type
is
used
to
represent
each
melody
.

Second
,
using
forms
of
the
Lotka
(
1926
)
and
Zipf
(
1935
;
1949
)
distributions
,
we
will
examine
the
results
of
our
goodness
-
of
-
fit
term
-
distribution
modelling
.

Each
of
these
models
was
tested
against
1
)
the
distribution
of
types
over
each
database
as
a
whole
(
ViewA
and
,
2
)
the
distribution
of
types
over
the
100
songs
,
where
only
one
token
per
type
is
used
to
represent
each
song
(
ViewB
Third
,
an
information
theory
analysis
(
Shannon
1949
;
1951
where
the
entropy
of
i.e
the
average
information
contained
within
)
each
â€œ
view
â€
of
the
four
databases
will
be
presented
.

Details
concerning
,
and
the
implications
of
,
each
of
the
three
sets
of
analyses
will
be
presented
in
turn
below
.

Summary
statistics
Table
1
presents
the
summary
statistics
.

The
number
of
types
found
in
the
databases
ranges
from
3567
(
C7L6
)
to
5087
(
C12L5
with
a
mean
of
4306
.The
total
number
of
tokens
in
ViewA
of
the
databases
ranges
from
14207
(
C7L6
)
to
14407

(
C23L4
with
a
mean
of
14307
.

The
number
of
tokens
in
ViewB
ranges
from
7099
(
C7L6
)
to
7562
(
C12L5
with
a
mean
of
7353
.

Taking
ViewA
,
the
number
of
types
represents
,
on
average
,
30.47
percent
of
the
total
number
of
tokens
.

Under
ViewB
,
the
number
of
types
represents
,
on
average
,
59.13
percent
of
the
tokens
.

As
rough
estimators
of
MIR
system
characteristics
,
the
preceding
averages
are
very
useful
.

A
search
for
a
given
type
using
a
linear
scan
of
these
databases
would
require
the
comparison
of
,
on
average
,
14307
tokens
(
using
ViewA
)
or
7353
tokens
(
using
ViewB
An
inverted
file
,
or
hash
table
,
would
be
,
on
average
,
31
percent
of
the
length
of
a
ViewA
database
but
nearly
60
percent
of
a
ViewB
database
.

These
values
are
higher
than
one
would
expect
to
find
in
collections
of
text
containing
similar
numbers
of
tokens
.

Estimating
from
a
table
of
token
/
types
presented
by
Schuegraf
and
van
Bommel
(
1993
)
one
would
expect
for
text
of
about
14000
tokens
that
the
index
would
be
20
percent
of
the
size
of
the
collection
.

For
text
of
about
7300
tokens
one
would
expect
an
index
30
percent
the
length
of
the
collection
.

The
difference
between
collections
of
â€œ
musical
words
â€
and
â€œ
real
words
â€
with
regard
to
the
necessary
index
length
indicates
that
â€œ
musical
words
â€
occur
less
frequently
than
â€œ
real
words
The
implications
of
this
will
be
discussed
later
.


-DOCSTART-

Edward
A.
FOX
DepL
of
Computer
Science
VPI
&
State
University

This
system
involves
the
development
of
a
usercentemd
hypermedia
@abase
from
the
computer
science
literature
.

We
will
demonstrate
several
innovative
savens
for
â€œ
digital
library
â€
access
,
that
am
the
result
of
an
ongoing
program
of
user
-
interviewing
,
interface
desi~
usability
testing
,
and
iterative
refinement
,
Searching
,
nXrievaL
and
display
of
ACM
publications
builds
upon
our
wok
with
the
Large
External
object
-
oriented
Network
Database
(
LEND
)
and
MARIAN
systems
,
along
with
extensions
to
the
Z39.50
information
retrieval
protocol
.


-DOCSTART-

The
significance
of
time
in
information
production
and
consumption
has
been
recognised
in
information
retrieval
research
.

Temporal
information
plays
an
important
role
in
the
webpage
retrieval
.

The
webpage
has
both
the
temporal
metadata
and
temporal
semantics
in
the
content
.

However
,
the
existing
search
engines
conduct
the
information
retrieval
based
on
text
keywords
rather
than
temporal
semantics
.

To
address
this
issue
,
a
Temporal
semantics
Information
Retrieval
(
TSIR
)
System
is
proposed
to
deal
with
the
Chinese
temporal
information
retrieval
.

The
TSIR
system
is
deployed
on
Hadoop
and
implemented
by
the
means
of
MapReduce
.

Firstly
,
the
Chinese
temporal
regular
expression
rule
is
introduced
to
extract
the
explicit
and
implicit
temporal
phrases
in
the
query
keywords
and
webpages
.

Secondly
,
the
scores
of
webpages
are
re
-
evaluated
by
taking
text
relevance
and
temporal
semantics
relevance
into
account
and
the
returned
results
are
ranked
according
to
re
-
evaluation
.

Experiment
shows
that
TSIR
system
could
precisely
and
effectively
match
the
keywords
queries
related
to
temporal
expression
.


-DOCSTART-

W
have
been
studying
on
complex
phenomena
from
two
distinct
,
but
complementary
and
supplementary
perspectives
,
namely
generality
and
individuality
,
in
our
research
project
of
the
Aihara
Complexity
Modelling
Project
,
ERATO
,
JST
(
Japan
Science
and
Technology
Agency

Regarding
generality
,
efforts
are
made
to
formulate
mathematical
theory
as
well
as
analysis
methodology
for
modelling
complex
systems
,
putting
great
emphasis
on
bifurcation
analysis
and
nonlinear
time
series
analysis
.

However
,
the
inputs
to
make
such
theory
are
pursued
by
application
studies
of
many
individual
real
-
world
systems
,
from
the
viewpoint
of
mathematical
engineering
.

Both
aspects
of
generalized
theory
and
individual
systems
analysis
are
necessary
and
indispensable
.

Thus
,
information
is
obtained
from
and
fed
back
to
a
wide
range
of
disciplines
:
nonlinear
science
,
information
science
,
life
science
,
engineering
,
social
science
,
and
economics
.

The
applications
of
the
complex
systems
modelling
include
(
1
)
dynamical
computation
of
biological
systems
like
neural
networks
and
the
brain
1â€“5
and
genetic
networks

6â€“12
2
)
a
new
kind
of
computation
by
complex
systems
and
its
softwware
,
hardware
and
wetware

implementations
13â€“17
and
(
3
)
modelling
of
diseases
like
novel
pandemic
influenza
and
prostate
cancer
,
and
mathematical
analysis
,
e.g.
on
bifurcations
peculiar
to
such
hybrid
systems
models

These
applications
show
that
complex
systems
modelling
and
their
mathematical
analysis
are
useful
for
understanding
,
controlling
,
and
harnessing
various
complex
systems
in
this
real
world
.


-DOCSTART-

In
a
typical
spatial
search
problem
mobile
)
users
search
for
(
stationary
or
mobile
)
entities
that
have
spatial
attributes
.

The
user
â€™s
current
location
and/or
the
entitiesâ€™
locations
are
considered
to
assess
the
relevance
of
the
search
result
.

On
one
hand
,
we
believe
that
the
user
â€™s
future
location
is
more
relevant
to
the
search
result
than
the
current
location
.

Hence
,
we
study
spatial
search
queries
under
predictive
models
of
user
locations
.

On
the
other
hand
and
with
the
ubiquity
of
hand
held
devices
,
most
users
do
not
utilize
the
full
power
of
spatial
search
and
they
do
not
know
what
to
search
for
.

Hence
,
we
introduce
a
framework
to
answer
the
question
given
a
user
â€™s
current
and
predicted
locations
,
what
would
the
user
be
interested
in
searching
for
and
seeing
as
a
query
result

More
specifically
,
we
propose
a
predictive
spatial
search
approach
that
continuously
monitors
the
user
â€™s
current
location
to
1
)
predict
the
user
â€™s
future
location
and
integrate
this
prediction
efficiently
in
the
spatial
search
query
processing
pipeline
,
and
(
2
)
predict
the
search
keywords
that
are
of
relevance
to
the
user
,
given
the
user
â€™s
location
and
context
.

The
second
type
of
prediction
leverages
the
knowledge
of
existing
search
engines
about
the
behavior
of
a
global
set
of
spatial
search
users
and
social
media
users
.

Such
twophase
prediction
capability
will
enable
search
engines
to
â€œ
pre
-
search
â€
on
behalf
of
their
users
and
,
thereby
,
leading
to
gains
in
user
experience
,
search
accuracy
,
and
communication
costs
.

Introduction
Commercial
search
engines
recognize
the
value
of
tracking
the
usersâ€™
location
to
provide
a
better
service
.

The
typical
straightforward
approach
is
to
upload
the
user
â€™s
location
every
time
a
search
query
is
issued
.

However
,
the
vision
presented
in
[
1
]
proposes
a
novel
approach
that
utilizes
the
power
of
data
streaming
systems
to
track
the
usersâ€™
locations
based
on
a
routeselection
preference
policy
.

In
this
approach
,
the
system
accepts
(
voluntarily
)
the
user
â€™s
declared
route
preferences
along
with
other
context
-
based
information
.

This
information
helps
the
system
â€œ
predict
â€
the
user
â€™s
location
even
under
the
absence
of
the
real
-
time
location
feed
,
survive
2014
Specialist
Meeting
â€”
Spatial
Search
Ali
,
Hendawi
,
De
Cock
,
Teredesaiâ€”2
offline
periods
,
reduce
communication
cost
and
predict
the
user
â€™s
future
location
with
acceptable
accuracy
.

The
next
set
of
challenges
is
to
identify
the
issues
that
can
help
integrate
spatial
search
capabilities
with
such
predictive
data
streaming
systems
.

This
integration
is
the
theme
of
our
vision
.

Consider
the
scenario
that
a
user
is
driving
on
the
highway
and
searches
for
a
restaurant
.

It
may
be
desirable
and
more
relevant
to
direct
the
user
to
his
favorite
restaurant
chain
(
or
similar
such
chains
that
are
still
several
minutes
ahead
from
his
current
location
)
than
to
simply
identify
a
nearby
less
favorite
restaurant
chain
.

It
is
even
better
to
predict
that
this
user
is
,
for
example
,
heading
to
a
football
game
.

Consequently
,
we
direct
the
user
to
the
parade
,
events
and
activities
next
to
the
stadium
that
the
user
is
not
aware
of
.

In
this
scenario
,
we
notice
two
main
characteristics
:
first
,
the
search
engine
is
geared
up
to
perform
a
spatial
search
around
a
nondeterministic
(
yet
to
be
predicted
)
future
location
ahead
of
time
.

Second
,
the
search
engine
aims
at
discovering
and
recommending
personalized
search
topics
on
a
per
user
basis
.

Ranking
of
the
search
result
needs
to
be
performed
spatiotemporally
taking
into
consideration
the
probability
of
the
user
â€™s
expected
location
along
the
future
timeline
.

Currently
,
users
rely
on
multiple
data
sources
such
as
live
feeds
from
social
media
augmented
with
organic
search
to
discover
related
events
/
facilities
at
the
destination
.

An
advantage
of
the
proposed
predictive
spatial
search
frameworks
is
to
help
consolidate
search
results
better
across
various
faceted
search
channels
.

We
believe
that
future
geospatial
search
engine
will
inevitably
adopt
the
â€œ
we
know
where
you
will
be
and
we
search
before
you
search
â€
principle
.

It
is
the
proactiveness
of
search
engines
that
will
shape
the
future
of
spatial
search
.

The
search
engine
is
the
user
â€™s
agent
that
continuously
(
1
)
adjusts
the
search
result
according
to
the
user
â€™s
predicted
destination
and
(
2
)
interacts
/
socializes
with
neighboring
people
/
agents
to
highlight
interesting
topics
that
the
user
is
totally
unaware
of
.

Predictive
Trees
:

An
Index
for
Predictive
Queries
on
Road
Networks
Practical
experience
tells
that
it
is
absolutely
a
myth
to
assume
that
commercial
search
engines
know
everything
about
the
user
â€™s
past
locations
.

Lots
of
research
utilize
manufactured
databases
of
historical
trajectories
and
apply
machine
learning
techniques
to
predict
the
user
â€™s
future
location
.

These
trajectory
databases
are
usually
collected
by
researchers
or
volunteers
for
research
purposes
.

Yet
,
from
a
practical
perspective
,
and
due
to
privacy
concerns
[
2
the
user
location
is
revealed
on
a
session
basis
such
that
each
session
is
no
longer
than
few
minutes
.

Commercial
search
engines
care
about
usersâ€™
privacy
.

Consequently
,
techniques
that
assume
full
knowledge
of
the
user
â€™s
behavior
over
extended
periods
of
time
are
not
considered
practical
in
our
approach
.

We
propose
a
new
index
structure
,
the
predictive
tree
[
3
,
4
that
enables
the
evaluation
of
predictive
queries
[
5
]
in
the
absence
of
the
objectsâ€™
historical
trajectories
.

Based
solely
on
the
connectivity
of
the
road
network
graph
and
assuming
that
the
object
follows
the
shortest
route
to
destination
,
the
predictive
tree
determines
the
reachable
nodes
of
a
moving
object
within
a
specified
time
window
T
in
the
future
.

Moreover
,
predictive
trees
utilize
every
additional
piece
of
information
and
enhance
the
probability
assignment
of
the
predicted
location
as
more
trajectory
data
becomes
available
on
the
user
.

Specialist
Meeting
â€”
Spatial
Search
Ali
,
Hendawi
,
De
Cock
,
Teredesaiâ€”3
The
predictive
tree
1
)
provides
a
generic
infrastructure
for
answering
the
common
types
of
predictive
queries
including
predictive
point
,
range
,
KNN
,
and
aggregate
queries
2
)
updates
the
probabilistic
prediction
of
the
object
â€™s
future
locations
dynamically
and
incrementally
as
the
object
moves
around
on
the
road
network
,
and
(
3
)
provides
an
extensible
mechanism
to
customize
the
probability
assignments
of
the
object
â€™s
expected
future
locations
,
with
the
help
of
user
defined
functions
.

In
our
ongoing
effort
,
we
leverage
predictive
trees
to
support
spatial
search
and
integrate
this
work
with
predictive
data
streaming
systems
.

References
[
1
]
Mohamed
Ali
,
Badrish
Chandramouli
,
Balan
Raman
,
and
Ed
Katibah
.

Spatio
-
Temporal
Stream
Processing
in
Microsoft
StreamInsight
.

IEEE
Data
Eng
.

69â€“74
(
2010
2
]

From
GPS
and
Virtual
Globes
to
Spatial
Computingâ€”2020
:
The
Next
Transformative
Technology
.

A
Community
Whitepaper
resulting
from
the
2012
CCC
Spatial
Computing
2020
Workshop
3
]
A.
M.
Hendawi
,
J.
Bao
,
and
M.
F.
Mokbel
.

Predictive
Tree
Source
Code
and
Sample
Data
.

URL
:
http
www-users.cs.umn.edu/~hendawi/PredictiveTree
Aug.
2014
4
]
A.
M.
Hendawi
and
M.
F.
Mokbel
.

A
Predictive
Spatio
-
Temporal
Query
Processor
.

In
ACM
SIGSPATIAL
GIS
,
2012
5
]
A.
M.
Hendawi
and
M.
F.
Mokbel
.

Predictive
Spatio
-
Temporal
Queries
:

A
Comprehensive
Survey
and
Future
Directions
.

In
MobiGIS
,
California
,
USA
,
Nov.
2012
.

Specialist
Meeting
â€”

Spatial
Search
Ballatoreâ€”4

The
Search
for
Places
as
Emergent
Aggregates
ANDREA
BALLATORE
Postdoctoral
Researcher
Center
for
Spatial
Studies
University
of
California
,
Santa
Barbara
Email
:
aballatore@spatial.ucsb.edu
earching
for
places
is
the
most
popular
geographic
online
task
,
in
which
names
and
categories
are
used
as
the
main
referents
to
locate
places
in
the
geographic
space
.

By
typing
â€œ
hotels
in
Santa
Barbara
â€
in
any
popular
search
engine
,
the
user
expects
a
list
of
places
matching
a
category
hotel
contained
within
another
place
called
â€œ
Santa
Barbara
Answers
to
such
a
query
can
be
generated
by
relying
on
a
gazetteer
containing
some
form
of
spatial
footprint
for
the
symbol
â€œ
Santa
Barbara
a
database
of
points
-
of
-
interest
categorized
as
â€œ
hotels

â€
and
,
indeed
,
some
strategy
to
compute
the
relevance
of
the
potential
results
.

This
approach
satisfies
welldefined
information
needs
,
but
fails
to
account
for
more
complex
,
nuanced
,
fuzzy
,
and
yet
cognitively
intuitive
questions
about
the
town
.

What
places
are
similar
to
Santa
Barbara
with
respect
to
its
general
atmosphere
â€”
but
perhaps
less
expensive
?

What
other
towns
in
Southern
California
offer
a
comparable
array
of
amenities
?

What
tourist
areas
in
Italy
provide
a
similar
combination
of
mountain
-
related
and
marine
activities
?

Our
current
computational
models
of
place
search
do
not
seem
to
provide
easy
answers
.

Our
intimate
familiarity
with
place
clashes
with
the
difficulty
of
dealing
with
it
computationally
.

Because
of
its
centrality
in
human
cognition
and
culture
,
the
notion
of
place
is
unsurprisingly
characterized
by
high
polysemy
,
strong
context
-
dependence
,
and
innumerable
metaphorical
uses
,
carving
social
meanings
from
neutral
,
unbounded
spaces
(
Agnew
,
2011
The
intellectual
prominence
of
place
has
waxed
and
waned
over
time
,
being
obscured
for
centuries
by
more
abstract
notions
of
space
,
and
making
a
reappearance
in
recent
decades
(
Casey
,
1997
Being
intensely
debated
in
the
social
sciences
and
the
humanities
,
place
and
its
representations
have
now
become
an
active
research
frontier
in
geographic
information
science
(
Goodchild
,
2011

In
this
area
,
a
central
concern
is
that
place
names
are
often
ambiguous
,
vague
,
and
vernacular
.

Place
categories
are
culturally
-
dependent
,
arbitrary
,
and
inconsistently
applied
.

More
strikingly
,
places
are
implicitly
assumed
to
have
a
name
and
to
fit
,
at
least
to
some
degree
,
known
categories
.

Current
efforts
focus
on
more
sophisticated
place
-
name
interpretation
in
text
documents
(
Purves
,
and
Jones
,
2011
new
reference
theories
tailored
to
place
(
Scheider
and
Janowicz
,
2014
and
semantically
more
expressive
gazetteers
(
KeÃŸler
et
al
2009

In
a
complementary
approach
,
I
advocate
a
view
of
place
as
an
aggregate
of
objects
and
processes
that
interact
at
a
given
scale
,
inter
-
locked
by
spatial
colocation
.

This
view
of
place
relies
on
the
discovery
of
implicit
relations
,
and
not
on
some
explicit
labels
assigned
by
an
observer
.

The
approach
relies
on
some
assumptions
:
Places
are
inescapably
multi
-
faceted
(
comprising
diverse
processes
they
are
socially
constructed
(
emerging
as
the
result
of
human
agency
and
practices
relational
(
emerging
in
a
context
,
not
in
a
vacuum
scale
-
dependent
(
different
places
exist
at
S
2014
Specialist
Meeting
â€”

Spatial
Search
Ballatoreâ€”5
different
scales
and
they
are
dynamic
(
emerging
,
changing
,
and
ultimately
disappearing
Following
Thrift
(
1999
I
regard
places
as
emergent
entities
in
a
complex
,
non
-
linear
system
,
and
they
appear
as
assemblages
of
heterogeneous
things
that
meet
in
space
and
time
.

Place
can
be
fruitfully
viewed
through
a
holistic
lens
,
emphasizing
its
contextuality
and
inherent
interconnectedness
,
rather
than
as
an
object
in
isolation
(
Ballatore
et
al
2012

In
practical
terms
,
this
approach
aims
at
supporting
multi
-
faceted
,
context
-
dependent
aggregate
search
,
going
beyond
the
current
forms
of
name
-
based
search
for
well
-
defined
,
individual
places
.

In
this
sense
,
places
can
be
searched
for
on
the
basis
of
their
emergent
distributional
characteristics
,
rather
than
in
an
arbitrary
,
crisp
categorization
(
e.g
city
or
town

For
example
,
a
Japanese
tourist
in
San
Francisco
might
search
for
places
in
which
architectural
landmarks
co
-
occur
with
museums
and
galleries
,
or
for
places
that
,
as
an
aggregate
,
present
similar
characteristics
to
Shibuya
,
the
Tokyo
shopping
district
she
is
familiar
with
.

Text
Information
Retrieval
Place
-
as
-
aggregate
Search
Vector
space
Set
of
text
documents

Geographic
space
Vector
A
document
as
a
sequence
of
words
A
place
as
an
aggregate
of
spatially
located
objects
Dimensions
Words
(
high
dimensionality
)
Characteristics
of
objects
(
dimensionality
defined
by
application
,
potentially
very
high
)

Index
Sparse
document
-
word
matrix
Sparse
object
-
to
-
object
colocation
matrix

Search
Weighted
keyword
matching
,
topic
models
,
similarity
Colocation
queries
,
query
-
by
-
place
Table
1
:

Place
search
overview

Computationally
,
this
approach
to
place
search
can
be
modeled
in
analogy
to
traditional
information
retrieval
in
a
vector
space
model
,
as
summarized
in
Table
1
.

Given
a
geographic
space
,
treated
as
a
corpus
containing
a
potentially
infinite
number
of
place
-
as
-
aggregate
,
the
proposed
approach
has
a
number
of
opportunities
and
challenges
to
be
tackled
.

The
efficient
computation
of
spatial
colocation
at
a
large
scale
â€”
the
identification
of
categories
of
objects
that
co
-
occur
spatially
(
and
temporally
)
in
non
-
random
patterns
â€”
is
an
open
problem

(
Cromley
et
al
2014

As
many
alternative
places
-
as
-
aggregates
can
encompass
the
same
entities
at
different
scales
,
new
heuristics
are
needed
to
construct
optimal
aggregates
that
meet
user
informational
needs
at
a
given
scale
,
based
on
statistical
measures
of
informational
entropy
.

As
I
hope
I
have
demonstrated
,
the
uneasy
relationship
between
space
and
place
offers
opportunities
for
unlocking
new
ways
of
searching
the
ocean
of
geo
-
information
.

References
Agnew
,
J
2011
Space
and
Place
.

In
J.
Agnew
D.
Livingstone
(
Eds
Handbook
of
Geographical
Knowledge
(
pp
.
316â€“330
London
:
Sage
Publications
.

Casey
,
E.
S

The
fate
of
place
:
A
philosophical
history
.

Berkeley
,
CA
:
University
of
California
Press
.

Cromley
,
R.
G
Hanink
,
D.
M
Bentley
,
G.
C
2014

Geographically
Weighted
Colocation
Quotients
:
Specification
and
Application
.

The
Professional
Geographer

Specialist
Meeting
â€”

Spatial
Search
Ballatoreâ€”6
Goodchild
,
M.
F

2011
Formalizing
Place
in
Geographic
Information
Systems
.

In
L.
M.
Burton
,
S.
A.
Matthews
,
M.
Leung
,
S.
P.
Kemp
D.
T.
Takeuchi
(
Eds
Communities
,
Neighborhoods
,
and
Health
(
pp
.

21â€“33
New
York
:
Springer
.

KeÃŸler
,
C
Janowicz
,
K
Bishr
,
M
2009

An
Agenda
for
the
Next
Generation
Gazetteer
:
Geographic
Information
Contribution
and
Retrieval
.

In
Proceedings
of
ACM
GIS
â€™09
(
pp
.

New
York
:
ACM
.

Purves
,
R
Jones
,
C
2011
Geographic
information
retrieval
.

SIGSPATIAL
Special
3(2
2â€“4
.

Ballatore
,
A
Wilson
,
D.
C
Bertolotto
,
M
2012
A
holistic
semantic
similarity
measure
for
viewports
in
interactive
maps
.

In
S.
Di
Martino
,
A.
Peron
T.
Tezuka
(
Eds
Web
and
Wireless
Geographical
Information
Systems
(
pp
.

151â€“166
Berlin
:
Springer
.

Scheider
,
S
Janowicz
,
K
2014
Place
reference
systems
.

Applied
Ontology
9
:
97â€“127
.

Thrift
,
N
1999
Steps
to
an
Ecology
of
Place
.

In
D.
Massey
,
J.
Allen
P.
Sarre
(
Eds
Human
Geography

295â€“322
Cambridge
,
UK
:
Polity
Press
.

Specialist
Meeting
â€”

Spatial
Search
Card
et
al.â€”7

The
VERP
Explorerâ€”
A
Tool
for
Applying
Recursion
Plots
to
the
Eye
-
Movements
of
Visual
-
Cognitive
Tasks
STUART
K.
CARD
Consulting
Professor
Department
of
Computer
Science
Stanford
University
Email
:
scard@cs.stanford.edu
with
Ã‡AÄžATAY
DEMIRALP
Post
Graduate
Fellow
Department
of
Computer

Science
Stanford
University
Email
:

cagaytay@cs.stanford.edu
JESSE
CIRIMELE
Graduate
Student
Department
of
Computer
Science
Stanford
University
Email
:
cirimele@cs.stanford.edu
esigns
in
human
-
computer
interaction
(
HCI
)
often
involve
trading
between
spatial
and
textual
representations
to
achieve
a
nuance
of
representation
that
makes
a
task
faster
to
execute
,
easier
to
learn
,
or
less
prone
to
error
.

Such
designs
can
be
very
effective
,
but
they
can
also
be
subtle
,
and
it
can
be
difficult
to
understand
the
mechanisms
in
play
.

Even
generally
successful
interfaces
can
still
hide
bad
combinations
of
interface
,
task
,
and
context
that
could
be
improved
were
they
identified
.

One
method
of
approaching
this
problem
is
to
run
chronometric
experiments
with
contrasting
conditions
.

Aside
from
being
expensive
for
development
work
,
this
method
is
at
such
an
aggregate
level
that
it
often
does
not
provide
much
access
or
insight
into
the
underlying
mechanisms
at
work
.

Another
method
is
cognitive
simulation
(
Kieras
,
2014

The
intent
is
to
specify
the
likely
mechanisms
at
work
and
to
validate
them
by
their
ability
to
predict
chronometric
or
other
data
.

The
validated
simulator
can
then
be
put
to
work
on
inferring
other
consequences
of
the
design
with
some
claim
to
knowing
why
.

While
this
method
has
advantages
,
it
is
even
more
expensive
and
is
most
practical
for
large
projects
or
projects
close
to
an
existing
model
that
can
provide
a
starting
point
.

The
VERP
Explorer
.

Specialist
Meeting
â€”
Spatial
Search
Card
et
al.â€”8

A
third
method
is
to
construct
a
tool
that
makes
the
mechanisms
at
work
visible
by
when
applied
to
samples
of
user
behavior
.

In
this
paper
,
we
propose
such
a
method
and
tool
,
The
VERP
Explorer
(
VERP
stands
for
Visualization
of
Eye
-
Movements
based
on
Recurrence
Plots
an
interactive
visualization
based
recurrence
plots
.

Eye
-
movement
sequences
are
taken
of
users
performing
visual
-
cognitive
tasks
with
the
subject
system
.

These
are
mapped
into
recurrence
plot
visualizations
to
highlight
patterns
of
quasi
-
sequential
behavior
.

In
our
system
,
these
patterns
are
then
back
-
mapped
into
â€”
and
overlaid
on
â€”
the
eye
-
movement
scene
to
help
characterize
and
provide
insights
into
the
behavior
.

Recurrence
plots
are
a
type
of
non
-
linear
analysis
that
has
been
used
in
the
study
of
dynamical
systems
and
other
areas
(
Eichmann
et
al
1978
;
Marwan
,
2008

Recently
it
has
been
applied
to
eye
-
movements
(
Anderson
et
al
,
2013

Our
tool
extends
and
integrates
eye
-
movement
and
recursion
plot
analysis
into
an
interactive
tool
,
simplifying
exploratory
analysis
.

Eye
-
movements
can
be
thought
of
as
a
sequence
of
eye
gaze
positions
fi
parameterized
by
time
.

To
obtain
the
matrix
that
is
the
basis
for
a
recurrence
plot
,
we
start
with
the
first
eye
-
position
f1
and
compare
it
to
all
the
other
eye
-
positions
in
the
sequence
,
including
itself
.

If
the
distance
d
(
fij
between
the
two
compared
eye
positions
is
within
some
small
distance
then
we
put
a
1
at
that
position
in
the
matrix
,
otherwise
a
0
.
rij
1
,
d
(
fij
0
,
otherwise


-DOCSTART-

We
describe
our
approach
and
results
towards
the
Hyperlinking
sub
-
task
at
MediaEval
2012
.

We
approached
this
as
an
Information
Retrieval
task
and
used
re
-
ranking
strategies
for
finding
relevant
videos
.

A
three
-
step
approach
was
then
applied
on
results
to
extract
the
most
relevant
part
of
the
video
regarding
the
query
content
.

Our
results
show
that
reranking
strategies
and
integration
of
metadata
information
both
improve
the
system
performance
.


-DOCSTART-

The
F
-
measure
,
originally
introduced
in
information
retrieval
,
is
nowadays
routinely
used
as
a
performance
metric
for
problems
such
as
binary
classification
,
multi
-
label
classification
,
and
structured
output
prediction
.

Optimizing
this
measure
remains
a
statistically
and
computationally
challenging
problem
,
since
no
closed
-
form
maximizer
exists
.

Current
algorithms
are
approximate
and
typically
rely
on
additional
assumptions
regarding
the
statistical
distribution
of
the
binary
response
variables
.

In
this
paper
,
we
present
an
algorithm
which
is
not
only
computationally
efficient
but
also
exact
,
regardless
of
the
underlying
distribution
.

The
algorithm
requires
only
a
quadratic
number
of
parameters
of
the
joint
distribution
(
with
respect
to
the
number
of
binary
responses
We
illustrate
its
practical
performance
by
means
of
experimental
results
for
multi
-
label
classification
.


-DOCSTART-

As
the
email
service
is
becoming
an
important
communication
way
on
the
Network
,
the
spam
is
increasing
every
day
.

This
paper
describes
a
new
filtering
model
based
on
email
content
by
using
Back
-
Propagation
Neural
Networks
(
BPNN

And
for
the
Chinese
email
,
it
uses
Natural
Language
Processing
Information
Retrieval
Sharing
Platform
(
NLPIR
)
system
to
perform
Chinese
word
segmentation
.

The
simulation
results
show
that
this
model
can
precisely
filter
the
Chinese
spam
.


-DOCSTART-

Earlier
work
has
identified
the
potential
for
reuse
and
reproducibility
when
applying
workflow
systems
to
audio
analysis
and
Music
Information
Retrieval
.

In
this
paper
we
extend
this
approach
with
the
introduction
of
Research
Objects
to
capture
semantic
information
about
the
use
of
workflows
within
the
audio
research
and
development
process
.

Once
aggregated
,
the
metadata
encapsulated
in
a
Research
Object
can
be
used
to
manage
and
disseminate
research
output
,
providing
a
well
structured
foundation
for
meeting
the
needs
of
reproducibility
.

We
report
on
the
development
and
deployment
of
a
software
suite
that
practically
applies
this
notion
of
Research
Objects
to
capture
the
semantics
surrounding
the
use
of
an
audio
processing
workflow
,
and
reflect
upon
how
this
might
be
further
integrated
with
lower
level
semantics
from
the
audio
processing


-DOCSTART-

Mining
geo
-
spatial
data
is
an
important
task
in
many
application
domains
,
such
as
environmental
science
,
geographic
information
science
,
and
social
networks
.

In
this
paper
,
we
introduce
a
data
mining
framework
,
which
includes
pre
-
processing
of
environmental
and
geo
-
spatial
data
,
geo
-
spatial
data
mining
techniques
,
and
visual
analysis
of
environmental
and
geo
-
spatial
data
.

In
particular
,
we
propose
new
density
-
based
clustering
algorithms
to
identify
interesting
distribution
patterns
from
geo
-
spatial
data
,
a
change
pattern
discovery
technique
to
detect
dynamic
change
patterns
within
spatial
clusters
,
and
a
post
-
processing
technique
to
extract
interesting
patterns
and
useful
knowledge
from
geo
-
spatial
data
.

Our
density
-
based
clustering
algorithms
are
based
on
the
well
-
established
density
-
based
shared
nearest
neighbor
clustering
algorithm
,
which
can
find
clusters
of
different
shape
,
size
,
and
densities
in
high
-
dimensional
data
.

The
post
-
processing
analysis
technique
allows
automatic
screening
of
interesting
spatial
clusters
.

The
change
pattern
discovery
algorithm
is
able
to
detect
and
analyze
dynamic
patterns
of
changes
within
spatial
clusters
.

This
paper
focuses
on
developing
a
framework
integrating
a
sequence
of
data
mining
process
including
clustering
algorithm
,
analysis
technique
and
pattern
changing
discovery
algorithm
.

In
contrast
to
previous
works
in
this
area
,
our
approaches
can
cluster
and
analyze
dynamically
evolved
complex
objects
,
i.e
polygons
.

We
evaluate
the
effectiveness
of
our
techniques
through
a
challenging
real
case
study
involving
ozone
pollution
events
in
the
Houston
â€“
Galveston
â€“
Brazoria
area
.

The
experimental
results
show
that
our
approaches
can
discover
interesting
patterns
and
useful
information
from
geo
-
spatial
air
-
quality
data
.


-DOCSTART-

We
have
developed
a
distributed
search
engine
,
Cooperative
Search
Engine
(
CSE
)
to
retrieve
fresh
information
.

In
CSE
,
a
local
search
engine
located
in
each
web
server
makes
an
index
of
local
pages
.

And
,
a
Meta
search
server
integrates
these
local
search
engines
to
realize
a
global
search
engine
.

In
such
a
way
,
the
communication
delay
occurs
at
retrieval
.

So
,
we
have
developed
several
speedup
techniques
in
order
to
realize
real
time
retrieval
.

In
addition
,
the
meta
server
is
a
single
point
of
failure
in
CSE
.

So
,
we
introduce
redundancy
of
the
meta
search
server
increase
availability
of
CSE
.

In
this
paper
,
we
describe
scalability
and
reliability
of
CSE
and
their
evaluations
.


-DOCSTART-

Seamless
multivariate
affine
errorin
-
variables
transformation
and
its
application
to
map
rectification
Bofeng
Li

a
b
Yunzhong
Shen
a
c
Xingfu
Zhang

Chuang
Li
a
Lizhi
Lou
a
a
College
of
Surveying
and
Geo
-
Informatics
,
Tongji
University
Shanghai
PR
China
b
State
Key
Laboratory
of
Geo
-
information
Engineering
,
Xi'an
Research
of
Surveying
and
Mapping
Xian
PR
China
c
Centre
for
Spatial
Information
Science
and
Sustainable
Development
,
Tongji
University
Shanghai
PR
China

d
Department
of
Surveying
and
Geomatics
Guangdong
University
of
Technology
Guangzhou
PR
China
Published
online
:
14
Apr
2013
.


-DOCSTART-

An
important
area
of
environmental
science
involves
the
combination
of
information
from
diverse
sources
relating
to
a
similar
endpoint
.

Majority
of
optical
remote
sensing
techniques
used
for
marine
oil
spills
detection
have
been
reported
lately
of
having
high
number
of
false
alarms
(
oil
slick
look
-
a
-
likes
)
phenomena
which
give
rise
to
signals
which
appear
to
be
oil
but
are
not
.

Suggestions
for
radar
image
as
an
operational
tool
has
also
been
made
.

However
,
due
to
the
inherent
risk
in
these
tools
,
this
paper
presents
the
possible
research
directions
of
combining
statistical
techniques
with
remote
sensing
in
marine
oil
spill
detection
and
estimation
.

DOI
:
10.4018/joris.2013010105
International
Journal
of
Operations
Research
and
Information
Systems
,

4(1
84
-
111
,
January
-
March
2013
85

Copyright
2013
,
IGI
Global
.

Copying
or
distributing
in
print
or
electronic
forms
without
written
permission
of
IGI
Global
is
prohibited
.

also
extensively
used
for
the
detection
of
oil
spills
in
the
marine
environment
,
as
they
are
independent
from
sun
light
and
are
not
affected
by
cloudiness
,
they
cover
large
areas
and
are
more
cost
-
effective
than
air
patrolling
(
Keramitsoglou
et
al
2006

However
,
there
are
problems
with
the
majority
of
optical
remote
sensing
techniques
used
for
oil
spill
detection
which
is
the
high
number
of
false
alarms
(
oil
slick
look
-
a
-
likes
)
phenomena
which
give
rise
to
signals
which
appear
to
be
oil
but
are
not
(
Mercier
Girard
-
Ardhuin
,
2006
;
Topouzelis
et

It
is
believed
that
visible
satellite
systems
are
susceptible
to
false
alarms
due
to
sun
glint
,
wind
sheen
,
bottom
features
,
cloud
shadows
,
and
biogenic
material
such
as
surface
weeds
and
sunken
kelp
beds
(
Serra
-
Sogas
,
2008
;
Oâ€™Neil
,
1983
;
Goodman
,
1989
,
1992
;
Schnell
,
1992
;
Goodman
,
1988
;
1989

According
to
Mansor
et
al
2010
there
are
certain
times
when
visual
techniques
and
optical
satellite
image
are
unsuitable
for
mapping
of
oil
spill
;
it
is
in
these
cases
where
radar
remote
sensing
is
required
.

These
situations
include
spills
covering
vast
areas
of
the
marine
environment
,
and
when
the
oil
can
not
be
seen
or
discriminated
against
the
background
.

The
discrimination
of
oil
in
these
circumstances
presents
several
unique
problems
.

The
remotely
sensed
data
collected
in
these
situations
often
provide
complex
signatures
,
which
must
be
deciphered
in
order
to
locate
the
spilled
oil
.

Environmental
conditions
such
as
precipitation
,
fog
,
and
the
amounts
of
daylight
present
also
may
pose
problems
especially
in
optical
images
.

Given
the
complexity
of
remote
sensing
tool
and
due
to
the
risk
inherent
in
dependence
on
radar
images
,
there
is
a
sustainable
benefit
in
aiding
the
combination
of
different
tools
for
spill
detection
and
estimation
.

Thus
,
we
proposed
a
pluralistic
strategy
known
as
coherent
pluralism
(
Jackson
,
1999
)
that
involves
the
combination
of
remote
sensing
with
sound
statistical
techniques
as
essential
tool
.

Considering
this
as
a
linear
system
,
the
important
aspect
of
this
proposal
is
the
application
of
Bayesian
classification
,
Gaussian
error
analysis
,
and
statistical
estimation
for
the
linear
system
.

THE
CONCEPTS
OF
COHERENT
PLURALISM

An
important
area
of
environmental
science
involves
the
combination
of
information
from
diverse
sources
relating
to
a
similar
endpoint
.

A
common
rubric
for
combining
the
results
of
independent
studies
is
to
apply
a
meta
-
analysis
(
Piegorsch
Bailer
,
2005

The
term
suggests
a
move
past
an
analysis
of
standalone
data
or
of
a
single
analysis
of
pooled
,
multisource
data
,
to
one
incorporating
and
synthesizing
information
from
many
associated
sources
.

It
was
first
coined
by
Glass
(
1976
)
in
an
application
combining
results
in
multiple
social
science
studies
and
is
now
quite
common
in
many
social
and
biomedical
applications
(
Wang
Wall
,
2003
;
Piegorsch
Cox
,
1996

The
typical
goal
of
this
so
called
metaanalysis
is
to
consolidate
outcomes
of
independent
studies
,
reanalyze
the
possibly
disparate
results
within
the
context
of
their
common
endpoints
,
increase
the
sensitivity
of
the
analysis
to
detect
the
presence
of
environmental
effects
,
and
provide
a
quantitative
analysis
of
the
phenomenon
of
interest
based
on
the
combined
data
.

The
result
is
often
a
pooled
estimate
of
the
overall
effect
.

For
example
,
when
assessing
environmental
risks
of
some
chemical
hazard
,
it
is
increasingly
difficult
for
a
single
,
large
,
well
designed
ecological
or
toxicological
evaluation
to
assess
definitively
the
risk(s
)
of
exposure
to
the
chemical
.

This
informs
our
proposition
of
coherent
pluralism
(
meta
-
analysis
which
involves
the
combination
of
remote
sensing
technique
and
statistical
methods
in
spills
detection
and
estimation
.

Pluralism
,
according
to
Jackson
(
1999
)
can
be
viewed
as
the
use
of
different
methodologies
,
methods
and/or
techniques
in
combination
.

Jackson
â€™s
explanation
put
more
light
on
the
need
for
coherent
pluralism
in
practice
as
pluralism
encourages
flexibility
in
use
of
the
widest
variety
of
methods
,
models
,
tools
and
techniques
in
any
intervention
.

In
Operations
Research/
Management
Science
field
,
the
concepts
of
coherent
pluralism
have
gained
a
wider
ground
.

Many
in
literature
are
the
works
26
more
pages
are
available
in
the
full
version
of
this
document
,
which
may
be
purchased
using
the
"
Add
to
Cart
"
button
on
the
product
's
webpage
:
www.igi-global.com/article/optimal-detection-estimationmarine-oil/76674?camid=4v1
This
title
is
available
in
InfoSci
-
Journals
,
InfoSci
-
Journal
Disciplines
Business
,
Administration
,
and
Management
.

Recommend
this
product
to
your
librarian
:
www.igi-global.com/e-resources/libraryrecommendation/?id=2


-DOCSTART-

A
major
problem
concerning
the
reusability
of
software
is
the
retrieval
of
software
components
.

Different
approaches
have
been
followed
to
solve
this
problem
.

In
this
paper
we
present
the
Reuse
Assistant
,
a
hybrid
approach
to
support
the
retrieval
of
software
components
from
a
library
of
object
classes
.

The
Reuse
Assistant
consists
of
two
subsystems
that
follow
two
different
approaches
:
information
retrieval
techniques
based
on
statistical
methods
,
and
knowledge
-
based
techniques
using
some
of
the
representation
and
indexing
mechanisms
found
in
case
-
based
systems
.

The
Information
Retrieval
approach
grants
system
extendibility
,
and
permits
the
use
of
a
natural
language
interface
.

The
Case
-
Based
approach
enables
reasoning
about
concepts
,
allowing
the
retrieval
of
â€œ
approximate
â€
components
.

Both
subsystems
can
be
operated
from
a
common
interface
,
where
free
-
text
and
form
-
filling
queries
can
be
posed
.


-DOCSTART-

Recently
,
the
use
of
social
media
for
health
information
exchange
is
expanding
among
patients
,
physicians
,
and
other
health
care
professionals
.

In
medical
areas
,
social
media
allows
non
-
experts
to
access
,
interpret
,
and
generate
medical
information
for
their
own
care
and
the
care
of
others
.

Researchers
paid
much
attention
on
social
media
in
medical
educations
,
patient
â€“
pharmacist
communications
,
adverse
drug
reactions
detection
,
impacts
of
social
media
on
medicine
and
healthcare
,
and
so
on
.

However
,
relatively
few
papers
discuss
how
to
extract
useful
knowledge
from
a
huge
amount
of
textual
comments
in
social
media
effectively
.

Therefore
,
this
study
aims
to
propose
a
Fuzzy
adaptive
resonance
theory
network
based
Information
Retrieval
(
FIR
)
scheme
by
combining
Fuzzy
adaptive
resonance
theory
(
ART
)
network
,
Latent
Semantic
Indexing
(
LSI
and
association
rules
(
AR
)
discovery
to
extract
knowledge
from
social
media
.

In
our
FIR
scheme
,
Fuzzy
ART
network
firstly
has
been
employed
to
segment
comments
.

Next
,
for
each
customer
segment
,
we
use
LSI
technique
to
retrieve
important
keywords
.

Then
,
in
order
to
make
the
extracted
keywords
understandable
,
association
rules
mining
is
presented
to
organize
these
extracted
keywords
to
build
metadata
.

These
extracted
useful
voices
of
customers
will
be
transformed
into
design
needs
by
using
Quality
Function
Deployment
(
QFD
)
for
further
decision
making
.

Unlike
conventional
information
retrieval
techniques
which
acquire
too
many
keywords
to
get
key
points
,
our
FIR
scheme
can
extract
understandable
metadata
from
social
media
.


-DOCSTART-

With
the
development
of
wireless
networks
and
mobile
terminals
,
mobile
commerce
has
become
a
research
hotspot
in
recent
years
for
its
commercial
value
and
has
been
considered
to
be
significant
supplement
and
potential
substitute
of
traditional
e
-
commerce
.

Tourism
,
which
has
the
feature
of
mobility
,
is
a
typical
mobile
commerce
application
.

In
this
paper
,
we
propose
a
RFID
and
personalized
recommendation
based
tourism
information
system
.

In
this
system
,
RFID
tags
together
with
RFID
readers
provide
user
identification
.

And
implicit
feedbacks
(
including
operations
on
short
messages
and
web
pages
,
and
user
locations
and
purchase
records
)
based
recommendation
improves
information
retrieval
efficiency
and
user
satisfaction
greatly
.

System
overview
and
issues
on
RFID
,
personalized
recommendation
and
scalable
multimedia
coding
in
proposed
tourism
information
system
are
discussed
respectively
.

Furthermore
,
some
possible
future
work
is
presented
.


-DOCSTART-

Nowadays
,
one
of
crucial
problems
of
the
Semantic
Web
is
to
offer
a
simple
and
convenient
access
to
knowledge
bases
and
ontologies
.

Advances
in
semantic
search
have
been
delayed
because
of
the
complexity
of
nRQL
like
query
languages
,
as
well
as
the
ambiguities
of
the
Natural
Language
(
NL
To
go
beyond
these
difficulties
,
we
propose
an
approach
that
aims
to
support
mechanisms
and
techniques
for
analyzing
and
processing
a
natural
language
query
.

As
result
,
we
obtain
an
intermediate
representation
in
description
logics
,
easy
to
be
interpreted
in
any
query
language
oriented
information
retrieval
,
as
nRQL
.

This
system
is
composed
of
four
basic
modules
that
provide
a
sequential
processing
of
the
query
expressed
in
NL
until
the
expected
results
.


-DOCSTART-

Due
to
the
constantly
growing
number
of
information
sources
,
intelligent
information
retrieval
becomes
a
more
and
more
important
task
.

We
model
information
sources
by
description
logic
(
DL
)
terminologies
.

The
commonalities
of
user
-
speciied
examples
can
be
computed
by
the
least
common
subsumer
(
LCS
)
operator
.

However
,
in
some
cases
this
operator
delivers
too
general
results
.

In
this
article
we
solve
this
problem
by
presenting
a
probabilistic
extension
of
the
LCS
operator
for
a
probabilistic
description
logic
.

By
computing
gradual
commonalities
between
description
logic
concepts
,
this
operator
serves
as
a
crucial
means
for
content
-
based
information
retrieval
for
all
kinds
of
information
sources
.

We
also
describe
an
extension
of
our
operator
to
consider
unwanted
information
.

The
probabilistic
LCS
can
be
applied
for
information
retrieval
in
a
scenario
of
multiple
information
sources
.

The
number
of
structured
but
heterogeneous
information
sources
that
are
available
online
is
growing
rapidly
.

In
particular
,
many
sources
in
the
WorldWide
Web
ooer
information
about
all
kinds
of
themes
.

Often
the
user
must
manually
combine
information
items
from
multiple
sources
.

If
information
is
distributed
in
diierent
semi
-
structured
formats
(
see
the
XML
discussion
automatic
integration
techniques
are
required
to
provide
adequate
information
systems
.

Basically
,
the
same
situation
occurs
in
standard
database
contexts
,
and
thus
,
many
of
the
well
-
known
integration
techniques
can
be
reused
in
the
Web
context
(
see
,
e.g
CL93

As
a
remedy
to
the
integration
and
combination
problems
,
the
"
information
agent
"
abstraction
has
been
proposed
(
e.g
SIMS
AKS96
Information
agents
are
understood
as
systems
that
provide
a
uniform
query
interface
to
multiple
information
sources
.

In
the
Web
context
,
most
users
of
information
systems
are
only
casual
users
.

Hence
,
they
are
often
overtaxed
when
asked
to
(
formally
)
describe
the
exact
kind
of
information
they
desire
.

In
many
applications
they
can
,
however
,
supply
examples
concerning
the
information
of
interest
.

In
contrast
to
approaches
where
the
user
has
to
learn
query
languages
(
or
agent
communication
languages
in
this
paper
we
focus
on
providing
the
theoretical
background
for
information
retrieval
on
the
basis
of
user
-
speciied
examples
which
express
his
information
demands
.

An
information
system
can
automatically
determine
a
description
of
the
user
's
information
demands
by
evaluating
the


-DOCSTART-

In
recent
years
,
the
information
retrieval
(
IR
)
community
has
witnessed
the
first
successful
applications
of
deep
neural
network
models
to
short
-
text
matching
and
ad
-
hoc
retrieval
tasks
.

However
,
the
two
communities
focused
on
deep
neural
networks
and
on
IR
have
less
in
common
when
it
comes
to
the
choice
of
programming
languages
.

Indri
,
an
indexing
framework
popularly
used
by
the
IR
community
,
is
written
in
C
while
Torch
,
a
popular
machine
learning
library
for
deep
learning
,
is
written
in
the
light
-
weight
scripting
language
Lua
.

To
bridge
this
gap
,
we
introduce
Luandri
(
pronounced
i
>
laundry</i
a
simple
interface
for
exposing
the
search
capabilities
of
Indri
to
Torch
models
implemented
in
Lua
.


-DOCSTART-

This
paper
presents
the
2007
MIRACLE
â€™s
team
approach
to
the
AdHoc
Information
Retrieval
track
.

The
main
work
carried
out
for
this
campaign
has
been
around
monolingual
experiments
,
in
the
standard
and
in
the
robust
tracks
.

The
most
important
contributions
have
been
the
general
introduction
of
automatic
named
-
entities
extraction
and
the
use
of
wikipedia
resources
.

For
the
2007
campaign
,
runs
were
submitted
for
the
following
languages
and
tracks
:
a
)
Monolingual
:
Bulgarian
,
Hungarian
,
and
Czech
.

Robust
monolingual
:
French
,
English
and
Portuguese
.


-DOCSTART-

A
parallel
corpus
is
an
essential
resource
for
statistical
machine
translation
(
SMT
)
but
is
often
not
available
in
the
required
amounts
for
all
domains
and
languages
.

An
approach
is
presented
here
which
aims
at
producing
parallel
corpora
from
available
comparable
corpora
.

An
SMT
system
is
used
to
translate
the
source
-
language
part
of
a
comparable
corpus
and
the
translations
are
used
as
queries
to
conduct
information
retrieval
from
the
target
-
language
side
of
the
comparable
corpus
.

Simple
filters
are
then
used
to
score
the
SMT
output
and
the
IR
-
returned
sentence
with
the
filter
score
defining
the
degree
of
similarity
between
the
two
.

Using
SMT
system
output
gives
the
benefit
of
trying
to
correct
one
of
the
common
errors
by
sentence
tail
removal
.

The
approach
was
applied
to
Arabic
â€“
English
and
French
â€“
English
systems
using
comparable
news
corpora
and
considerable
improvements
were
achieved
in
the
BLEU
score
.

We
show
that
our
approach
is
independent
of
the
quality
of
the
SMT
system
used
to
make
the
queries
,
strengthening
the
claim
of
applicability
of
the
approach
for
languages
and
domains
with
limited
parallel
corpora
available
to
start
with
.

We
compare
our
approach
with
one
of
the
earlier
approaches
and
show
that
our
approach
is
easier
to
implement
and
gives
equally
good
improvements
.


-DOCSTART-

In
computer
graphics
(
CG
making
photorealistic
images
using
a
computer
is
now
commonplace
.

As
a
result
,
directors
can
create
convincing
,
imaginary
worlds
,
and
designers
can
virtually
prototype
,
visualize
,
and
evaluate
potential
products
and
spaces
.

In
order
to
achieve
these
purposes
,
rendering
methods
such
as
ray
tracing
and
radiosity
rendering
methods
have
been
developed
.

However
,
the
rendering
of
more
photorealistic
images
requires
both
accurate
object
surface
reflectance
parameters
and
object
surface
geometries
to
be
obtained
.

Therefore
,
in
augmented
virtuality
,
it
is
important
to
estimate
surface
reflectometry
and
surface
geometry
from
real
objects
or
scenes
.

In
particular
,
object
surface
reflectometry
estimation
is
of
primary
importance
because
,
unlike
object
surface
geometry
,
which
can
be
measured
using
a
range
finder
,
no
device
has
been
developed
to
measure
the
variation
of
object
surface
reflectance
properties
.

The
present
study
investigates
the
problem
of
object
surface
reflectance
estimation
,
which
is
sometimes
referred
to
as
inverse
reflectometry
,
for
photorealistic
rendering
and
effective
multimedia
applications
.

A
number
of
methods
have
been
developed
for
estimating
object
surface
reflectance
properties
in
order
to
render
real
objects
under
arbitrary
illumination
conditions
.

However
,
it
is
difficult
to
densely
estimate
surface
reflectance
properties
faithfully
for
complex
objects
with
interreflections
.

This
thesis
describes
three
new
methods
for
densely
estimating
the
non
-
uniform
surface
reflectance
properties
of
real
objects
constructed
of
convex
and
concave
surfaces
.

Specifically
,
we
use
registered
range
and
surface
color
texture
images
obtained
by
a
laser
rangefinder
.

The
proposed
methods
determine
the
positions
of
light
sources
in
order
to
capture
color
Ph
.
D.
Dissertation
,
Department
of
Information
Systems
,
Graduate
School
of
Information
Science
,
Nara
Institute
of
Science
and
Technology
,
March
17
,
2005
.


-DOCSTART-

Similarity
search
refers
to
any
searching
problem
which
retrieves
objects
from
a
set
that
are
close
to
a
given
query
object
as
re
ected
by
some
similarity
criterion
.

It
has
a
vast
number
of
applications
in
many
branches
of
computer
science
,
from
pattern
recognition
to
textual
and
multimedia
information
retrieval
.

In
this
thesis
,
we
examine
algorithms
designed
for
similarity
search
over
arbitrary
metric
spaces
rather
than
restricting
ourselves
to
vector
spaces
.

The
contributions
in
this
paper
include
the
following
:
First
,
after
de
ning
pivot
sharing
and
pivot
localization
,
we
prove
probabilistically
that
pivot
sharing
level
should
be
increased
for
scattered
data
while
pivot
localization
level
should
be
increased
for
clustered
data
.

This
conclusion
is
supported
by
extensive
experiments
.

Moreover
,
we
proposed
two
new
algorithms
,
RLAESA
and
NGH
-
tree
.

RLAESA
,
using
high
pivot
sharing
level
and
low
pivot
localization
level
,
outperforms
the
fastest
algorithm
in
the
same
category
,
MVP
-
tree
.

NGH
-
tree
is
used
as
a
framework
to
show
the
e
ect
of
increasing
pivot
sharing
level
on
search
e
ciency
.

It
provides
a
way
to
improve
the
search
e
ciency
in
almost
all
algorithms
.

The
experiments
with
RLAESA
and
NGH
-
tree
not
only
show
their
preformance
,
but
also
support
the
rst
conclusion
we
mentioned
above
.

Second
,
we
analyzed
the
issue
of
disk
I
/
O
on
similarity
search
and
proposed
a
new
algorithm
SLAESA
to
improve
the
search
e
ciency
by
switching
random
I
/
O
access
to
sequential
I
/
O
access
.


-DOCSTART-

Ontology
has
a
richer
internal
structure
as
it
includes
relations
and
constraints
between
the
concepts
.

Ontology
can
be
used
for
information
retrieval
.

Ontology
is
a
halfway
determination
of
a
conceptual
vocabulary
to
be
utilized
for
formulating
knowledge
-
level
hypotheses
around
a
domain
of
discourse
.

The
key
part
of
ontology
is
to
help
knowledge
sharing
and
reuse
.

The
process
of
allotting
descriptions
to
documents
in
an
IRS
is
called
indexing
.

In
previous
system
zone
based
indexing
is
introduced
which
has
certain
drawbacks
.

It
helps
finding
results
of
user&apos;s
query
with
exact
match
.

A
new
technique
is
proposed
which
improves
results
.

In
this
technique
web
pages
are
stored
in
xml
database
.

Zones
are
formed
in
database
.

In
case
exact
match
is
not
found
in
xml
database
using
zone
based
indexing
then
proximity
of
keyword


-DOCSTART-

The
growing
usage
of
mobile
devices
has
led
to
proliferation
of
many
mobile
applications
.

A
growing
trend
in
mobile
applications
is
centered
on
mobile
landmark
recognition
.

It
is
a
new
mobile
application
that
recognizes
a
captured
landmark
using
the
mobile
device
and
retrieves
related
information
.

This
paper
will
present
a
survey
on
mobile
landmark
recognition
for
information
retrieval
.

A
general
overview
of
existing
mobile
landmark
recognition
systems
will
be
summarized
.

The
techniques
and
algorithms
used
in
the
literatures
,
including
content
analysis
of
landmarks
and
classification
methods
for
recognition
,
will
be
described
.


-DOCSTART-

Lexical
taxonomies
are
tree
or
directed
acyclic
graph
-
like
structures
where
each
node
represents
a
concept
and
each
edge
encodes
a
binary
hypernymic
(
is
-
a
)
relation
.

These
lexical
resources
are
useful
for
AI
tasks
like
Information
Retrieval
or
Machine
Translation
.

Two
main
trends
exist
in
the
construction
and
exploitation
of
these
resources
:
On
one
hand
,
general
purpose
taxonomies
like
WordNet
,
and
on
the
other
,
domain
-
specific
databases
such
as
the
CheBi
chemical
ontology
,
or
MusicBrainz
in
the
music
domain
.

In
both
cases
these
are
based
on
finding
correct
hypernymic
relations
between
pairs
of
concepts
.

In
this
paper
,
we
propose
a
generic
framework
for
hypernym
discovery
,
based
on
exploiting
linear
relations
between
(
term
,
hypernym
)
pairs
in
Wikidata
,
and
apply
it
to
the
domain
of
music
.

Our
promising
results
,
based
on
several
metrics
used
in
Information
Retrieval
,
show
that
in
several
cases
we
are
able
to
discover
the
correct
hypernym
for
a
given
novel
term
.


-DOCSTART-

A
new
algorithm
for
information
retrieval
is
described
.

It
is
a
vector
space
method
with
automatic
query
expansion
.

The
original
user
query
is
projected
onto
a
Krylov
subspace
generated
by
the
query
and
the
term
-
document
matrix
.

Each
dimension
of
the
Krylov
space
is
generated
by
a
simple
vector
space
search
,
using
first
the
user
query
and
then
new
queries
generated
by
the
algorithm
and
orthogonal
to
the
previous
query
vectors
.

The
new
algorithm
is
closely
related
to
latent
semantic
indexing
(
LSI

but
it
is
a
local
algorithm
that
works
on
a
new
subspace
of
very
low
dimension
for
each
query
.

This
makes
it
faster
and
more
flexible
than
LSI
.

No
preliminary
computation
of
the
singular
value
decomposition
(
SVD
)
is
needed
,
and
changes
in
the
data
base
cause
no
complication
.

Numerical
tests
on
both
small
(
Cranfield
)
and
larger
(
Financial
Times
data
from
the
TREC
collection
)
data
sets
are
reported
.

The
new
algorithm
gives
better
precision
at
given
recall
levels
than
simple
vector
space
and
LSI
in
those
cases
that
have
been
compared
.


-DOCSTART-

The
Problem
:
Text
retrieval
is
a
difficult
problem
that
has
become
both
more
difficult
and
more
important
in
recent
years
.

This
is
because
of
the
increased
amount
of
electronic
information
available
and
the
greater
demand
for
text
search
as
a
result
of
the
World
Wide
Web
.

People
are
surrounded
with
large
quantities
of
information
,
but
unable
to
use
that
information
effectively
because
of
its
overabundance
.


-DOCSTART-

This
study
investigates
if
and
why
assessing
relevance
of
clinical
records
for
a
clinical
retrieval
task
is
cognitively
demanding
.

Previous
research
has
highlighted
the
challenges
and
issues
information
retrieval
systems
are
faced
with
when
determining
the
relevance
of
documents
in
this
domain
,
e.g
the
vocabulary
mismatch
problem
.

Determining
if
this
assessment
imposes
cognitive
load
on
human
assessors
,
and
why
this
is
the
case
,
may
shed
lights
on
what
are
the
(
cognitive
)
processes
that
assessors
use
for
determining
document
relevance
(
in
this
domain
High
cognitive
load
may
impair
the
ability
of
the
user
to
make
accurate
relevance
judgements
and
hence
the
design
of
IR
mechanisms
may
need
to
take
this
into
account
in
order
to
reduce
the
load
.


-DOCSTART-

By
the
increase
of
the
volume
of
the
saved
information
on
web
,
Question
Answering
(
QA
)
systems
have
been
very
important
for
Information
Retrieval
(
IR
QA
systems
are
a
specialized
form
of
information
retrieval
.

Given
a
collection
of
documents
,
a
Question
Answering
system
attempts
to
retrieve
correct
answers
to
questions
posed
in
natural
language
.

Web
QA
system
is
a
sample
of
QA
systems
that
in
this
system
answers
retrieval
from
web
â€Ženvironment
doing
.

In
contrast
to
the
databases
,
the
saved
information
on
web
does
not
follow
a
distinct
structure
and
are
not
generally
defined
.

Web
QA
systems
is
the
task
of
automatically
answering
a
question
posed
in
Natural
Language
Processing
(
NLP
NLP
techniques
are
used
in
applications
that
make
queries
to
databases
,
extract
information
from
text
,
retrieve
relevant
documents
from
a
collection
,
translate
from
one
language
to
another
,
generate
text
responses
,
or
recognize
spoken
words
converting
them
into
text
.

To
find
the
needed
information
on
a
mass
of
the
non
-
structured
information
we
have
to
use
techniques
in
which
the
accuracy
and
retrieval
factors
are
implemented
well
.

In
this
paper
in
order
to
well
IR
in
web
environment
,
The
QA
system
in
designed
and
also
implemented
based
on
the
Hidden
Markov
Model
(
HMM


-DOCSTART-

This
article
looks
at
the
access
to
geographic
information
through
a
review
of
information
science
theory
and
its
application
to
the
WWW
.

The
two
most
common
retrieval
systems
are
information
and
data
retrieval
.

A
retrieval
system
has
seven
elements
:
retrieval
models
,
indexing
,
match
and
retrieval
,
relevance
,
order
,
query
languages
and
query
specification
.

The
goal
of
information
retrieval
is
to
match
the
user
â€™s
needs
to
the
information
that
is
in
the
system
.

Retrieval
of
geographic
information
is
a
combination
of
both
information
and
data
retrieval
.

Aids
to
effective
retrieval
of
geographic
information
are
:
query
languages
that
employ
icons
and
natural
language
,
automatic
indexing
of
geographic
information
,
and
standardization
of
geographic
information
.

One
area
that
has
seen
an
explosion
of
geographic
information
retrieval
systems
(
GIR
â€™s
)
is
the
World
Wide
Web
(
WWW

The
final
section
of
this
article
discusses
how
seven
WWW
GIR
â€™s
solve
the
the
problem
of
matching
the
user
â€™s
information
needs
to
the
information
in
the
system
.

INTRODUCTION
Information
retrieval

(
IR
)
systems
attempt
to
satisfy
requests
for
either
general
information
by
providing
documents
on
a
topic
,
or
precise
data
from
a
database
.

Geographic
information
requests
are
often
a
blend
of
the
two
.

Information
can
come
in
a
variety
of
formats
but
the
critical
measure
of
success
is
the
relevancy
of
the
retrieved
documents
to
the
information
needed
.

The
solution
to
the
traditional
information
1
This
article
deals
with
information
science
issues
pertaining
to
geographic
information
,
such
as
the
retrieval
of
information
and
ability
to
effectively
query
a
retrieval
system
for
information
.

This
is
not
a
technical
article
dealing
with
design
or
interface
issues
of
specific
geographic
information
systems
.

Pertinent
articles
not
discussed
are
included
after
the
reference
section
.

retrieval
problem
is
a
system
â€™s
best
match
between
a
user
's
information
need
and
the
system
-
held
documents
that
resolve
the
information
need
.

One
very
problematic
issue
for
the
retrieval
of
geographic
information
is
query
formulation
.

Retrieval
systems
currently
have
limited
support
to
operationalize
a
user
â€™s
natural
language
or
other
geospatial
queries
as
well
as
limited
support
for
users
to
express
their
information
needs
to
the
system
.

Fortunately
there
are
alternative
query
languages
beyond
that
of
basic
boolean
operators
of
â€œ
anding
â€
and
â€œ
oring
â€
keywords
.

One
alternative
is
â€œ
iconic
query
language
â€
whereby
a
user
creates
a
geographic
query
by
using
icons
.

An
icon
is
a
computer
generated
representation
of
a
physical
object
;
for
example
,
a
line
representing
a
road
or
stream
.

Geographic
information
deals
with
physical
objects
that
are
in
some
cases
hard
to
express
with
words
.

The
ability
to
graphically
create
the
query
may
facilitate
more
effective
retrieval
.

Traditional
query
languages
deal
only
with
the
precise
(
Cats
AND
Dogs
)
and
can
not
handle
the
vagueness
of
concepts
such
as
â€œ
extremely
low
humidity
â€
or
â€œ
recent
information

Users
do
not
always
think
in
precise
terms
,
which
can
also
make
it
difficult
to
retrieve
information
.

By
using
a
fuzzy
query
language
,
the
concept
can
be
converted
into
a
number
sequence
that
the
system
can
use
.

Similarly
,
automatic
indexing
of
georeferenced
information
can
also
help
a
user
to
retrieve
geographic
information
.

The
introduction
of
the
World
Wide
Web
(
WWW
)
and
clickable
image
maps
have
simplified
the
usersâ€™
search
and
retrieval
for
geographic
information
.

The
relative
infancy
of
the
WWW
and
online
geographic
information
systems
(
GIS
)
currently
limit
most
GIR
â€™s
to
queries
for
specific
factual
data
,
further
constrained
from
choices
only
from
menus
.

Relatively
few
WWW
GIR
systems
allow
users
to
edit
and
save
geographic
information
online
.

We
examined
seven
WWW
GIS
systems
to
discover
how
â€œ
the
match
â€
between
a
user
â€™s
information
need
and
the
system
response
for
information
was
accomplished
.

We
present
our
results
below
.

INFORMATION
RETRIEVAL

This
is
the
age
of
information
.

Year
by
year
the
amount
of
data
and
information
increases
exponentially
with
geospatial
data
accounting
for
a
large
proportion
of
this
increase
.

Too
much
information
can
cause
an
information
overload
driving
the
need
to
weed
out
the
irrelevant
information
in
order
to
retrieve
the
relevant
information
.

In
information
science
,
the
discipline
that
deals
with
this
subject
is
information
retrieval
or
IR
.

IR
deals
with
â€œ
the
process
of
searching
some
collection
of
documents
,
using
the
term
documents
in
its
widest
sense
in
order
to
identify
those
documents
which
deal
with
a
particular
subject
Lancaster
1979
,
11
Most
IR
systems
and
research
attempt
to
resolve
the
classic
IR
matching
problem
mentioned
earlier
:
The
systems
try
to
efficiently
and
effectively
â€œ
match
â€
a
user
's
information
need
to
the
document
or
documents
that
can
fulfill
that
need
.

Typical
commercial
IR
systems
use
a
Boolean
-
based
query
mechanism
and
an
inverted
file
of
terms
and
documents
(
e.g
DIALOG
or
LEXIS
/

There
are
other
types
of
IR
systems
such
as
Salton
â€™s
SMART
system
based
on
vector
space
and
Belkin
â€™s

ASK
Anomalous
States
of
Knowledge
system
which
instead
of
trying
to
get
the
perfect
â€œ
match

â€
tries
to
get
the
user
close
enough
to
the
right
documents
,
similar
to
browsing
in
a
library
stack
.

According
to
Ray
R.
Larson
(
1996
six
basic
elements
make
up
a
retrieval
system
.

All
six
retrieval
elements
are
pertinent
to
the
retrieval
of
geospatial
data
and
information
and
will
be
addressed
in
the
Geographic
Information
and
Information
Retrieval
section
of
this
paper
.

The
six
elements
are
1
)
retrieval
models
2
)
indexing
3
)
how
items
are
matched
and
retrieved
4
)
relevance
5
)
order
of
results
,
and
(
6
)
query
2
Salton
,
G
McGill
,
M.J.
1983
.

Introduction
to
modern
information
retrieval
.

McGraw
-
Hill
.

3
Belkin
,
N.J
R.N.

Oddy
H.M.
Brooks
1982
.

ASK
for
Information
Retrieval
:

Part
I.
Background
and
Theory
.

Journal
of
Documentation
38(2
)
:


-DOCSTART-

In
addition
to
the
human
-
readable
contents
of
the
shared
documents
,
the
new
generation
of
WWW
requires
machinereadable
formalization
of
data
.

We
present
an
ontology
-
based
information
management
system
called
knOWLer
,
targeting
semantic
integration
into
large
-
scale
information
systems
.

In
this
paper
,
we
are
specially
focusing
on
large
-
scale
information
retrieval
systems
.

The
semantic
is
provided
through
an
ontology
language
(
OWL
showing
that
ontological
reasoning
can
be
scaled
to
sizes
of
standard
IR
systems
.

We
propose
an
information
management
system
for
automatic
document
manipulation
based
on
the
semantics
added
by
an
ontology
to
the
raw
data
.

The
main
capabilities
of
our
application
are
expressivity
provided
by
the
ontology
language
and
scalability
induced
by
the
persistent
storage
mechanism
.


-DOCSTART-

Intelligent
foraging
,
gathering
and
matching
(
I
-
FGM
)
has
been
shown
to
be
an
effective
tool
for
intelligence
analysts
who
have
to
deal
with
large
and
dynamic
search
spaces
.

I
-
FGM
introduced
a
unique
resource
allocation
strategy
based
on
a
partial
information
processing
paradigm
which
,
along
with
a
modular
system
architecture
,
makes
it
a
truly
novel
and
comprehensive
solution
to
information
retrieval
in
such
search
spaces
.

This
paper
provides
further
validation
of
its
performance
by
studying
its
behavior
while
working
with
highly
dynamic
databases
.

Results
from
earlier
experiments
were
analyzed
and
important
changes
have
been
made
in
the
system
parameters
to
deal
with
dynamism
in
the
search
space
.

These
changes
also
help
in
our
goal
of
providing
relevant
search
results
quickly
and
with
minimum
wastage
of
computational
resources
.

Experiments
have
been
conducted
on
I
-
FGM
in
a
realistic
and
dynamic
simulation
environment
,
and
its
results
are
compared
with
two
other
control
systems
.

I
-
FGM
clearly
outperforms
the
control
systems
.


-DOCSTART-

Deep
architectures
are
families
of
functions
corresponding
to
deep
circuits
.

Deep
Learning
algorithms
are
based
on
parametrizing
such
circuits
and
tuning
their
parameters
so
as
to
approximately
optimize
some
training
objective
.

Whereas
it
was
thought
too
difficult
to
train
deep
architectures
,
several
successful
algorithms
have
been
proposed
in
recent
years
.

We
review
some
of
the
theoretical
motivations
for
deep
architectures
,
as
well
as
some
of
their
practical
successes
,
and
propose
directions
of
investigations
to
address
some
of
the
remaining
challenges
.

Learning
Artificial
Intelligence

An
intelligent
agent
takes
good
decisions
.

In
order
to
do
so
it
needs
some
form
of
knowledge
.

Knowledge
can
be
embodied
into
a
function
that
maps
inputs
and
states
to
states
and
actions
.

If
we
saw
an
agent
that
always
took
what
one
would
consider
as
the
good
decisions
,
we
would
qualify
the
agent
as
intelligent
.

Knowledge
can
be
explicit
,
as
in
the
form
of
symbolically
expressed
rules
and
facts
of
expert
systems
,
or
in
the
form
of
linguistic
statements
in
an
encyclopedia
.

However
,
knowledge
can
also
be
implicit
,
as
in
the
complicated
wiring
and
synaptic
strengths
of
animal
brains
,
or
even
in
the
mechanical
properties
of
an
animal
â€™s
body
.

Whereas
Artificial
Intelligence
(
AI
)
research
initially
focused
on
providing
computers
with
knowledge
in
explicit
form
,
it
turned
out
that
much
of
our
knowledge
was
not
easy
to
express
formally
.

What
is
a
chair
?

We
might
write
a
definition
that
can
help
another
human
understand
the
concept
(
if
he
did
not
know
about
it

but
it
is
difficult
to
make
it
sufficiently
complete
for
a
computer
to
translate
into
the
same
level
of
competence
(
e.g.
in
recognizing
chairs
in
images
Much
so
-
called
common
-
sense
knowledge

has
this
property
.

If
we
can
not
endowe
computers
with
all
the
required
knowledge
,
an
alternative
is
to
let
them
learn
it
from
examples
.

Machine
learning
algorithms
aim
to
extract
knowledge
from
examples
(
i.e
data
so
as
to
be
able
to
properly
generalize
to
new
examples
.

Our
own
implicit
knowledge
arises
either
out
of
our
life
experiences
(
lifetime
learning
)
or
from
the
longer
scale
form
of
learning
that
evolution
really
represents
,
where
the
result
of
adaptation
is
encoded
in
the
genes
.

Science
itself
is
a
process
of
learning
from
observations
and
experiments
in
order
to
produce
actionable
knowledge
.

Understanding
the
principles
by
which
agents
can
capture
knowledge
through
examples
,
i.e
learn
,
is
therefore
a
central
scientific
question
with
implications
not
only
for
AI
and
technology
,
but
also
to
understand
brains
and
evolution
.

Formally
,
a
learning
algorithm
can
be
seen
as
a
functional
that
maps
a
dataset
(
a
set
of
examples
)
to
a
function
(
typically
,
a
decision
function
Since
the
dataset
is
itself
a
random
variable
,
the
learning
process
involves
the
application
of
a
procedure
to
a
target
distribution
from
which
the
examples
are
drawn
and
for
which
one
would
like
to
infer
a
good
decision
function
.

Many
modern
learning
algorithms
are
expressed
as
an
optimization
problem
,
in
which
one
tries
to
find
a
compromise
between
minimizing
empirical
error
on
training
examples
and
minimizing
a
proxy
for
the
richness
of
the
family
of
functions
that
contains
the
solution
.

A
particular
challenge
of
learning
algorithms
for
AI
tasks
(
such
as
understanding
images
,
video
,
natural
language
text
,
or
speech
)
is
that
such
tasks
involve
a
large
number
of
variables
with
complex
dependencies
,
and
that
the
amount
of
knowledge
required
to
master
these
tasks
is
very
large
.

Statistical
learning
theory
teaches
us
that
in
order
to
represent
a
large
body
of
knowledge
,
one
requires
a
correspondingly
large
number
of
degrees
of
freedom
(
or
richness
of
a
class
of
functions
)
and
a
correspondingly
large
number
of
training
examples
.

In
addition
to
the
statistical
challenge
,
machine
learning
often
involves
a
computational
challenge
due
to
the
difficulty
of
optimizing
the
training
criterion
.

Indeed
,
in
many
cases
,
that
training
criterion
is
not
convex
,
and
in
some
cases
it
is
not
even
directly
measurable
in
a
deterministic
way
and
its
gradient
is
estimated
by
stochastic
(
sampling
-
based
)
methods
,
and
from
only
a
few
examples
at
a
time
(
online
learning
One
of
the
characteristics
that
has
spurred
much
interest
and
research
in
recent
years
is
depth
of
the
architecture
.

In
the
case
of
a
multi
-
layer
neural
network
,
depth
corresponds
to
the
number
of
(
hidden
and
output
)
layers
.

A
fixed
-
kernel
Support
Vector
Machine
is
considered
to
have
depth
2
(
Bengio
and
LeCun
,
2007a
)
and
boosted
decision
trees
to
have
depth
3
(
Bengio
et
al
2010

Here
we
use
the
word
circuit
or
network
to
talk
about
a
directed
acyclic
graph
,
where
each
node
is
associated
with
some
output
value
which
can
be
computed
based
on
the
values
associated
with
its
predecessor
nodes
.

The
arguments
of
the
learned
function
are
set
at
the
input
nodes
of
the
circuit
(
which
have
no
predecessor
)
and
the
outputs
of
the
function
are
read
off
the
output
nodes
of
the
circuit
.

Different
families
of
functions
correspond
to
different
circuits
and
allowed
choices
of
computations
in
each
node
.

Learning
can
be
performed
by
changing
the
computation
associated
with
a
node
,
or
rewiring
the
circuit
(
possibly
changing
the
number
of
nodes
The
depth
of
the
circuit
is
the
length
of
the
longest
path
in
the
graph
from
an
input
node
to
an
output
node
.

This
paper
also
focuses
on
Deep
Learning
,
i.e
learning
multiple
levels
of
representation
.

The
intent
is
to
discover
more
abstract
features
in
the
higher
levels
of
the
representation
,
which
hopefully
make
it
easier
to
separate
from
each
other
the
various
explanatory
factors
extent
in
the
data
.

Theoretical
results
(
Yao
,
1985
;
HÌŠastad
,
1986
;
HÌŠastad
and
Goldmann
,
1991
;
Bengio
et
al
2006
;
Bengio
and
Delalleau
,
2011
;
Braverman
,
2011
reviewed
briefly
here
(
see
also
a
previous
discussion
by
Bengio
and
LeCun
,
2007b
)
suggest
that
in
order
to
learn
the
kind
of
complicated
functions
that
can
represent
high
-
level
abstractions
(
e.g
in
vision
,
language
,
and
other
AI
-
level
tasks
)
associated
with
functions
with
many
variations
but
an
underlying
simpler
structure
,
one
may
need
deep
architectures
.

The
recent
surge
in
experimental
work
in
the
field
seems
to
support
this
notion
,
accumulating
evidence
that
in
challenging
AI
-
related
tasks
such
as
computer
vision
(
Bengio
et
al
2007
;
Ranzato
et
al
2007
;
Larochelle
et
al
2007
;
Ranzato
et
al
2008
;
Lee
et
al
2009
;
Mobahi
et
al
2009
;
Osindero
and
Hinton
,
2008
natural
language
processing
(
NLP
Collobert
and
Weston
,
2008a
;
Weston
et
al
2008
robotics
(
Hadsell
et
al
2008
or
information
retrieval
(
Salakhutdinov
and
Hinton
,
2007
;
Salakhutdinov
et
al
2007
deep
learning
methods
significantly
out
-
perform
comparable
but
shallow
competitors
(
e.g.
winning
the
Unsupervised
and
Transfer
Learning
Challenge
;
Mesnil
et
al
2011
and
often
match
or
beat
the
state
-
of
-
the
-
art
.

In
this
paper
we
discuss
some
of
the
theoretical
motivations
for
deep
architectures
,
and
quickly
review
some
of
the
current
layer
-
wise
unsupervised
featurelearning
algorithms
used
to
train
them
.

We
conclude
with
a
discussion
of
principles
involved
,
challenges
ahead
,
and
ideas
to
face
them
.

2
Local
and
Non
-
Local
Generalization
:

The
Challenge
and
Curse
of
Many
Factors
of
Variation

How
can
learning
algorithms
generalize
from
training
examples
to
new
cases
?

It
can
be
shown
that
there
are
no
completely
universal
learning
procedures
,
in
the
sense
that
for
any
learning
procedure
,
there
is
a
target
distribution
on
which
it
does
poorly
(
Wolpert
,
1996

Hence
,
all
generalization
principles
exploit
some
property
of
the
target
distribution
,
i.e
some
kind
of
prior
.

The
most
exploited
generalization
principle
is
that
of
local
generalization
.

It
relies
on
a
smoothness
assumption
,
i.e
that
the
target
function
(
the
function
to
be
learned
)
is
smooth
(
according
to
some
measure
of
smoothness
i.e
changes
slowly
and
rarely
(
Barron
,
1993
Contrary
to
what
has
often
been
said
,
what
mainly
hurts
many
algorithms
relying
only
on
this
assumption
(
pretty
much
all
of
the
nonparametric
statistical
learning
algorithms
)
is
not
the
dimensionality
of
the
input
but
instead
the
insufficient
smoothness
of
the
target
function
.

To
make
a
simple
picture
,
imagine
the
supervised
learning
framework
and
a
target
function
that
is
locally
smooth
but
has
many
ups
and
downs
in
the
domain
of
interest
.

We
showed
that
if
one
considers
a
straight
line
in
the
input
domain
,
and
counts
the
number
of
ups
and
downs
along
that
line
,
then
a
learner
based
purely
on
local
generalization
(
such
as
a
Gaussian
kernel
machine
)
requires
at
least
as
many
examples
as
there
are
ups
and
downs
(
Bengio
et
al
2006
Manifold
learning
algorithms
are
unsupervised
learning
procedures
aiming
to
characterize
a
low
-
dimensional
manifold
near
which
the
target
distribution
concentrates
.

Bengio
and
Monperrus
(
2005
)
argued
that
many
real
-
world
manifolds
(
such
as
the
one
generated
by
translations
or
rotations
of
images
,
when
the
image
is
represented
by
its
pixel
intensities
)
are
highly
curved
(
translating
by
1
1
but
of
course
additional
noisy
dimensions
,
although
they
do
not
change
smoothness
of
the
target
function
,
require
more
examples
to
cancel
the
noise
.

pixel
can
change
the
tangent
plane
of
the
manifold
by
about
90
degrees
The
manifold
learning
algorithms
of
the
day
,
based
implicitly
or
explicitly
on
nonparametric
estimation
of
the
local
tangent
planes
to
the
manifold
,
are
relying
on
purely
local
generalization
.

Hence
they
would
require
a
number
of
examples
that
grows
linearly
with
the
dimension
d
of
the
manifold
and
the
number
of
patches
O
D
r
)
d
needed
to
cover
its
nooks
and
crannies
,
i.e
in
O
d

D
r
)
d
)
examples
,
where
D
is
a
diameter
of
the
domain
of
interest
and
r
a
radius
of
curvature
.

3
Expressive
Power
of
Deep
Architectures
To
fight
an
exponential

,
it
seems
reasonable
to
arm
oneself
with
other
exponentials
.

We
discuss
two
strategies
that
can
bring
a
potentially
exponential
statistical
gain
thanks
to
a
combinatorial
effect
:
distributed
(
possibly
sparse
)
representations
and
depth
of
architecture
.

We
also
present
an
example
of
the
latter
in
more
details
in
the
specific
case
of
so
-
called
sum
-
product
networks
.

3.1
Distributed
and
Sparse
Representations
Learning
algorithms
based
on
local
generalization
can
generally
be
interpreted
as
creating
a
number
of
local
regions
(
possibly
overlapping
,
possibly
with
soft
rather
than
hard
boundaries
such
that
each
region
is
associated
with
its
own
degrees
of
freedom
(
parameters
,
or
examples
such
as
prototypes

Such
learning
algorithms
can
then
learn
to
discriminate
between
these
regions
,
i.e
provide
a
different
response
in
each
region
(
and
possibly
doing
some
form
of
smooth
interpolation
when
the
regions
overlap
or
have
soft
boundaries
Examples
of
such
algorithms
include
the
mixture
of
Gaussians
(
for
density
estimation
Gaussian
kernel
machines
(
for
all
kinds
of
tasks
ordinary
clustering
(
such
as
k
-
means
,
agglomerative
clustering
or
affinity
propagation
decision
trees
,
nearest
-
neighbor
and
Parzen
windows
estimators
,
etc
As
discussed
in
previous
work
(
Bengio
et
al
2010
all
of
these
algorithms
will
generalize
well
only
to
the
extent
that
there
are
enough
examples
to
cover
all
the
regions
that
need
to
be
distinguished
from
each
other
.

As
an
example
of
such
algorithms
,
the
way
a
clustering
algorithm
or
a
nearestneighbor
algorithm
could
partition
the
input
space
is
shown
on
the
left
side
of
Fig
.

Instead
,
the
right
side
of
the
figure
shows
how
an
algorithm
based
on
distributed
representations
(
such
as
a
Restricted
Boltzmann
Machine
;
Hinton
et
al
2006
)
could
partition
the
input
space
.

Each
binary
hidden
variable
identifies
on
which
side
of
a
hyper
-
plane
the
current
input
lies
,
thus
breaking
out
input
space
in
a
number
of
regions
that
could
be
exponential
in
the
number
of
hidden
units
(
because
one
only
needs
a
few
examples
to
learn
where
to
put
each
hyper
-
plane
i.e
in
the
number
of
parameters
.

If
one
assigns
a
binary
code
to
each
region
,
this
is
also
a
form
of
clustering
,
which
has
been
called
multi
-
clustering
(
Bengio
,
2009
Distributed
representations
were
put
forward
in
the
early
days
of
connectionism
and
artificial
neural
networks
(
Hinton
,
1986
,
1989

More
recently
,
a


-DOCSTART-

This
paper
discusses
an
email
discovery
and
information
retrieval
tool
based
on
formal
concept
analysis
.

The
ECA
program
allows
its
users
to
navigate
email
using
a
visual
lattice
metaphor
rather
than
a
tree
.

It
implements
a
virtual
file
structure
over
email
where
files
and
entire
directories
can
appear
in
multiple
positions
in
a
given
view
.

The
content
and
shape
of
the
lattice
formed
by
the
conceptual
ontology
can
assist
in
email
discovery
.

The
system
described
provides
more
flexibility
in
retrieving
stored
emails
than
what
is
normally
availableq
in
email
clients
and
the
approach
can
be
generalised
to
any
electronic
document
type
.

The
paper
discusses
how
conceptual
ontologies
can
leverage
traditional
IR
systems
.


-DOCSTART-

Publisher
UPGRADE
is
published
on
behalf
of
CEPIS
(
Council
of
European
Professional
Informatics
Societies
http
www
.

cepis.org
by
NovÃ¡tica
<
http
www.ati.es/novatica
journal
of
the
CEPIS
society
ATI
(
AsociaciÃ³n
de
TÃ©cnicos
de
InformÃ¡tica
,

Spain
<
http
www.ati.es
UPGRADE
is
also
published
in
Spanish
(
full
issue
printed
,
some
articles
online
)
by
NovÃ¡tica
,
and
in
Italian
(
online
edition
,
abstracts
only
)
by
the
Italian
CEPIS
society
ALSI
<
http
www.alsi.it
>
and
the
Italian
IT
portal
Tecnoteca
<
http
www.tecnoteca.it
UPGRADE
was
created
in
October
2002
by
CEPIS
and
was
first
published
by
NovÃ¡tica
and
by
Informatik
/
Informatique
,
bimonthly
journal
of
SVI
/
FSI
(
Swiss
Federation
of
Professional
Informatics
Societies
http
www.svifsi.ch
>
)


-DOCSTART-

OBJECTIVE
The
main
objective
of
this
study
is
to
present
a
methodology
for
computing
information
relevance
.

BACKGROUND
Relevance
is
a
pervasive
term
used
in
several
domains
,
such
as
pragmatics
,
information
science
,
and
psychology
.

Quantifying
the
relevance
of
information
can
be
helpful
in
effective
display
design
.

Displays
should
be
designed
so
that
the
more
relevant
information
is
more
easily
accessed
.

This
procedure
focuses
on
computing
the
relevance
of
a
piece
of
information
by
taking
into
account
three
aspects
of
tasks
that
use
the
information
:
the
number
of
different
tasks
that
make
use
of
the
information
,
the
frequency
of
occurrence
of
those
tasks
,
and
the
criticality
of
those
tasks
.

The
methodology
can
be
used
to
compute
the
aggregate
relevance
of
a
piece
of
information
for
a
particular
component
of
a
system
or
for
the
entire
system
.

This
methodology
was
illustrated
using
the
domain
of
air
traffic
control
(
ATC
RESULTS

In
support
of
the
validity
of
the
methodology
,
we
were
able
to
confirm
the
value
of
weather
information
and
traffic
information
in
ATC
towers
.

The
method
can
be
used
to
derive
information
relevance
,
a
characteristic
of
information
that
has
implications
for
display
design
for
any
domain
.

APPLICATION
Designers
can
use
information
about
aggregate
relevance
to
design
information
displays
that
feature
the
most
relevant
information
.


-DOCSTART-

This
paper
revises
David
Ellis
â€™s
information
-
seeking
behavior
model
of
social
scientists
,
which
includes
six
generic
features
:
starting
,
chaining
,
browsing
,
differentiating
,
monitoring
,
and
extracting
.

The
paper
uses
social
science
faculty
researching
stateless
nations
as
the
study
population
.

The
description
and
analysis
of
the
information
-
seeking
behavior
of
this
group
of
scholars
is
based
on
data
collected
through
structured
and
semistructured
electronic
mail
interviews
.

Sixty
faculty
members
from
14
different
countries
were
interviewed
by
e
-
mail
.

For
reality
check
purposes
,
face
-
to
-
face
interviews
with
five
faculty
members
were
also
conducted
.

Although
the
study
confirmed
Ellis
â€™s
model
,
it
found
that
a
fuller
description
of
the
information
-
seeking
process
of
social
scientists
studying
stateless
nations
should
include
four
additional
features
besides
those
identified
by
Ellis
.

These
new
features
are
:
accessing
,
networking
,
verifying
,
and
information
managing
.

In
view
of
that
,
the
study
develops
a
new
model
,
which
,
unlike
Ellis
â€™s
,
groups
all
the
features
into
four
interrelated
stages
:
searching
,
accessing
,
processing
,
and
ending
.

This
new
model
is
fully
described
and
its
implications
on
research
and
practice
are
discussed
.

How
and
why
scholars
studied
here
are
different
than
other
academic
social
scientists
is
also
discussed
.


-DOCSTART-

Ordinal
regression
is
an
important
type
of
learning
,
which
has
properties
of
both
classification
and
regression
.

Here
we
describe
an
effective
approach
to
adapt
a
traditional
neural
network
to
learn
ordinal
categories
.

Our
approach
is
a
generalization
of
the
perceptron
method
for
ordinal
regression
.

On
several
benchmark
datasets
,
our
method
(
NNRank
)
outperforms
a
neural
network
classification
method
.

Compared
with
the
ordinal
regression
methods
using
Gaussian
processes
and
support
vector
machines
,
NNRank
achieves
comparable
performance
.

Moreover
,
NNRank
has
the
advantages
of
traditional
neural
networks
:
learning
in
both
online
and
batch
modes
,
handling
very
large
training
datasets
,
and
making
rapid
predictions
.

These
features
make
NNRank
a
useful
and
complementary
tool
for
large
-
scale
data
mining
tasks
such
as
information
retrieval
,
Web
page
ranking
,
collaborative
filtering
,
and
protein
ranking
in
bioinformatics
.

The
neural
network
software
is
available
at
:
http
www.cs.missouri.edu/~chengji/cheng
software.html
.


-DOCSTART-

Stock
spam
is
a
part
of
unsolicited
electronic
mail
which
wants
its
receiver
to
buy
a
certain
share
at
the
stock
markets
.

As
there
is
no
direct
profit
for
the
sender
of
this
message
,
impacts
on
the
share
prices
seem
to
be
the
only
way
to
generate
earnings
by
the
spam
sender
.

To
hide
the
information
within
stock
spam
messages
from
automatic
processing
,
textual
and
image
-
based
distortions
are
used
.

This
thesis
will
to
show
that
the
automatic
processing
of
stock
spam
messages
is
possible
and
the
available
tools
from
the
domains
of
Optical
Character
Recognition
and
Information
Retrieval
are
sufficient
.

In
-
depth
analyses
on
different
tools
and
methods
are
performed
to
find
the
best
suitable
ones
.

Thereafter
the
model
will
be
implemented
and
a
large
number
of
messages
is
processed
to
identify
stock
spam
messages
.

Finally
,
a
descriptive
analysis
on
the
characteristics
of
stock
spam
messages
and
their
impact
to
financial
markets
is
conducted
.

Stock
Spam
ist
ein
Teil
der
unerwÃ¼nschten
eMail
Nachrichten
,
der
ihren
EmpfÃ¤nger
dazu
bringen
mÃ¶chte
,
bestimmte
Aktien
Ã¼ber
den
Aktienmarkt
zu
kaufen
.

Nachdem
dies
keinen
direkten
Vorteil
fÃ¼r
den
Sender
bringt
,
scheinen
Auswirkungen
auf
die
Aktienkurse
der
einzige
Weg
zu
sein
,
mit
dem
der
Sender
Gewinn
erzielen
kann
.

Um
die
Informationen
in
Stock
Spam
Nachrichten
vor
automatischen
Auswertungen
zu
schÃ¼tzen
,
werden
textund
bildbasierte
StÃ¶rungen
verwendet
.

Diese
Arbeit
wird
zeigen
,
dass
die
automatische

Verarbeitung
von
Stock
Spam
Nachrichten
mÃ¶glich

ist
und
die
vorhandenen
Werkzeuge
aus
den
Bereichen
Optische
Zeichenerkennung
und
Information
Retrieval
ausreichend
sind
.

Es
werden
detaillierte
Analysen
Ã¼ber
die
verschiedenen
Werkzeuge
und
Methoden
durchgefÃ¼hrt
,
um
diejenigen
zu
finden
,
die
das

beschriebene
Problem
am
besten
lÃ¶sen
.

Danach
wird
das
Ergebnis
implementiert
und
eine
groÃŸe
Zahl
an
Nachrichten

verarbeitet
um
Stock
Spam
Nachrichten
zu
finden
.

AbschlieÃŸend
wird
eine
deskriptive
Analyse
der
Eigenschaften
von
Stock
Spam
und
dessen
Auswirkung
auf
die
FinanzmÃ¤rkte
durchgefÃ¼hrt
.


-DOCSTART-

Music
information
retrieval
(
MIR
)
as
a
nascent
discipline
is
blessed
with
a
multi
-
disciplinary
group
of
people
endeavoring
to
bring
their
respective
knowledge
-
bases
and
research
paradigms
to
bear
on
MIR
problems
.

Communication
difficulties
across
disciplinary
boundaries
,
however
,
threaten
to
impede
the
maturation
of
MIR
into
a
full
-
fledge
discipline
.

The
principal
causes
of
the
communications
breakdown
among
members
of
the
MIR
community
are
a
)

the
lack
of
bibliographic
control
of
the
MIR
literature
;
and
,
b
)
the
use
of
discipline
-
specific
languages
and
methodologies
throughout
that
literature
.

This
poster
abstract
reports
upon
the
background
,
framework
,
goals
and
ongoing
development
of
the
MIR
Annotated
Bibliography
Website
Project
.

This
project
is
being
undertaken
to
specifically
address
and
overcome
these
bibliographic
control
and
communications
issues
.


-DOCSTART-

This
paper
addresses
the
issue
of
devising
a
new
document
prior
for
the
language
modeling
(
LM
)
approach
for
Information
Retrieval
.

The
prior
is
based
on
term
statistics
,
derived
in
a
probabilistic
fashion
and
portrays
a
novel
way
of
considering
document
length
.

Furthermore
,
we
developed
a
new
way
of
combining
document
length
priors
with
the
query
likelihood
estimation
based
on
the
risk
of
accepting
the
latter
as
a
score
.

This
prior
has
been
combined
with
a
document
retrieval
language
model
that
uses
Jelinek
-
Mercer
(
JM
a
smoothing
technique
which
does
not
take
into
account
document
length
.

The
combination
of
the
prior
boosts
the
retrieval
performance
,
so
that
it
outperforms
a
LM
with
a
document
length
dependent
smoothing
component
(
Dirichlet
prior
)
and
other
state
of
the
art
high
-
performing
scoring
function
(
BM25
Improvements
are
significant
,
robust
across
different
collections
and
query
sizes
.


-DOCSTART-

There
is
an
increasing
amount
of
structure
on
the
web
as
a
result
of
modern
web
languages
,
user
tagging
and
annotation
,
emerging
robust
NLP
tools
,
and
an
ever
growing
volume
of
linked
data
.

These
meaningful
,
semantic
,
annotations
hold
the
promise
to
significantly
enhance
information
access
,
by
enhancing
the
depth
of
analysis
of
today
â€™s
systems
.

Currently
,
we
have
only
started
exploring
the
possibilities
and
only
begin
to
understand
how
these
valuable
semantic
cues
can
be
put
to
fruitful
use
.

ESAIRâ€™13
focuses
on
two
of
the
most
challenging
aspects
to
address
in
the
coming
years
.

First
,
there
is
a
need
to
include
the
currently
emerging
knowledge
resources
(
such
as
DBpedia
,
Freebase
)
as
underlying
semantic
model
giving
access
to
an
unprecedented
scope
and
detail
of
factual
information
.

Second
,
there
is
a
need
to
include
annotations
beyond
the
topical
dimension
(
think
of
sentiment
,
reading
level
,
prerequisite
level
,
etc
)
that
contain
vital
cues
for
matching
the
specific
needs
and
profile
of
the
searcher
at
hand
.


-DOCSTART-

Users
increasingly
rely
on
their
mobile
devices
to
search
local
entities
,
typically
businesses
,
while
on
the
go
.

Even
though
recent
work
has
recognized
that
the
ranking
signals
in
mobile
local
search
(
e.g
distance
and
customer
rating
score
of
a
business
)
are
quite
different
from
general
Web
search
,
they
have
mostly
treated
these
signals
as
a
black
-
box
to
extract
very
basic
features
(
e.g
raw
distance
values
and
rating
scores
)
without
going
inside
the
signals
to
understand
how
exactly
they
affect
the
relevance
of
a
business
.

However
,
as
it
has
been
demonstrated
in
the
development
of
general
information
retrieval
models
,
it
is
critical
to
explore
the
underlying
behaviors
/
heuristics
of
a
ranking
signal
to
design
more
effective
ranking
features
In
this
paper
,
we
follow
a
data
-
driven
methodology
to
study
the
behavior
of
these
ranking
signals
in
mobile
local
search
using
a
large
-
scale
query
log
.

Our
analysis
reveals
interesting
heuristics
that
can
be
used
to
guide
the
exploitation
of
different
signals
.

For
example
,
users
often
take
the
mean
value
of
a
signal
(
e.g
rating
)
from
the
business
result
list
as
a
"
pivot
"
score
,
and
tend
to
demonstrate
different
click
behaviors
on
businesses
with
lower
and
higher
signal
values
than
the
pivot
;
the
clickrate
of
a
business
generally
is
sublinearly
decreasing
with
its
distance
to
the
user
,
etc
.

Inspired
by
the
understanding
of
these
heuristics
,
we
further
propose
different
transformation
methods
to
generate
more
effective
ranking
features
.

We
quantify
the
improvement
of
the
proposed
new
features
using
real
mobile
local
search
logs
over
a
period
of
14
months
and
show
that
the
mean
average
precision
can
be
improved
by
over
7
%
.


